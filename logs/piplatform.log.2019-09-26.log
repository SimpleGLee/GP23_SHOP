19:13:03.729 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
19:13:04.988 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
19:13:05.132 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
19:13:05.134 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
19:13:05.136 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
19:13:05.137 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
19:13:05.138 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
19:13:07.218 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 10263.
19:13:07.326 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
19:13:07.472 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
19:13:07.489 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19:13:07.490 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
19:13:07.517 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-ba01a85e-2237-48b4-b5f3-a5b0460d4c82
19:13:07.660 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
19:13:07.878 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
19:13:08.276 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @28825ms
19:13:08.530 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
19:13:08.555 INFO  [main] org.spark_project.jetty.server.Server - Started @29106ms
19:13:08.599 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@41c528bc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:13:08.600 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
19:13:08.668 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/jobs,null,AVAILABLE,@Spark}
19:13:08.670 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/jobs/json,null,AVAILABLE,@Spark}
19:13:08.670 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/jobs/job,null,AVAILABLE,@Spark}
19:13:08.674 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/jobs/job/json,null,AVAILABLE,@Spark}
19:13:08.675 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ea1fda{/stages,null,AVAILABLE,@Spark}
19:13:08.676 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@420745d7{/stages/json,null,AVAILABLE,@Spark}
19:13:08.677 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fa47fea{/stages/stage,null,AVAILABLE,@Spark}
19:13:08.681 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545f80bf{/stages/stage/json,null,AVAILABLE,@Spark}
19:13:08.683 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22fa55b2{/stages/pool,null,AVAILABLE,@Spark}
19:13:08.689 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6594402a{/stages/pool/json,null,AVAILABLE,@Spark}
19:13:08.690 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@405325cf{/storage,null,AVAILABLE,@Spark}
19:13:08.691 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c3f01f{/storage/json,null,AVAILABLE,@Spark}
19:13:08.693 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/storage/rdd,null,AVAILABLE,@Spark}
19:13:08.694 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/storage/rdd/json,null,AVAILABLE,@Spark}
19:13:08.695 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/environment,null,AVAILABLE,@Spark}
19:13:08.696 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5298dead{/environment/json,null,AVAILABLE,@Spark}
19:13:08.700 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/executors,null,AVAILABLE,@Spark}
19:13:08.701 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/executors/json,null,AVAILABLE,@Spark}
19:13:08.704 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/executors/threadDump,null,AVAILABLE,@Spark}
19:13:08.705 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b986cd{/executors/threadDump/json,null,AVAILABLE,@Spark}
19:13:08.722 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41c62850{/static,null,AVAILABLE,@Spark}
19:13:08.724 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f574cc2{/,null,AVAILABLE,@Spark}
19:13:08.727 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a9c84a5{/api,null,AVAILABLE,@Spark}
19:13:08.729 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f1249d{/jobs/job/kill,null,AVAILABLE,@Spark}
19:13:08.730 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a68df9{/stages/stage/kill,null,AVAILABLE,@Spark}
19:13:08.736 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
19:13:10.494 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
19:13:10.716 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10284.
19:13:10.771 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:10284
19:13:10.781 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19:13:10.880 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 10284, None)
19:13:10.921 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:10284 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 10284, None)
19:13:10.948 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 10284, None)
19:13:10.949 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 10284, None)
19:13:11.398 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e850122{/metrics/json,null,AVAILABLE,@Spark}
19:13:16.721 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
19:13:16.791 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
19:13:16.791 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
19:13:16.834 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@202898d7{/SQL,null,AVAILABLE,@Spark}
19:13:16.835 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c719bd4{/SQL/json,null,AVAILABLE,@Spark}
19:13:16.836 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6048e26a{/SQL/execution,null,AVAILABLE,@Spark}
19:13:16.837 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44286963{/SQL/execution/json,null,AVAILABLE,@Spark}
19:13:16.846 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75d366c2{/static/sql,null,AVAILABLE,@Spark}
19:13:18.391 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19:13:20.061 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19:13:20.125 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
19:13:23.191 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19:13:27.145 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
19:13:27.148 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
19:13:27.862 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
19:13:27.878 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
19:13:28.060 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
19:13:28.398 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
19:13:28.400 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
19:13:28.481 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
19:13:28.482 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
19:13:28.565 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
19:13:28.565 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19:13:28.569 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
19:13:28.570 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
19:13:28.575 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
19:13:28.575 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
19:13:28.580 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
19:13:28.580 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
19:13:28.585 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
19:13:28.585 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
19:13:28.589 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
19:13:28.589 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
19:13:28.593 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
19:13:28.593 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
19:13:32.016 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/30ed2dc2-ec02-41bf-ae82-fd2da1922735_resources
19:13:32.144 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/30ed2dc2-ec02-41bf-ae82-fd2da1922735
19:13:32.148 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/30ed2dc2-ec02-41bf-ae82-fd2da1922735
19:13:32.309 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/30ed2dc2-ec02-41bf-ae82-fd2da1922735/_tmp_space.db
19:13:32.340 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
19:13:32.476 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:13:32.476 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:13:32.487 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
19:13:32.487 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
19:13:32.494 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
19:13:33.065 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/c4c7ce0c-35f8-4574-8477-c92a3e4dfd1d_resources
19:13:33.095 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/c4c7ce0c-35f8-4574-8477-c92a3e4dfd1d
19:13:33.107 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/c4c7ce0c-35f8-4574-8477-c92a3e4dfd1d
19:13:33.118 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/c4c7ce0c-35f8-4574-8477-c92a3e4dfd1d/_tmp_space.db
19:13:33.144 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
19:13:33.419 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
19:13:33.436 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
19:13:34.154 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:13:34.155 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:13:34.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:13:34.268 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:13:35.346 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:13:35.346 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:13:35.420 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:35.464 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:35.464 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:35.465 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:35.465 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:35.466 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:35.466 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:35.467 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:35.467 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:35.467 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:13:35.580 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
19:13:36.947 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
19:13:36.971 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
19:13:36.972 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
19:13:36.974 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
19:13:36.975 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
19:13:36.976 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
19:13:36.977 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') idcard
19:13:37.013 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy') as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as age
19:13:37.062 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})',2) as int)%2 as gender
19:13:37.066 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') area_code
19:13:37.067 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') longitude
19:13:37.068 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') latitude
19:13:37.070 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') matter_id
19:13:37.073 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') model_code
19:13:37.075 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') model_version
19:13:37.076 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
19:13:37.077 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
19:13:37.080 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
19:13:37.095 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:13:37.096 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:13:37.106 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:13:37.107 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:13:37.114 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:13:37.114 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:13:37.131 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:13:37.132 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:13:37.141 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:13:37.142 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:13:37.181 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:13:37.181 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:13:37.224 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:13:37.224 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:13:37.240 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:13:37.240 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:13:37.250 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:13:37.250 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:13:37.262 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:13:37.262 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:13:37.272 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:13:37.273 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:13:37.282 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:13:37.282 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:13:37.290 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:13:37.291 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:13:37.301 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:13:37.302 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:13:38.149 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:13:38.149 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:13:38.156 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:13:38.157 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:13:38.199 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:13:38.200 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:13:38.241 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:38.241 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:38.242 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:38.243 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:38.243 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:38.244 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:38.244 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:38.245 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:38.245 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:38.245 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:13:38.405 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
19:13:38.405 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
19:13:38.419 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
19:13:38.420 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
19:13:39.082 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
19:13:39.086 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
19:13:39.090 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
19:13:39.149 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
19:13:39.159 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
19:13:40.226 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
19:13:40.637 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 731.547 ms
19:13:41.434 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 100.5409 ms
19:13:41.973 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 264.7 KB, free 897.3 MB)
19:13:42.660 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.4 KB, free 897.3 MB)
19:13:42.664 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:10284 (size: 22.4 KB, free: 897.6 MB)
19:13:42.696 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:62
19:13:42.769 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19:13:43.176 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:62
19:13:43.300 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:62)
19:13:43.305 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:62) with 1 output partitions
19:13:43.306 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:62)
19:13:43.307 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
19:13:43.312 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
19:13:43.345 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62), which has no missing parents
19:13:43.451 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
19:13:43.455 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
19:13:43.456 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:10284 (size: 2.2 KB, free: 897.6 MB)
19:13:43.457 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
19:13:43.474 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62) (first 15 tasks are for partitions Vector(0))
19:13:43.476 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
19:13:43.634 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
19:13:43.699 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
19:13:43.906 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
19:13:43.925 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 32 ms
19:13:44.031 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1225 bytes result sent to driver
19:13:44.064 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 464 ms on localhost (executor driver) (1/1)
19:13:44.080 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
19:13:44.135 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:62) finished in 0.544 s
19:13:44.152 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:62, took 0.974852 s
19:13:44.225 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:62
19:13:44.227 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at DWReleaseCustomer.scala:62) with 3 output partitions
19:13:44.227 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at DWReleaseCustomer.scala:62)
19:13:44.227 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
19:13:44.227 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
19:13:44.228 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62), which has no missing parents
19:13:44.233 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
19:13:44.239 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
19:13:44.244 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:10284 (size: 2.2 KB, free: 897.6 MB)
19:13:44.245 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
19:13:44.250 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62) (first 15 tasks are for partitions Vector(1, 2, 3))
19:13:44.251 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
19:13:44.253 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
19:13:44.253 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
19:13:44.254 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
19:13:44.258 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
19:13:44.266 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
19:13:44.266 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
19:13:44.271 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1225 bytes result sent to driver
19:13:44.276 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
19:13:44.279 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
19:13:44.280 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 28 ms on localhost (executor driver) (1/3)
19:13:44.288 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
19:13:44.288 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
19:13:44.289 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1182 bytes result sent to driver
19:13:44.291 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
19:13:44.291 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
19:13:44.293 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1225 bytes result sent to driver
19:13:44.302 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 48 ms on localhost (executor driver) (2/3)
19:13:44.313 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 60 ms on localhost (executor driver) (3/3)
19:13:44.317 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at DWReleaseCustomer.scala:62) finished in 0.065 s
19:13:44.326 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at DWReleaseCustomer.scala:62, took 0.101682 s
19:13:44.328 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
19:13:44.425 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
19:13:44.530 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@41c528bc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:13:44.545 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
19:13:44.611 INFO  [dispatcher-event-loop-2] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
19:13:44.632 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
19:13:44.633 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
19:13:44.637 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
19:13:44.642 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
19:13:44.648 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
19:13:44.660 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
19:13:44.661 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-a18deedb-80fd-40bf-a9cf-8f2fa9c22df0
19:16:20.413 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
19:16:21.269 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
19:16:21.299 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
19:16:21.300 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
19:16:21.300 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
19:16:21.301 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
19:16:21.302 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
19:16:23.566 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 10343.
19:16:23.597 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
19:16:23.623 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
19:16:23.627 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19:16:23.628 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
19:16:23.641 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-92e350ca-4c50-47a6-98bf-93d048488049
19:16:23.667 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
19:16:23.765 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
19:16:24.034 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @9475ms
19:16:24.149 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
19:16:24.170 INFO  [main] org.spark_project.jetty.server.Server - Started @9615ms
19:16:24.210 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@243f2636{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:16:24.211 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
19:16:24.285 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/jobs,null,AVAILABLE,@Spark}
19:16:24.286 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/jobs/json,null,AVAILABLE,@Spark}
19:16:24.287 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/jobs/job,null,AVAILABLE,@Spark}
19:16:24.302 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/jobs/job/json,null,AVAILABLE,@Spark}
19:16:24.303 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ea1fda{/stages,null,AVAILABLE,@Spark}
19:16:24.305 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@420745d7{/stages/json,null,AVAILABLE,@Spark}
19:16:24.309 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fa47fea{/stages/stage,null,AVAILABLE,@Spark}
19:16:24.312 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545f80bf{/stages/stage/json,null,AVAILABLE,@Spark}
19:16:24.317 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22fa55b2{/stages/pool,null,AVAILABLE,@Spark}
19:16:24.320 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6594402a{/stages/pool/json,null,AVAILABLE,@Spark}
19:16:24.323 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@405325cf{/storage,null,AVAILABLE,@Spark}
19:16:24.324 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c3f01f{/storage/json,null,AVAILABLE,@Spark}
19:16:24.326 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/storage/rdd,null,AVAILABLE,@Spark}
19:16:24.331 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/storage/rdd/json,null,AVAILABLE,@Spark}
19:16:24.332 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/environment,null,AVAILABLE,@Spark}
19:16:24.333 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5298dead{/environment/json,null,AVAILABLE,@Spark}
19:16:24.334 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/executors,null,AVAILABLE,@Spark}
19:16:24.342 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/executors/json,null,AVAILABLE,@Spark}
19:16:24.344 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/executors/threadDump,null,AVAILABLE,@Spark}
19:16:24.345 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b986cd{/executors/threadDump/json,null,AVAILABLE,@Spark}
19:16:24.359 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41c62850{/static,null,AVAILABLE,@Spark}
19:16:24.360 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f574cc2{/,null,AVAILABLE,@Spark}
19:16:24.366 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a9c84a5{/api,null,AVAILABLE,@Spark}
19:16:24.367 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f1249d{/jobs/job/kill,null,AVAILABLE,@Spark}
19:16:24.368 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a68df9{/stages/stage/kill,null,AVAILABLE,@Spark}
19:16:24.370 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
19:16:24.672 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
19:16:24.740 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10364.
19:16:24.741 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:10364
19:16:24.743 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19:16:24.813 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 10364, None)
19:16:24.820 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:10364 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 10364, None)
19:16:24.828 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 10364, None)
19:16:24.829 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 10364, None)
19:16:25.370 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e850122{/metrics/json,null,AVAILABLE,@Spark}
19:16:29.418 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
19:16:29.463 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
19:16:29.464 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
19:16:29.475 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37a0ec3c{/SQL,null,AVAILABLE,@Spark}
19:16:29.475 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62a54948{/SQL/json,null,AVAILABLE,@Spark}
19:16:29.477 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b736fee{/SQL/execution,null,AVAILABLE,@Spark}
19:16:29.478 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35adf623{/SQL/execution/json,null,AVAILABLE,@Spark}
19:16:29.480 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2047981{/static/sql,null,AVAILABLE,@Spark}
19:16:29.966 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19:16:31.163 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19:16:31.206 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
19:16:34.017 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19:16:37.692 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
19:16:37.696 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
19:16:38.545 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
19:16:38.630 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
19:16:38.868 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
19:16:39.049 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
19:16:39.052 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
19:16:39.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
19:16:39.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
19:16:39.383 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
19:16:39.384 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19:16:39.411 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
19:16:39.411 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
19:16:39.436 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
19:16:39.437 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
19:16:39.460 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
19:16:39.460 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
19:16:39.484 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
19:16:39.484 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
19:16:39.551 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
19:16:39.551 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
19:16:39.579 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
19:16:39.580 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
19:16:41.421 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/2a7899bc-961b-4ddb-9f89-d171e02c18b8_resources
19:16:41.443 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/2a7899bc-961b-4ddb-9f89-d171e02c18b8
19:16:41.447 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/2a7899bc-961b-4ddb-9f89-d171e02c18b8
19:16:41.566 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/2a7899bc-961b-4ddb-9f89-d171e02c18b8/_tmp_space.db
19:16:41.572 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
19:16:41.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:16:41.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:16:41.636 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
19:16:41.636 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
19:16:41.668 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
19:16:42.065 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/e10a4812-78b8-4168-8998-90ecd4092116_resources
19:16:42.073 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/e10a4812-78b8-4168-8998-90ecd4092116
19:16:42.079 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/e10a4812-78b8-4168-8998-90ecd4092116
19:16:42.113 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/e10a4812-78b8-4168-8998-90ecd4092116/_tmp_space.db
19:16:42.128 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
19:16:42.173 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
19:16:42.183 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
19:16:42.567 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:16:42.568 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:16:42.595 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:16:42.595 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:16:42.830 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:16:42.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:16:42.930 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:42.950 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:42.950 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:42.951 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:42.951 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:42.951 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:42.951 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:42.952 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:42.952 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:42.952 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:16:42.974 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
19:16:43.692 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
19:16:43.719 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
19:16:43.719 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
19:16:43.723 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
19:16:43.724 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
19:16:43.725 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
19:16:43.726 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') idcard
19:16:43.758 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy') as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as age
19:16:43.833 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})',2) as int)%2 as gender
19:16:43.837 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') area_code
19:16:43.838 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') longitude
19:16:43.842 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') latitude
19:16:43.844 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') matter_id
19:16:43.846 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') model_code
19:16:43.848 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') model_version
19:16:43.852 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
19:16:43.855 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
19:16:43.858 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
19:16:43.982 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:16:43.983 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:16:44.012 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:16:44.013 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:16:44.034 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:16:44.035 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:16:44.064 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:16:44.065 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:16:44.090 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:16:44.091 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:16:44.177 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:16:44.177 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:16:44.199 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:16:44.199 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:16:44.580 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:16:44.580 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:16:44.616 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:16:44.616 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:16:44.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:16:44.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:16:44.666 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:16:44.666 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:16:44.690 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:16:44.690 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:16:44.716 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:16:44.716 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:16:44.743 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:16:44.743 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:16:45.295 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:16:45.295 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:16:45.338 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:16:45.338 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:16:45.418 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:16:45.419 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:16:45.501 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:45.502 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:45.502 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:45.503 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:45.503 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:45.504 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:45.504 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:45.504 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:45.504 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:16:45.505 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:16:45.553 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
19:16:45.553 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
19:16:45.562 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
19:16:45.562 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
19:16:45.823 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
19:16:45.827 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
19:16:45.834 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
19:16:45.855 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
19:16:45.866 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
19:16:47.353 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 1157.5435 ms
19:16:48.421 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 195.5584 ms
19:16:48.676 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 264.7 KB, free 897.3 MB)
19:16:49.216 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.4 KB, free 897.3 MB)
19:16:49.222 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:10364 (size: 22.4 KB, free: 897.6 MB)
19:16:49.232 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:62
19:16:49.252 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19:16:49.573 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:62
19:16:49.636 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:62)
19:16:49.658 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:62) with 1 output partitions
19:16:49.660 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:62)
19:16:49.662 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
19:16:49.674 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
19:16:49.692 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62), which has no missing parents
19:16:49.796 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
19:16:49.862 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
19:16:49.864 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:10364 (size: 2.2 KB, free: 897.6 MB)
19:16:49.865 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
19:16:49.898 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62) (first 15 tasks are for partitions Vector(0))
19:16:49.900 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
19:16:49.919 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
19:16:50.023 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
19:16:50.076 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
19:16:50.725 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
19:16:50.727 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 9 ms
19:16:50.783 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1268 bytes result sent to driver
19:16:50.841 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 848 ms on localhost (executor driver) (1/1)
19:16:50.847 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
19:16:50.856 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:62) finished in 0.885 s
19:16:50.868 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:62, took 1.294174 s
19:16:50.885 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:62
19:16:50.892 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at DWReleaseCustomer.scala:62) with 3 output partitions
19:16:50.892 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at DWReleaseCustomer.scala:62)
19:16:50.892 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
19:16:50.893 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
19:16:50.894 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62), which has no missing parents
19:16:50.915 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
19:16:50.921 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
19:16:50.922 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:10364 (size: 2.2 KB, free: 897.6 MB)
19:16:50.923 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
19:16:50.926 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62) (first 15 tasks are for partitions Vector(1, 2, 3))
19:16:50.926 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
19:16:50.927 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
19:16:50.927 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
19:16:50.927 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
19:16:50.929 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
19:16:51.058 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
19:16:51.058 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
19:16:51.073 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1268 bytes result sent to driver
19:16:51.224 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
19:16:51.224 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
19:16:51.225 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 298 ms on localhost (executor driver) (1/3)
19:16:51.237 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
19:16:51.237 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
19:16:51.238 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1182 bytes result sent to driver
19:16:51.240 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
19:16:51.241 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
19:16:51.243 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1225 bytes result sent to driver
19:16:51.249 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 322 ms on localhost (executor driver) (2/3)
19:16:51.249 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 322 ms on localhost (executor driver) (3/3)
19:16:51.250 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
19:16:51.250 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at DWReleaseCustomer.scala:62) finished in 0.324 s
19:16:51.251 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at DWReleaseCustomer.scala:62, took 0.366200 s
19:16:51.268 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
19:16:51.280 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@243f2636{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:16:51.285 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
19:16:51.306 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
19:16:51.327 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
19:16:51.329 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
19:16:51.331 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
19:16:51.335 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
19:16:51.340 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
19:16:51.341 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
19:16:51.342 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-f19d0779-c6a1-468e-826f-fb84a92970ce
19:57:57.357 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
19:57:59.827 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
19:58:00.281 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
19:58:00.282 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
19:58:00.313 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
19:58:00.314 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
19:58:00.330 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
19:58:02.839 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 11143.
19:58:02.905 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
19:58:02.972 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
19:58:02.990 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19:58:02.991 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
19:58:03.011 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-6ffdb01e-5d8d-45c6-86d6-151533074c57
19:58:03.122 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
19:58:03.210 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
19:58:03.491 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @12770ms
19:58:03.643 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
19:58:03.670 INFO  [main] org.spark_project.jetty.server.Server - Started @12952ms
19:58:03.726 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@ecd8ada{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:58:03.727 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
19:58:03.776 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/jobs,null,AVAILABLE,@Spark}
19:58:03.780 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/jobs/json,null,AVAILABLE,@Spark}
19:58:03.781 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/jobs/job,null,AVAILABLE,@Spark}
19:58:03.783 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/jobs/job/json,null,AVAILABLE,@Spark}
19:58:03.784 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/stages,null,AVAILABLE,@Spark}
19:58:03.785 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ea1fda{/stages/json,null,AVAILABLE,@Spark}
19:58:03.786 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@420745d7{/stages/stage,null,AVAILABLE,@Spark}
19:58:03.788 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b43e173{/stages/stage/json,null,AVAILABLE,@Spark}
19:58:03.789 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545f80bf{/stages/pool,null,AVAILABLE,@Spark}
19:58:03.791 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22fa55b2{/stages/pool/json,null,AVAILABLE,@Spark}
19:58:03.795 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6594402a{/storage,null,AVAILABLE,@Spark}
19:58:03.797 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@405325cf{/storage/json,null,AVAILABLE,@Spark}
19:58:03.798 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c3f01f{/storage/rdd,null,AVAILABLE,@Spark}
19:58:03.799 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/storage/rdd/json,null,AVAILABLE,@Spark}
19:58:03.800 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/environment,null,AVAILABLE,@Spark}
19:58:03.801 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/environment/json,null,AVAILABLE,@Spark}
19:58:03.813 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5298dead{/executors,null,AVAILABLE,@Spark}
19:58:03.818 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/executors/json,null,AVAILABLE,@Spark}
19:58:03.820 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/executors/threadDump,null,AVAILABLE,@Spark}
19:58:03.821 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/executors/threadDump/json,null,AVAILABLE,@Spark}
19:58:03.838 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b986cd{/static,null,AVAILABLE,@Spark}
19:58:03.840 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c09d180{/,null,AVAILABLE,@Spark}
19:58:03.841 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f574cc2{/api,null,AVAILABLE,@Spark}
19:58:03.842 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60222fd8{/jobs/job/kill,null,AVAILABLE,@Spark}
19:58:03.843 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f1249d{/stages/stage/kill,null,AVAILABLE,@Spark}
19:58:03.847 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
19:58:04.257 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
19:58:04.361 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11164.
19:58:04.362 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:11164
19:58:04.365 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19:58:04.416 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 11164, None)
19:58:04.426 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:11164 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 11164, None)
19:58:04.433 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 11164, None)
19:58:04.434 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 11164, None)
19:58:04.905 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@151335cb{/metrics/json,null,AVAILABLE,@Spark}
19:58:08.758 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
19:58:08.792 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
19:58:08.792 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
19:58:08.805 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c719bd4{/SQL,null,AVAILABLE,@Spark}
19:58:08.805 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37a0ec3c{/SQL/json,null,AVAILABLE,@Spark}
19:58:08.806 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44286963{/SQL/execution,null,AVAILABLE,@Spark}
19:58:08.806 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b736fee{/SQL/execution/json,null,AVAILABLE,@Spark}
19:58:08.815 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5232e3f1{/static/sql,null,AVAILABLE,@Spark}
19:58:09.596 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19:58:10.957 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19:58:11.022 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
19:58:13.032 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19:58:26.538 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
19:58:26.571 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
19:58:27.221 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
19:58:27.227 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
19:58:27.368 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
19:58:27.680 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
19:58:27.682 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
19:58:27.734 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
19:58:27.735 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
19:58:27.835 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
19:58:27.836 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19:58:27.842 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
19:58:27.842 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
19:58:27.848 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
19:58:27.848 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
19:58:27.857 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
19:58:27.857 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
19:58:27.865 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
19:58:27.865 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
19:58:27.870 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
19:58:27.871 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
19:58:27.876 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
19:58:27.877 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
19:58:29.785 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/6d70e69c-2938-454a-a12d-4fdd71f8f1ca_resources
19:58:29.801 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/6d70e69c-2938-454a-a12d-4fdd71f8f1ca
19:58:29.812 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/6d70e69c-2938-454a-a12d-4fdd71f8f1ca
19:58:29.829 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/6d70e69c-2938-454a-a12d-4fdd71f8f1ca/_tmp_space.db
19:58:29.836 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
19:58:29.954 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:58:29.954 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:58:29.974 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
19:58:29.975 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
19:58:29.980 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
19:58:30.427 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/fb3409a9-93a8-407d-8879-017bc1929330_resources
19:58:30.435 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/fb3409a9-93a8-407d-8879-017bc1929330
19:58:30.446 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/fb3409a9-93a8-407d-8879-017bc1929330
19:58:30.467 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/fb3409a9-93a8-407d-8879-017bc1929330/_tmp_space.db
19:58:30.474 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
19:58:30.652 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
19:58:30.686 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
19:58:31.340 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:58:31.340 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:58:31.349 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:58:31.350 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:58:31.713 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:58:31.714 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:58:31.774 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:31.817 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:31.817 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:31.818 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:31.818 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:31.818 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:31.818 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:31.819 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:31.820 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:31.820 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:58:31.861 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
19:58:33.113 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
19:58:33.135 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
19:58:33.138 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
19:58:33.139 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
19:58:33.141 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
19:58:33.142 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
19:58:33.143 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') idcard
19:58:33.196 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy') as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as age
19:58:33.256 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})',2) as int)%2 as gender
19:58:33.260 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') area_code
19:58:33.261 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') longitude
19:58:33.263 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') latitude
19:58:33.267 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') matter_id
19:58:33.268 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') model_code
19:58:33.269 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') model_version
19:58:33.270 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
19:58:33.271 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
19:58:33.272 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
19:58:33.279 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:58:33.279 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:58:33.293 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:58:33.294 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:58:33.320 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:58:33.320 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:58:33.339 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:58:33.340 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:58:33.424 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:58:33.425 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:58:33.455 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:58:33.455 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:58:33.469 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:58:33.469 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:58:33.490 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:58:33.491 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:58:33.506 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:58:33.506 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:58:33.520 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:58:33.521 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:58:33.597 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:58:33.597 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:58:33.619 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:58:33.619 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:58:33.724 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:58:33.725 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:58:33.731 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:58:33.731 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:58:41.790 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:58:41.790 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:58:41.799 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:58:41.799 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:58:41.844 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:58:41.844 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:58:41.886 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:41.886 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:41.886 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:41.887 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:41.887 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:41.887 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:41.888 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:41.888 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:41.889 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:58:41.889 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:58:42.006 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
19:58:42.006 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
19:58:42.051 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
19:58:42.051 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
19:58:42.570 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
19:58:42.580 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
19:58:42.585 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
19:58:42.659 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
19:58:42.681 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
19:58:44.101 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 581.9587 ms
19:58:44.778 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 102.8465 ms
19:58:45.810 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 264.7 KB, free 897.3 MB)
19:58:46.649 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.4 KB, free 897.3 MB)
19:58:46.655 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:11164 (size: 22.4 KB, free: 897.6 MB)
19:58:46.706 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:62
19:58:46.845 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19:58:47.574 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:62
19:58:47.718 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:62)
19:58:47.746 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:62) with 1 output partitions
19:58:47.747 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:62)
19:58:47.748 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
19:58:47.753 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
19:58:47.791 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62), which has no missing parents
19:58:48.021 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
19:58:48.033 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
19:58:48.033 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:11164 (size: 2.2 KB, free: 897.6 MB)
19:58:48.034 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
19:58:48.084 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62) (first 15 tasks are for partitions Vector(0))
19:58:48.092 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
19:58:48.315 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
19:58:48.385 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
19:58:48.673 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
19:58:48.692 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 41 ms
19:58:48.852 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1268 bytes result sent to driver
19:58:48.897 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 609 ms on localhost (executor driver) (1/1)
19:58:48.915 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
19:58:48.951 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:62) finished in 0.748 s
19:58:49.012 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:62, took 1.436968 s
19:58:49.067 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:62
19:58:49.069 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at DWReleaseCustomer.scala:62) with 3 output partitions
19:58:49.069 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at DWReleaseCustomer.scala:62)
19:58:49.069 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
19:58:49.069 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
19:58:49.070 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62), which has no missing parents
19:58:49.072 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
19:58:49.079 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
19:58:49.090 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:11164 (size: 2.2 KB, free: 897.6 MB)
19:58:49.091 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
19:58:49.094 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62) (first 15 tasks are for partitions Vector(1, 2, 3))
19:58:49.094 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
19:58:49.095 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
19:58:49.096 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
19:58:49.097 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
19:58:49.097 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
19:58:49.098 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
19:58:49.098 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
19:58:49.121 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
19:58:49.122 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
19:58:49.122 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
19:58:49.122 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
19:58:49.125 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1225 bytes result sent to driver
19:58:49.126 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1182 bytes result sent to driver
19:58:49.129 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
19:58:49.129 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
19:58:49.133 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1225 bytes result sent to driver
19:58:49.134 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 39 ms on localhost (executor driver) (1/3)
19:58:49.135 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 39 ms on localhost (executor driver) (2/3)
19:58:49.136 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 40 ms on localhost (executor driver) (3/3)
19:58:49.136 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
19:58:49.139 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at DWReleaseCustomer.scala:62) finished in 0.045 s
19:58:49.140 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at DWReleaseCustomer.scala:62, took 0.071904 s
19:58:49.215 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
19:58:49.334 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@ecd8ada{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:58:49.382 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
19:58:49.515 INFO  [dispatcher-event-loop-1] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
19:58:49.577 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
19:58:49.594 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
19:58:49.595 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
19:58:49.600 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
19:58:49.614 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
19:58:49.629 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
19:58:49.630 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-38663d2b-b9b7-4c0e-8d55-8943d746382f
