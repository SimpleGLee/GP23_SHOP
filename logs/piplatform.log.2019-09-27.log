10:58:09.912 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
10:58:10.917 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
10:58:11.060 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
10:58:11.062 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
10:58:11.073 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
10:58:11.074 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
10:58:11.075 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
10:58:13.255 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 13985.
10:58:13.366 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
10:58:13.501 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
10:58:13.517 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
10:58:13.518 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
10:58:13.644 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-2ead3f52-ba67-4713-b2e0-d56cb4e2b13e
10:58:14.001 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
10:58:14.204 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
10:58:14.584 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @26619ms
10:58:14.890 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
10:58:14.911 INFO  [main] org.spark_project.jetty.server.Server - Started @26948ms
10:58:14.962 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@6f0cbf8c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
10:58:14.963 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
10:58:15.011 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62df0ff3{/jobs,null,AVAILABLE,@Spark}
10:58:15.011 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/jobs/json,null,AVAILABLE,@Spark}
10:58:15.012 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/jobs/job,null,AVAILABLE,@Spark}
10:58:15.014 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/jobs/job/json,null,AVAILABLE,@Spark}
10:58:15.014 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/stages,null,AVAILABLE,@Spark}
10:58:15.015 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e11ab3d{/stages/json,null,AVAILABLE,@Spark}
10:58:15.016 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2392212b{/stages/stage,null,AVAILABLE,@Spark}
10:58:15.020 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66f66866{/stages/stage/json,null,AVAILABLE,@Spark}
10:58:15.021 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d666b41{/stages/pool,null,AVAILABLE,@Spark}
10:58:15.022 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30f4b1a6{/stages/pool/json,null,AVAILABLE,@Spark}
10:58:15.023 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e1162e7{/storage,null,AVAILABLE,@Spark}
10:58:15.024 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2f1700{/storage/json,null,AVAILABLE,@Spark}
10:58:15.025 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/storage/rdd,null,AVAILABLE,@Spark}
10:58:15.028 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/storage/rdd/json,null,AVAILABLE,@Spark}
10:58:15.029 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/environment,null,AVAILABLE,@Spark}
10:58:15.030 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@553f3b6e{/environment/json,null,AVAILABLE,@Spark}
10:58:15.032 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/executors,null,AVAILABLE,@Spark}
10:58:15.033 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/executors/json,null,AVAILABLE,@Spark}
10:58:15.034 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/executors/threadDump,null,AVAILABLE,@Spark}
10:58:15.044 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bb7cce7{/executors/threadDump/json,null,AVAILABLE,@Spark}
10:58:15.060 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b530eb9{/static,null,AVAILABLE,@Spark}
10:58:15.061 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@680bddf5{/,null,AVAILABLE,@Spark}
10:58:15.064 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d83c5a5{/api,null,AVAILABLE,@Spark}
10:58:15.065 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@710b30ef{/jobs/job/kill,null,AVAILABLE,@Spark}
10:58:15.066 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28b576a9{/stages/stage/kill,null,AVAILABLE,@Spark}
10:58:15.070 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
10:58:15.459 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
10:58:15.580 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14006.
10:58:15.583 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:14006
10:58:15.591 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
10:58:15.662 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 14006, None)
10:58:15.703 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:14006 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 14006, None)
10:58:15.717 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 14006, None)
10:58:15.718 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 14006, None)
10:58:16.208 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27fde870{/metrics/json,null,AVAILABLE,@Spark}
10:58:20.532 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
10:58:20.598 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
10:58:20.599 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
10:58:20.632 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53aa38be{/SQL,null,AVAILABLE,@Spark}
10:58:20.632 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@422ad5e2{/SQL/json,null,AVAILABLE,@Spark}
10:58:20.634 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c8f97a7{/SQL/execution,null,AVAILABLE,@Spark}
10:58:20.635 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3249e278{/SQL/execution/json,null,AVAILABLE,@Spark}
10:58:20.658 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c8fe7a4{/static/sql,null,AVAILABLE,@Spark}
10:58:22.313 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
10:58:24.129 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
10:58:24.261 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
10:58:27.852 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
10:58:31.179 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
10:58:31.182 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
10:58:31.887 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
10:58:31.895 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
10:58:32.026 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
10:58:32.315 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
10:58:32.318 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
10:58:32.427 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
10:58:32.427 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
10:58:32.526 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
10:58:32.526 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
10:58:32.534 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
10:58:32.535 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
10:58:32.984 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
10:58:32.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
10:58:32.991 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
10:58:32.991 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
10:58:32.997 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
10:58:32.997 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
10:58:33.003 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
10:58:33.003 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
10:58:33.009 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
10:58:33.009 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
10:58:36.141 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/eaca5e14-b227-40ee-9ff9-63381ec51f09_resources
10:58:36.257 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/eaca5e14-b227-40ee-9ff9-63381ec51f09
10:58:36.261 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/eaca5e14-b227-40ee-9ff9-63381ec51f09
10:58:36.315 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/eaca5e14-b227-40ee-9ff9-63381ec51f09/_tmp_space.db
10:58:36.353 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
10:58:36.523 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
10:58:36.523 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
10:58:36.532 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
10:58:36.532 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
10:58:36.539 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
10:58:36.954 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/58ea3b5c-0721-4d13-b016-f64b76b7e8c9_resources
10:58:36.962 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/58ea3b5c-0721-4d13-b016-f64b76b7e8c9
10:58:36.967 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/58ea3b5c-0721-4d13-b016-f64b76b7e8c9
10:58:36.999 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/58ea3b5c-0721-4d13-b016-f64b76b7e8c9/_tmp_space.db
10:58:37.011 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
10:58:37.434 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
10:58:37.477 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
10:58:38.266 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
10:58:38.266 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
10:58:38.313 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
10:58:38.314 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
10:58:38.784 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
10:58:38.785 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
10:58:38.837 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:38.882 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:38.883 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:38.883 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:38.884 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:38.884 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:38.884 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:38.885 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:38.885 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:38.886 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
10:58:38.985 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
10:58:40.263 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
10:58:40.314 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
10:58:40.322 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
10:58:40.325 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
10:58:40.366 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
10:58:40.367 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
10:58:40.370 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') idcard
10:58:40.622 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy') as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as age
10:58:40.966 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})',2) as int)%2 as gender
10:58:41.024 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') area_code
10:58:41.026 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') longitude
10:58:41.032 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') latitude
10:58:41.036 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') matter_id
10:58:41.041 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') model_code
10:58:41.048 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') model_version
10:58:41.049 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
10:58:41.052 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
10:58:41.054 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
10:58:41.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
10:58:41.081 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
10:58:41.103 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
10:58:41.104 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
10:58:41.118 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
10:58:41.118 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
10:58:41.133 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
10:58:41.133 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
10:58:41.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
10:58:41.149 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
10:58:41.159 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
10:58:41.160 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
10:58:41.171 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
10:58:41.171 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
10:58:41.211 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
10:58:41.211 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
10:58:41.224 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
10:58:41.225 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
10:58:41.243 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
10:58:41.243 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
10:58:41.303 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
10:58:41.304 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
10:58:41.353 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
10:58:41.354 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
10:58:41.443 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
10:58:41.444 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
10:58:41.479 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
10:58:41.479 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
10:58:42.638 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
10:58:42.638 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
10:58:42.646 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
10:58:42.646 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
10:58:42.689 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
10:58:42.691 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
10:58:42.766 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:42.767 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:42.767 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:42.767 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:42.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:42.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:42.769 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:42.769 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:42.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
10:58:42.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
10:58:42.941 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
10:58:42.941 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
10:58:42.956 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
10:58:42.956 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
10:58:43.505 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
10:58:43.508 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
10:58:43.511 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
10:58:43.688 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
10:58:43.724 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
10:58:45.541 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 451.7202 ms
10:58:45.959 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
10:58:46.478 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 195.1723 ms
10:58:46.960 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 264.7 KB, free 897.3 MB)
10:58:47.327 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.4 KB, free 897.3 MB)
10:58:47.336 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:14006 (size: 22.4 KB, free: 897.6 MB)
10:58:47.351 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:62
10:58:47.400 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
10:58:47.746 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:62
10:58:47.847 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:62)
10:58:47.851 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:62) with 1 output partitions
10:58:47.852 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:62)
10:58:47.852 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
10:58:47.854 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
10:58:47.878 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62), which has no missing parents
10:58:48.033 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
10:58:48.042 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
10:58:48.044 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:14006 (size: 2.2 KB, free: 897.6 MB)
10:58:48.047 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
10:58:48.084 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62) (first 15 tasks are for partitions Vector(0))
10:58:48.086 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
10:58:48.203 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
10:58:48.230 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
10:58:48.462 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
10:58:48.469 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 21 ms
10:58:48.576 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1268 bytes result sent to driver
10:58:48.610 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 422 ms on localhost (executor driver) (1/1)
10:58:48.624 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
10:58:48.633 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:62) finished in 0.480 s
10:58:48.675 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:62, took 0.928970 s
10:58:48.724 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:62
10:58:48.726 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at DWReleaseCustomer.scala:62) with 3 output partitions
10:58:48.726 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at DWReleaseCustomer.scala:62)
10:58:48.726 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
10:58:48.726 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
10:58:48.728 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62), which has no missing parents
10:58:48.739 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
10:58:48.746 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
10:58:48.755 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:14006 (size: 2.2 KB, free: 897.6 MB)
10:58:48.756 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
10:58:48.759 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62) (first 15 tasks are for partitions Vector(1, 2, 3))
10:58:48.759 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
10:58:48.761 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
10:58:48.762 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
10:58:48.764 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
10:58:48.765 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
10:58:48.772 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
10:58:48.772 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
10:58:48.775 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1182 bytes result sent to driver
10:58:48.784 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
10:58:48.784 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
10:58:48.787 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 27 ms on localhost (executor driver) (1/3)
10:58:48.801 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
10:58:48.801 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
10:58:48.803 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1182 bytes result sent to driver
10:58:48.812 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
10:58:48.813 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
10:58:48.816 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1182 bytes result sent to driver
10:58:48.839 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 75 ms on localhost (executor driver) (2/3)
10:58:48.845 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 84 ms on localhost (executor driver) (3/3)
10:58:48.846 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
10:58:48.846 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at DWReleaseCustomer.scala:62) finished in 0.086 s
10:58:48.847 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at DWReleaseCustomer.scala:62, took 0.122777 s
10:58:48.916 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
10:58:48.942 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@6f0cbf8c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
10:58:48.947 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
10:58:48.999 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
10:58:49.030 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
10:58:49.032 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
10:58:49.037 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
10:58:49.045 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
10:58:49.048 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
10:58:49.061 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
10:58:49.064 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-9d438d80-0723-4eb0-b572-0dc75d6002a6
11:02:56.603 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
11:03:19.660 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
11:03:20.387 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
11:03:20.392 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
11:03:20.403 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
11:03:20.405 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
11:03:20.418 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
11:03:23.514 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 14086.
11:03:23.614 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
11:03:23.749 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
11:03:23.766 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
11:03:23.767 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
11:03:23.795 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-57aaaec6-f50c-4c02-ad53-7a63b232dc2a
11:03:23.892 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
11:03:24.049 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
11:03:24.460 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @58511ms
11:03:24.739 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
11:03:24.771 INFO  [main] org.spark_project.jetty.server.Server - Started @58824ms
11:03:24.933 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4bbd4ed{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:03:24.934 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
11:03:24.991 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/jobs,null,AVAILABLE,@Spark}
11:03:24.992 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/jobs/json,null,AVAILABLE,@Spark}
11:03:24.993 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/jobs/job,null,AVAILABLE,@Spark}
11:03:24.994 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/jobs/job/json,null,AVAILABLE,@Spark}
11:03:24.995 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ea1fda{/stages,null,AVAILABLE,@Spark}
11:03:24.995 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@420745d7{/stages/json,null,AVAILABLE,@Spark}
11:03:24.996 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fa47fea{/stages/stage,null,AVAILABLE,@Spark}
11:03:24.998 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545f80bf{/stages/stage/json,null,AVAILABLE,@Spark}
11:03:24.999 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22fa55b2{/stages/pool,null,AVAILABLE,@Spark}
11:03:25.000 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6594402a{/stages/pool/json,null,AVAILABLE,@Spark}
11:03:25.001 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@405325cf{/storage,null,AVAILABLE,@Spark}
11:03:25.002 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c3f01f{/storage/json,null,AVAILABLE,@Spark}
11:03:25.007 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/storage/rdd,null,AVAILABLE,@Spark}
11:03:25.008 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/storage/rdd/json,null,AVAILABLE,@Spark}
11:03:25.009 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/environment,null,AVAILABLE,@Spark}
11:03:25.010 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5298dead{/environment/json,null,AVAILABLE,@Spark}
11:03:25.012 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/executors,null,AVAILABLE,@Spark}
11:03:25.013 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/executors/json,null,AVAILABLE,@Spark}
11:03:25.017 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/executors/threadDump,null,AVAILABLE,@Spark}
11:03:25.019 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b986cd{/executors/threadDump/json,null,AVAILABLE,@Spark}
11:03:25.056 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41c62850{/static,null,AVAILABLE,@Spark}
11:03:25.058 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f574cc2{/,null,AVAILABLE,@Spark}
11:03:25.059 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a9c84a5{/api,null,AVAILABLE,@Spark}
11:03:25.060 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f1249d{/jobs/job/kill,null,AVAILABLE,@Spark}
11:03:25.061 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a68df9{/stages/stage/kill,null,AVAILABLE,@Spark}
11:03:25.109 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
11:03:25.442 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
11:03:25.569 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14107.
11:03:25.576 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:14107
11:03:25.593 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
11:03:25.666 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 14107, None)
11:03:25.695 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:14107 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 14107, None)
11:03:25.715 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 14107, None)
11:03:25.715 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 14107, None)
11:03:26.191 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e850122{/metrics/json,null,AVAILABLE,@Spark}
11:03:30.785 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
11:03:30.847 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
11:03:30.848 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
11:03:30.888 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c719bd4{/SQL,null,AVAILABLE,@Spark}
11:03:30.889 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37a0ec3c{/SQL/json,null,AVAILABLE,@Spark}
11:03:30.891 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44286963{/SQL/execution,null,AVAILABLE,@Spark}
11:03:30.892 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b736fee{/SQL/execution/json,null,AVAILABLE,@Spark}
11:03:30.897 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5232e3f1{/static/sql,null,AVAILABLE,@Spark}
11:03:32.131 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
11:03:33.696 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
11:03:33.892 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
11:03:41.526 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
11:03:43.994 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
11:03:43.997 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
11:03:44.595 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
11:03:44.602 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
11:03:44.709 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
11:03:44.965 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
11:03:44.967 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
11:03:45.023 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
11:03:45.023 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
11:03:45.129 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
11:03:45.129 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
11:03:45.136 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
11:03:45.136 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
11:03:45.142 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
11:03:45.142 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
11:03:45.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
11:03:45.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
11:03:45.154 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
11:03:45.155 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
11:03:45.163 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
11:03:45.163 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
11:03:45.171 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
11:03:45.172 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
11:03:47.088 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/f57703c8-c6b8-461f-a927-066f6eabda1d_resources
11:03:47.114 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/f57703c8-c6b8-461f-a927-066f6eabda1d
11:03:47.117 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/f57703c8-c6b8-461f-a927-066f6eabda1d
11:03:47.158 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/f57703c8-c6b8-461f-a927-066f6eabda1d/_tmp_space.db
11:03:47.200 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
11:03:47.337 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:03:47.337 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:03:47.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
11:03:47.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
11:03:47.353 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
11:03:47.795 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/8c365189-fa98-4105-a77f-5d50fb6835d6_resources
11:03:47.808 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/8c365189-fa98-4105-a77f-5d50fb6835d6
11:03:47.812 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/8c365189-fa98-4105-a77f-5d50fb6835d6
11:03:47.820 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/8c365189-fa98-4105-a77f-5d50fb6835d6/_tmp_space.db
11:03:47.826 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
11:03:48.050 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
11:03:48.071 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:03:48.570 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:03:48.570 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:03:48.595 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:03:48.595 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:03:48.924 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:03:48.924 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:03:48.997 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:49.041 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:49.042 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:49.042 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:49.042 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:49.042 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:49.043 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:49.043 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:49.043 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:49.044 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:03:49.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
11:03:50.567 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:03:50.586 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:03:50.588 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:03:50.589 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:03:50.589 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:03:50.590 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:03:50.591 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') idcard
11:03:50.635 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy') as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as age
11:03:50.687 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})',2) as int)%2 as gender
11:03:50.690 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') area_code
11:03:50.692 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') longitude
11:03:50.693 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') latitude
11:03:50.695 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') matter_id
11:03:50.696 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') model_code
11:03:50.697 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') model_version
11:03:50.698 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
11:03:50.699 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:03:50.700 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:03:50.717 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:03:50.718 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:03:50.731 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:03:50.732 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:03:50.747 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:03:50.748 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:03:50.758 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:03:50.759 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:03:50.770 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:03:50.770 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:03:50.792 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:03:50.792 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:03:50.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:03:50.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:03:50.838 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:03:50.839 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:03:50.850 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:03:50.850 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:03:50.861 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:03:50.862 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:03:50.869 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:03:50.870 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:03:50.883 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:03:50.883 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:03:50.895 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:03:50.895 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:03:50.908 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:03:50.909 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:03:51.954 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:03:51.955 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:03:51.965 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:03:51.965 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:03:52.023 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:03:52.023 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:03:52.080 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:52.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:52.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:52.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:52.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:52.083 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:52.083 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:52.083 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:52.084 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:03:52.084 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:03:52.237 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:03:52.238 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:03:52.253 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:03:52.253 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:03:52.824 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
11:03:52.828 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
11:03:52.831 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:03:52.902 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:03:52.917 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:03:54.756 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 1288.8126 ms
11:03:55.032 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
11:03:55.524 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 114.7101 ms
11:03:55.926 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 264.7 KB, free 897.3 MB)
11:03:56.191 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.4 KB, free 897.3 MB)
11:03:56.195 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:14107 (size: 22.4 KB, free: 897.6 MB)
11:03:56.216 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:62
11:03:56.268 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:03:56.586 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:62
11:03:56.710 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:62)
11:03:56.713 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:62) with 1 output partitions
11:03:56.713 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:62)
11:03:56.714 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
11:03:56.717 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:03:56.756 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62), which has no missing parents
11:03:56.862 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
11:03:56.866 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
11:03:56.867 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:14107 (size: 2.2 KB, free: 897.6 MB)
11:03:56.868 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
11:03:56.896 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62) (first 15 tasks are for partitions Vector(0))
11:03:56.898 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
11:03:57.012 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:03:57.077 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
11:03:57.292 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:03:57.301 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 25 ms
11:03:57.422 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1311 bytes result sent to driver
11:03:57.453 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 462 ms on localhost (executor driver) (1/1)
11:03:57.468 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
11:03:57.477 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:62) finished in 0.519 s
11:03:57.541 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:62, took 0.955473 s
11:03:57.598 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:62
11:03:57.600 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at DWReleaseCustomer.scala:62) with 3 output partitions
11:03:57.600 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at DWReleaseCustomer.scala:62)
11:03:57.600 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
11:03:57.601 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:03:57.602 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62), which has no missing parents
11:03:57.612 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
11:03:57.623 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
11:03:57.628 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:14107 (size: 2.2 KB, free: 897.6 MB)
11:03:57.630 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
11:03:57.634 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62) (first 15 tasks are for partitions Vector(1, 2, 3))
11:03:57.634 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
11:03:57.639 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:03:57.640 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:03:57.641 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:03:57.642 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
11:03:57.651 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:03:57.652 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:03:57.654 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1225 bytes result sent to driver
11:03:57.673 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
11:03:57.674 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
11:03:57.677 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 38 ms on localhost (executor driver) (1/3)
11:03:57.689 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:03:57.690 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:03:57.694 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1182 bytes result sent to driver
11:03:57.696 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:03:57.696 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:03:57.698 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1225 bytes result sent to driver
11:03:57.709 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 68 ms on localhost (executor driver) (2/3)
11:03:57.712 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 72 ms on localhost (executor driver) (3/3)
11:03:57.712 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
11:03:57.716 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at DWReleaseCustomer.scala:62) finished in 0.077 s
11:03:57.717 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at DWReleaseCustomer.scala:62, took 0.117709 s
11:03:57.868 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
11:03:57.885 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@4bbd4ed{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:03:57.898 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
11:03:57.962 INFO  [dispatcher-event-loop-1] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
11:03:57.988 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
11:03:57.989 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
11:03:57.991 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
11:03:57.997 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
11:03:58.009 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
11:03:58.015 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
11:03:58.017 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-1bcc3df1-befd-4151-a04a-150acbb76f02
11:20:40.490 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
11:20:41.562 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
11:20:41.694 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
11:20:41.695 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
11:20:41.696 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
11:20:41.696 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
11:20:41.698 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
11:20:44.145 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 14432.
11:20:44.244 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
11:20:44.344 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
11:20:44.361 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
11:20:44.362 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
11:20:44.386 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-cfe5e158-5ecc-495b-ac3e-33865b2cadda
11:20:44.489 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
11:20:44.638 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
11:20:44.993 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @13921ms
11:20:45.179 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
11:20:45.210 INFO  [main] org.spark_project.jetty.server.Server - Started @14151ms
11:20:45.254 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@54f5f647{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:20:45.255 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
11:20:45.303 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62df0ff3{/jobs,null,AVAILABLE,@Spark}
11:20:45.304 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/jobs/json,null,AVAILABLE,@Spark}
11:20:45.305 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/jobs/job,null,AVAILABLE,@Spark}
11:20:45.306 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/jobs/job/json,null,AVAILABLE,@Spark}
11:20:45.307 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/stages,null,AVAILABLE,@Spark}
11:20:45.308 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e11ab3d{/stages/json,null,AVAILABLE,@Spark}
11:20:45.311 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2392212b{/stages/stage,null,AVAILABLE,@Spark}
11:20:45.313 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66f66866{/stages/stage/json,null,AVAILABLE,@Spark}
11:20:45.314 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d666b41{/stages/pool,null,AVAILABLE,@Spark}
11:20:45.315 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30f4b1a6{/stages/pool/json,null,AVAILABLE,@Spark}
11:20:45.316 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e1162e7{/storage,null,AVAILABLE,@Spark}
11:20:45.317 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2f1700{/storage/json,null,AVAILABLE,@Spark}
11:20:45.318 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/storage/rdd,null,AVAILABLE,@Spark}
11:20:45.321 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/storage/rdd/json,null,AVAILABLE,@Spark}
11:20:45.322 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/environment,null,AVAILABLE,@Spark}
11:20:45.322 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@553f3b6e{/environment/json,null,AVAILABLE,@Spark}
11:20:45.323 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/executors,null,AVAILABLE,@Spark}
11:20:45.324 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/executors/json,null,AVAILABLE,@Spark}
11:20:45.332 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/executors/threadDump,null,AVAILABLE,@Spark}
11:20:45.334 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bb7cce7{/executors/threadDump/json,null,AVAILABLE,@Spark}
11:20:45.350 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b530eb9{/static,null,AVAILABLE,@Spark}
11:20:45.351 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@680bddf5{/,null,AVAILABLE,@Spark}
11:20:45.353 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d83c5a5{/api,null,AVAILABLE,@Spark}
11:20:45.354 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@710b30ef{/jobs/job/kill,null,AVAILABLE,@Spark}
11:20:45.355 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28b576a9{/stages/stage/kill,null,AVAILABLE,@Spark}
11:20:45.360 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
11:20:46.096 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
11:20:46.499 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14454.
11:20:46.517 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:14454
11:20:46.531 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
11:20:46.612 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 14454, None)
11:20:46.645 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:14454 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 14454, None)
11:20:46.675 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 14454, None)
11:20:46.676 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 14454, None)
11:20:47.142 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27fde870{/metrics/json,null,AVAILABLE,@Spark}
11:20:51.467 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
11:20:51.548 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
11:20:51.549 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
11:20:51.569 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53aa38be{/SQL,null,AVAILABLE,@Spark}
11:20:51.569 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@422ad5e2{/SQL/json,null,AVAILABLE,@Spark}
11:20:51.570 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c8f97a7{/SQL/execution,null,AVAILABLE,@Spark}
11:20:51.570 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3249e278{/SQL/execution/json,null,AVAILABLE,@Spark}
11:20:51.602 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c8fe7a4{/static/sql,null,AVAILABLE,@Spark}
11:20:52.785 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
11:20:55.116 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
11:20:55.183 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
11:20:57.363 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
11:21:00.146 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
11:21:00.154 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
11:21:00.992 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
11:21:00.997 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
11:21:01.124 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
11:21:01.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
11:21:01.349 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
11:21:01.398 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
11:21:01.398 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
11:21:01.511 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
11:21:01.512 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
11:21:01.526 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
11:21:01.527 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
11:21:01.532 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
11:21:01.533 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
11:21:01.540 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
11:21:01.540 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
11:21:01.548 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
11:21:01.548 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
11:21:01.555 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
11:21:01.555 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
11:21:01.564 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
11:21:01.564 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
11:21:03.477 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/d5723e36-05f9-49ad-ac1f-9af9fe77a94d_resources
11:21:03.522 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/d5723e36-05f9-49ad-ac1f-9af9fe77a94d
11:21:03.526 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/d5723e36-05f9-49ad-ac1f-9af9fe77a94d
11:21:03.546 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/d5723e36-05f9-49ad-ac1f-9af9fe77a94d/_tmp_space.db
11:21:03.578 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
11:21:03.836 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:21:03.836 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:21:03.845 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
11:21:03.845 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
11:21:03.854 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
11:21:04.375 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/6e9f160a-8cef-44b8-adbc-4ca8918262ba_resources
11:21:04.402 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/6e9f160a-8cef-44b8-adbc-4ca8918262ba
11:21:04.406 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/6e9f160a-8cef-44b8-adbc-4ca8918262ba
11:21:04.423 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/6e9f160a-8cef-44b8-adbc-4ca8918262ba/_tmp_space.db
11:21:04.426 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
11:21:04.665 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
11:21:04.689 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:21:05.124 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:21:05.125 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:21:05.172 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:21:05.173 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:21:05.555 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:21:05.555 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:21:05.640 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:05.716 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:05.717 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:05.718 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:05.718 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:05.719 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:05.721 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:05.721 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:05.721 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:05.722 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:21:05.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
11:21:07.321 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:21:07.341 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:21:07.342 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:21:07.343 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:21:07.344 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:21:07.344 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:21:07.345 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
11:21:07.398 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
11:21:07.447 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
11:21:07.454 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
11:21:07.456 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
11:21:07.458 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
11:21:07.459 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
11:21:07.460 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
11:21:07.462 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
11:21:07.463 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
11:21:07.463 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:21:07.464 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:21:07.485 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:21:07.485 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:21:07.500 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:21:07.501 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:21:07.509 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:21:07.510 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:21:07.524 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:21:07.524 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:21:07.539 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:21:07.540 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:21:07.567 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:21:07.567 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:21:07.592 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:21:07.593 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:21:07.617 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:21:07.617 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:21:07.636 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:21:07.636 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:21:07.672 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:21:07.672 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:21:07.684 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:21:07.684 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:21:07.697 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:21:07.697 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:21:07.708 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:21:07.708 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:21:07.716 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:21:07.717 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:21:08.709 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:21:08.709 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:21:08.718 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:21:08.719 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:21:09.081 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:21:09.081 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:21:09.115 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:09.115 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:09.116 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:09.116 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:09.116 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:09.117 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:09.117 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:09.117 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:09.117 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:09.118 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:21:09.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:21:09.268 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:21:09.311 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:21:09.312 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:21:09.922 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190613)
11:21:09.925 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
11:21:09.928 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:21:10.005 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:21:10.040 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:21:11.034 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 468.4091 ms
11:21:11.672 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 151.4633 ms
11:21:11.787 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
11:21:12.216 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 264.7 KB, free 897.3 MB)
11:21:13.048 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.4 KB, free 897.3 MB)
11:21:13.056 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:14454 (size: 22.4 KB, free: 897.6 MB)
11:21:13.069 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:51
11:21:13.186 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:21:13.753 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:51
11:21:13.848 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:51)
11:21:13.853 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:51) with 1 output partitions
11:21:13.853 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:51)
11:21:13.854 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
11:21:13.865 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:21:13.896 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:51), which has no missing parents
11:21:14.020 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
11:21:14.025 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
11:21:14.026 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:14454 (size: 2.2 KB, free: 897.6 MB)
11:21:14.027 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
11:21:14.054 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:51) (first 15 tasks are for partitions Vector(0))
11:21:14.056 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
11:21:14.174 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:21:14.242 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
11:21:14.476 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:21:14.482 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 19 ms
11:21:14.592 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1268 bytes result sent to driver
11:21:14.623 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 470 ms on localhost (executor driver) (1/1)
11:21:14.630 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
11:21:14.648 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:51) finished in 0.519 s
11:21:14.666 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:51, took 0.912783 s
11:21:14.716 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:51
11:21:14.722 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at DWReleaseCustomer.scala:51) with 3 output partitions
11:21:14.722 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at DWReleaseCustomer.scala:51)
11:21:14.722 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
11:21:14.723 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:21:14.725 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:51), which has no missing parents
11:21:14.738 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
11:21:14.743 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
11:21:14.745 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:14454 (size: 2.2 KB, free: 897.6 MB)
11:21:14.746 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
11:21:14.748 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:51) (first 15 tasks are for partitions Vector(1, 2, 3))
11:21:14.748 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
11:21:14.752 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:21:14.755 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:21:14.756 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:21:14.756 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
11:21:14.772 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:21:14.772 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:21:14.775 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1182 bytes result sent to driver
11:21:14.776 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
11:21:14.776 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
11:21:14.787 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 35 ms on localhost (executor driver) (1/3)
11:21:14.799 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:21:14.799 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:21:14.799 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:21:14.799 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:21:14.801 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1182 bytes result sent to driver
11:21:14.801 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1182 bytes result sent to driver
11:21:14.805 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 51 ms on localhost (executor driver) (2/3)
11:21:14.807 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 52 ms on localhost (executor driver) (3/3)
11:21:14.810 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
11:21:14.812 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at DWReleaseCustomer.scala:51) finished in 0.061 s
11:21:14.812 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at DWReleaseCustomer.scala:51, took 0.096124 s
11:21:14.903 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
11:21:14.924 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:21:14.924 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:21:14.983 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:14.983 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:14.984 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:14.984 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:14.985 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:14.985 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:14.985 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:14.986 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:14.987 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:21:14.987 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:14.988 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:14.988 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:14.988 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:14.988 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:14.990 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:14.991 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:14.991 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:14.992 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:21:15.288 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/.hive-staging_hive_2019-09-27_11-21-15_272_3054412025190026665-1
11:21:15.986 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:21:15.986 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:21:15.994 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:21:15.994 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:21:16.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:21:16.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:21:16.060 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:16.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:16.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:16.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:16.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:16.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:16.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:16.062 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:16.062 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:16.062 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:21:16.065 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:21:16.065 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:21:16.065 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:21:16.065 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:21:16.103 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190613)
11:21:16.103 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
11:21:16.104 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:21:16.104 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:21:16.104 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:21:16.196 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:21:16.331 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 60.7842 ms
11:21:16.358 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 264.7 KB, free 897.0 MB)
11:21:16.383 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.4 KB, free 897.0 MB)
11:21:16.383 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.237.1:14454 (size: 22.4 KB, free: 897.6 MB)
11:21:16.386 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:33
11:21:16.387 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:21:16.615 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:33
11:21:16.616 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (insertInto at SparkHelper.scala:33)
11:21:16.616 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (insertInto at SparkHelper.scala:33) with 4 output partitions
11:21:16.617 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (insertInto at SparkHelper.scala:33)
11:21:16.617 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
11:21:16.617 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:21:16.617 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:33), which has no missing parents
11:21:16.770 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
11:21:16.805 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 159.9 KB, free 896.9 MB)
11:21:16.813 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 58.7 KB, free 896.8 MB)
11:21:16.817 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.237.1:14454 (size: 58.7 KB, free: 897.5 MB)
11:21:16.818 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
11:21:16.819 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 5 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:33) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:21:16.819 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 4 tasks
11:21:16.821 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:21:16.821 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 5, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:21:16.822 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 5.0 (TID 6, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:21:16.822 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 5.0 (TID 7, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:21:16.826 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 4)
11:21:16.826 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 1.0 in stage 5.0 (TID 5)
11:21:16.826 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 2.0 in stage 5.0 (TID 6)
11:21:16.863 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.237.1:14454 in memory (size: 2.2 KB, free: 897.5 MB)
11:21:16.948 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:21:16.948 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
11:21:16.960 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:21:16.961 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:21:16.974 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
11:21:16.974 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:21:16.974 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:21:16.976 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 3.0 in stage 5.0 (TID 7)
11:21:16.987 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.237.1:14454 in memory (size: 22.4 KB, free: 897.5 MB)
11:21:16.992 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 8
11:21:16.992 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 5
11:21:17.003 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:21:17.003 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:21:17.011 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 0
11:21:17.011 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 7
11:21:17.020 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.237.1:14454 in memory (size: 2.2 KB, free: 897.5 MB)
11:21:17.022 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
11:21:17.022 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 4
11:21:17.022 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
11:21:17.022 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 3
11:21:17.040 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.2403 ms
11:21:17.120 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.7269 ms
11:21:17.170 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:21:17.170 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:21:17.171 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:21:17.171 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:21:17.220 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 29.3175 ms
11:21:17.312 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 32.2479 ms
11:21:17.342 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 27.2821 ms
11:21:17.370 INFO  [Executor task launch worker for task 6] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190927112117_0005_m_000002_0
11:21:17.370 INFO  [Executor task launch worker for task 7] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190927112117_0005_m_000003_0
11:21:17.370 INFO  [Executor task launch worker for task 5] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190927112117_0005_m_000001_0
11:21:17.370 INFO  [Executor task launch worker for task 4] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190927112117_0005_m_000000_0
11:21:17.375 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 3.0 in stage 5.0 (TID 7). 2588 bytes result sent to driver
11:21:17.376 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 2.0 in stage 5.0 (TID 6). 2588 bytes result sent to driver
11:21:17.377 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 1.0 in stage 5.0 (TID 5). 2545 bytes result sent to driver
11:21:17.379 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 4). 2545 bytes result sent to driver
11:21:17.383 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 5.0 (TID 7) in 561 ms on localhost (executor driver) (1/4)
11:21:17.384 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 4) in 564 ms on localhost (executor driver) (2/4)
11:21:17.384 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 5) in 563 ms on localhost (executor driver) (3/4)
11:21:17.385 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 5.0 (TID 6) in 563 ms on localhost (executor driver) (4/4)
11:21:17.385 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
11:21:17.391 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (insertInto at SparkHelper.scala:33) finished in 0.571 s
11:21:17.395 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: insertInto at SparkHelper.scala:33, took 0.780262 s
11:21:18.775 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:21:18.793 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:21:18.793 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:21:18.830 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:21:18.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:21:18.865 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:18.865 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:18.866 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:18.866 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:18.866 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:18.867 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:18.867 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:18.867 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:18.867 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:21:18.867 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:18.868 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:18.868 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:18.869 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:18.869 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:18.870 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:18.870 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:18.870 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:18.871 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:21:18.874 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:21:18.874 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:21:18.935 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
11:21:18.970 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:21:18.970 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:21:19.050 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
11:21:19.056 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:21:19.056 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:21:19.076 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:21:19.077 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:21:19.145 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:21:19.145 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:21:19.181 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.181 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.184 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.185 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.185 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.185 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.185 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.185 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.186 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:21:19.186 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.186 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.187 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.187 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.187 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.187 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.187 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.187 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.187 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:21:19.227 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_customer (inference mode: INFER_AND_SAVE)
11:21:19.228 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:21:19.228 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:21:19.233 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:21:19.234 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:21:19.287 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:21:19.287 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:21:19.421 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.421 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.422 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.422 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.423 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.423 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.423 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.423 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.424 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:21:19.424 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.429 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.429 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.429 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.429 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.429 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.430 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.430 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:19.430 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:21:19.431 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_customer
11:21:19.431 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_customer	
11:21:20.078 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 88
11:21:20.079 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 82
11:21:20.080 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 89
11:21:20.080 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 83
11:21:20.085 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.237.1:14454 in memory (size: 58.7 KB, free: 897.6 MB)
11:21:20.089 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 192.168.237.1:14454 in memory (size: 22.4 KB, free: 897.6 MB)
11:21:20.090 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 85
11:21:20.090 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 84
11:21:20.090 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 87
11:21:20.090 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 1
11:21:20.091 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 91
11:21:20.091 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 92
11:21:20.091 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 86
11:21:20.091 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 90
11:21:20.091 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 93
11:21:20.485 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:33
11:21:20.488 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 3 (insertInto at SparkHelper.scala:33) with 1 output partitions
11:21:20.488 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (insertInto at SparkHelper.scala:33)
11:21:20.488 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
11:21:20.488 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:21:20.488 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[16] at insertInto at SparkHelper.scala:33), which has no missing parents
11:21:20.514 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 64.0 KB, free 897.5 MB)
11:21:20.517 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 22.9 KB, free 897.5 MB)
11:21:20.520 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.237.1:14454 (size: 22.9 KB, free: 897.6 MB)
11:21:20.521 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
11:21:20.521 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[16] at insertInto at SparkHelper.scala:33) (first 15 tasks are for partitions Vector(0))
11:21:20.521 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks
11:21:20.524 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 5028 bytes)
11:21:20.525 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 8)
11:21:23.128 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 8). 1978 bytes result sent to driver
11:21:23.130 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 8) in 2608 ms on localhost (executor driver) (1/1)
11:21:23.130 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
11:21:23.131 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (insertInto at SparkHelper.scala:33) finished in 2.609 s
11:21:23.131 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 3 finished: insertInto at SparkHelper.scala:33, took 2.644619 s
11:21:23.162 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Saving case-sensitive schema for table dw_release.dw_release_customer
11:21:23.163 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:21:23.163 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:21:23.171 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:21:23.171 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:21:23.207 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:21:23.207 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:21:23.250 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.251 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.252 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.252 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.252 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.252 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.253 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.253 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.253 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:21:23.254 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.255 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.255 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.256 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.256 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.258 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.258 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.258 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.258 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:21:23.283 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:21:23.283 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:21:23.374 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:21:23.374 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:21:23.454 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.455 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.455 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.456 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.456 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.456 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.456 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.590 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.590 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:21:23.590 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.591 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.591 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.591 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.591 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.591 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.592 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.592 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:21:23.592 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:21:23.604 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:21:23.604 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:21:24.083 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_table: db=dw_release tbl=dw_release_customer newtbl=dw_release_customer
11:21:24.083 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=alter_table: db=dw_release tbl=dw_release_customer newtbl=dw_release_customer	
11:21:24.516 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
11:21:24.555 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@54f5f647{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:21:24.565 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
11:21:24.601 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
11:21:24.626 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
11:21:24.627 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
11:21:24.631 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
11:21:24.634 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
11:21:24.637 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
11:21:24.643 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
11:21:24.644 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-b422428f-9c32-4f11-ab8d-83f28b55799b
11:37:37.285 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
11:37:38.168 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
11:37:38.490 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
11:37:38.492 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
11:37:38.493 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
11:37:38.494 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
11:37:38.495 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
11:37:40.552 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 14605.
11:37:40.650 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
11:37:40.773 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
11:37:40.790 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
11:37:40.791 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
11:37:40.816 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-c3e18554-954e-43c4-aa41-b2a59b296749
11:37:40.930 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
11:37:41.123 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
11:37:41.523 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @17967ms
11:37:41.719 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
11:37:41.769 INFO  [main] org.spark_project.jetty.server.Server - Started @18215ms
11:37:41.812 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@54f5f647{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:37:41.813 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
11:37:41.865 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62df0ff3{/jobs,null,AVAILABLE,@Spark}
11:37:41.866 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/jobs/json,null,AVAILABLE,@Spark}
11:37:41.867 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/jobs/job,null,AVAILABLE,@Spark}
11:37:41.869 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/jobs/job/json,null,AVAILABLE,@Spark}
11:37:41.871 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/stages,null,AVAILABLE,@Spark}
11:37:41.871 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e11ab3d{/stages/json,null,AVAILABLE,@Spark}
11:37:41.874 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2392212b{/stages/stage,null,AVAILABLE,@Spark}
11:37:41.877 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66f66866{/stages/stage/json,null,AVAILABLE,@Spark}
11:37:41.878 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d666b41{/stages/pool,null,AVAILABLE,@Spark}
11:37:41.879 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30f4b1a6{/stages/pool/json,null,AVAILABLE,@Spark}
11:37:41.880 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e1162e7{/storage,null,AVAILABLE,@Spark}
11:37:41.881 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2f1700{/storage/json,null,AVAILABLE,@Spark}
11:37:41.882 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/storage/rdd,null,AVAILABLE,@Spark}
11:37:41.883 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/storage/rdd/json,null,AVAILABLE,@Spark}
11:37:41.884 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/environment,null,AVAILABLE,@Spark}
11:37:41.885 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@553f3b6e{/environment/json,null,AVAILABLE,@Spark}
11:37:41.886 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/executors,null,AVAILABLE,@Spark}
11:37:41.887 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/executors/json,null,AVAILABLE,@Spark}
11:37:41.894 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/executors/threadDump,null,AVAILABLE,@Spark}
11:37:41.895 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bb7cce7{/executors/threadDump/json,null,AVAILABLE,@Spark}
11:37:41.926 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b530eb9{/static,null,AVAILABLE,@Spark}
11:37:41.927 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@680bddf5{/,null,AVAILABLE,@Spark}
11:37:41.929 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d83c5a5{/api,null,AVAILABLE,@Spark}
11:37:41.930 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@710b30ef{/jobs/job/kill,null,AVAILABLE,@Spark}
11:37:41.931 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28b576a9{/stages/stage/kill,null,AVAILABLE,@Spark}
11:37:41.934 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
11:37:42.358 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
11:37:42.480 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14626.
11:37:42.481 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:14626
11:37:42.505 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
11:37:42.563 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 14626, None)
11:37:42.586 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:14626 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 14626, None)
11:37:42.604 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 14626, None)
11:37:42.607 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 14626, None)
11:37:42.998 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27fde870{/metrics/json,null,AVAILABLE,@Spark}
11:37:47.652 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
11:37:47.734 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
11:37:47.735 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
11:37:47.757 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53aa38be{/SQL,null,AVAILABLE,@Spark}
11:37:47.758 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@422ad5e2{/SQL/json,null,AVAILABLE,@Spark}
11:37:47.759 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c8f97a7{/SQL/execution,null,AVAILABLE,@Spark}
11:37:47.760 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3249e278{/SQL/execution/json,null,AVAILABLE,@Spark}
11:37:47.766 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c8fe7a4{/static/sql,null,AVAILABLE,@Spark}
11:37:48.799 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
11:37:50.462 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
11:37:50.549 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
11:37:53.681 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
11:37:56.726 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
11:37:56.756 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
11:37:57.540 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
11:37:57.548 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
11:37:57.835 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
11:37:58.169 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
11:37:58.171 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
11:37:58.216 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
11:37:58.217 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
11:37:58.331 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
11:37:58.332 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
11:37:58.337 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
11:37:58.337 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
11:37:58.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
11:37:58.342 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
11:37:58.349 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
11:37:58.349 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
11:37:58.354 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
11:37:58.354 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
11:37:58.360 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
11:37:58.360 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
11:37:58.365 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
11:37:58.366 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
11:38:00.902 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/2f293599-c3e7-4869-9a39-74771961490c_resources
11:38:00.918 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/2f293599-c3e7-4869-9a39-74771961490c
11:38:00.924 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/2f293599-c3e7-4869-9a39-74771961490c
11:38:00.965 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/2f293599-c3e7-4869-9a39-74771961490c/_tmp_space.db
11:38:00.975 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
11:38:01.172 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:38:01.172 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:38:01.198 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
11:38:01.199 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
11:38:01.214 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
11:38:01.812 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/0da844d9-fda9-4209-bce7-bb1bc6c00e10_resources
11:38:01.895 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/0da844d9-fda9-4209-bce7-bb1bc6c00e10
11:38:01.916 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/0da844d9-fda9-4209-bce7-bb1bc6c00e10
11:38:01.942 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/0da844d9-fda9-4209-bce7-bb1bc6c00e10/_tmp_space.db
11:38:01.953 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
11:38:02.224 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
11:38:02.263 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:38:02.910 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:38:02.911 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:38:02.941 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:38:02.941 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:38:03.329 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:38:03.329 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:38:03.372 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:03.440 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:03.441 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:03.441 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:03.441 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:03.442 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:03.442 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:03.442 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:03.443 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:03.443 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:38:03.536 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
11:38:05.876 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:38:05.897 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:38:05.898 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:38:05.899 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:38:05.899 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:38:05.900 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:38:05.901 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
11:38:05.959 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
11:38:06.032 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
11:38:06.037 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
11:38:06.038 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
11:38:06.039 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
11:38:06.040 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
11:38:06.041 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
11:38:06.042 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
11:38:06.044 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
11:38:06.044 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:38:06.045 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:38:06.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:38:06.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:38:06.081 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:38:06.081 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:38:06.095 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:38:06.096 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:38:06.105 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:38:06.106 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:38:06.126 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:38:06.127 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:38:06.135 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:38:06.136 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:38:06.159 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:38:06.159 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:38:06.167 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:38:06.168 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:38:06.185 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:38:06.185 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:38:06.199 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:38:06.199 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:38:06.215 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:38:06.215 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:38:06.222 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:38:06.223 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:38:06.232 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:38:06.233 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:38:06.252 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:38:06.253 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:38:08.538 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:38:08.538 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:38:08.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:38:08.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:38:08.588 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:38:08.588 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:38:08.631 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:08.631 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:08.632 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:08.632 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:08.632 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:08.633 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:08.633 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:08.633 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:08.634 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:38:08.634 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:38:08.753 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:38:08.754 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:38:08.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:38:08.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:38:09.467 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190613)
11:38:09.471 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
11:38:09.474 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:38:09.545 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:38:09.580 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:38:11.120 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 706.7736 ms
11:38:11.962 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 181.4369 ms
11:38:12.841 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 264.7 KB, free 897.3 MB)
11:38:13.192 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.4 KB, free 897.3 MB)
11:38:13.196 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:14626 (size: 22.4 KB, free: 897.6 MB)
11:38:13.223 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:51
11:38:13.318 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:38:13.501 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
11:38:13.917 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:51
11:38:14.023 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:51)
11:38:14.037 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:51) with 1 output partitions
11:38:14.038 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:51)
11:38:14.039 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
11:38:14.043 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:38:14.084 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:51), which has no missing parents
11:38:14.333 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
11:38:14.344 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
11:38:14.346 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:14626 (size: 2.2 KB, free: 897.6 MB)
11:38:14.347 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
11:38:14.408 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:51) (first 15 tasks are for partitions Vector(0))
11:38:14.413 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
11:38:14.609 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:38:14.665 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
11:38:14.983 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:38:14.987 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 29 ms
11:38:15.218 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1268 bytes result sent to driver
11:38:15.264 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 687 ms on localhost (executor driver) (1/1)
11:38:15.274 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
11:38:15.305 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:51) finished in 0.765 s
11:38:15.343 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:51, took 1.426056 s
11:38:15.406 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:51
11:38:15.410 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at DWReleaseCustomer.scala:51) with 3 output partitions
11:38:15.410 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at DWReleaseCustomer.scala:51)
11:38:15.410 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
11:38:15.410 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:38:15.411 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:51), which has no missing parents
11:38:15.414 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
11:38:15.417 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
11:38:15.421 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:14626 (size: 2.2 KB, free: 897.6 MB)
11:38:15.422 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
11:38:15.424 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:51) (first 15 tasks are for partitions Vector(1, 2, 3))
11:38:15.424 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
11:38:15.426 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:38:15.427 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:38:15.428 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:38:15.428 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
11:38:15.429 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
11:38:15.430 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
11:38:15.438 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:38:15.439 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:38:15.439 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:38:15.439 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:38:15.441 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1225 bytes result sent to driver
11:38:15.441 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1225 bytes result sent to driver
11:38:15.442 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:38:15.443 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:38:15.445 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 19 ms on localhost (executor driver) (1/3)
11:38:15.446 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1225 bytes result sent to driver
11:38:15.452 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 27 ms on localhost (executor driver) (2/3)
11:38:15.454 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 27 ms on localhost (executor driver) (3/3)
11:38:15.454 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
11:38:15.454 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at DWReleaseCustomer.scala:51) finished in 0.029 s
11:38:15.455 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at DWReleaseCustomer.scala:51, took 0.048455 s
11:38:15.613 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
11:38:15.695 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@54f5f647{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:38:15.714 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
11:38:15.802 INFO  [dispatcher-event-loop-2] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
11:38:15.873 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
11:38:15.880 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
11:38:15.881 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
11:38:15.890 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
11:38:15.902 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
11:38:15.904 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
11:38:15.905 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-7efc8542-74c1-4f56-82e5-e4420f09e84a
11:42:03.578 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
11:42:04.424 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
11:42:04.620 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
11:42:04.622 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
11:42:04.624 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
11:42:04.625 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
11:42:04.631 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
11:42:07.334 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 14686.
11:42:07.541 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
11:42:07.698 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
11:42:07.724 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
11:42:07.726 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
11:42:07.773 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-087a33b6-864b-4e11-82a7-b1f071b1557c
11:42:07.856 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
11:42:08.093 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
11:42:08.514 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @15198ms
11:42:08.684 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
11:42:08.726 INFO  [main] org.spark_project.jetty.server.Server - Started @15412ms
11:42:08.780 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4fdf8f12{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:42:08.784 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
11:42:08.830 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1162410a{/jobs,null,AVAILABLE,@Spark}
11:42:08.831 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b0f7d9d{/jobs/json,null,AVAILABLE,@Spark}
11:42:08.832 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/jobs/job,null,AVAILABLE,@Spark}
11:42:08.836 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/jobs/job/json,null,AVAILABLE,@Spark}
11:42:08.837 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/stages,null,AVAILABLE,@Spark}
11:42:08.838 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/stages/json,null,AVAILABLE,@Spark}
11:42:08.839 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e11ab3d{/stages/stage,null,AVAILABLE,@Spark}
11:42:08.841 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28f8e165{/stages/stage/json,null,AVAILABLE,@Spark}
11:42:08.843 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66f66866{/stages/pool,null,AVAILABLE,@Spark}
11:42:08.850 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d666b41{/stages/pool/json,null,AVAILABLE,@Spark}
11:42:08.851 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30f4b1a6{/storage,null,AVAILABLE,@Spark}
11:42:08.852 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e1162e7{/storage/json,null,AVAILABLE,@Spark}
11:42:08.855 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2f1700{/storage/rdd,null,AVAILABLE,@Spark}
11:42:08.857 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/storage/rdd/json,null,AVAILABLE,@Spark}
11:42:08.863 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/environment,null,AVAILABLE,@Spark}
11:42:08.878 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/environment/json,null,AVAILABLE,@Spark}
11:42:08.881 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@553f3b6e{/executors,null,AVAILABLE,@Spark}
11:42:08.885 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/executors/json,null,AVAILABLE,@Spark}
11:42:08.886 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/executors/threadDump,null,AVAILABLE,@Spark}
11:42:08.888 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/executors/threadDump/json,null,AVAILABLE,@Spark}
11:42:08.900 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bb7cce7{/static,null,AVAILABLE,@Spark}
11:42:08.901 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23aae55{/,null,AVAILABLE,@Spark}
11:42:08.904 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@680bddf5{/api,null,AVAILABLE,@Spark}
11:42:08.905 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53bf7094{/jobs/job/kill,null,AVAILABLE,@Spark}
11:42:08.906 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@710b30ef{/stages/stage/kill,null,AVAILABLE,@Spark}
11:42:08.910 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
11:42:09.266 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
11:42:09.358 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14707.
11:42:09.362 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:14707
11:42:09.365 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
11:42:09.445 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 14707, None)
11:42:09.475 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:14707 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 14707, None)
11:42:09.496 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 14707, None)
11:42:09.497 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 14707, None)
11:42:09.959 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a7761b1{/metrics/json,null,AVAILABLE,@Spark}
11:42:14.069 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
11:42:14.157 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
11:42:14.158 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
11:42:14.182 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@317890ea{/SQL,null,AVAILABLE,@Spark}
11:42:14.183 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53aa38be{/SQL/json,null,AVAILABLE,@Spark}
11:42:14.184 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50b93353{/SQL/execution,null,AVAILABLE,@Spark}
11:42:14.185 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c8f97a7{/SQL/execution/json,null,AVAILABLE,@Spark}
11:42:14.189 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8f40022{/static/sql,null,AVAILABLE,@Spark}
11:42:15.260 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
11:42:16.708 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
11:42:16.872 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
11:42:19.023 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
11:42:21.875 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
11:42:21.879 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
11:42:22.538 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
11:42:22.545 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
11:42:22.696 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
11:42:22.999 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
11:42:23.016 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
11:42:23.157 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
11:42:23.157 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
11:42:23.273 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
11:42:23.274 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
11:42:23.279 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
11:42:23.279 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
11:42:23.283 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
11:42:23.284 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
11:42:23.292 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
11:42:23.293 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
11:42:23.300 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
11:42:23.300 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
11:42:23.305 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
11:42:23.305 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
11:42:23.312 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
11:42:23.313 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
11:42:25.360 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/a89d0bcb-feb3-4bdd-a73d-4d65b03e59c8_resources
11:42:25.372 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/a89d0bcb-feb3-4bdd-a73d-4d65b03e59c8
11:42:25.391 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/a89d0bcb-feb3-4bdd-a73d-4d65b03e59c8
11:42:25.404 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/a89d0bcb-feb3-4bdd-a73d-4d65b03e59c8/_tmp_space.db
11:42:25.412 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
11:42:25.518 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:42:25.518 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:42:25.528 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
11:42:25.528 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
11:42:25.533 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
11:42:26.148 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/fc88f865-d74e-4b6c-8112-2d18dab36a2f_resources
11:42:26.155 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/fc88f865-d74e-4b6c-8112-2d18dab36a2f
11:42:26.165 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/fc88f865-d74e-4b6c-8112-2d18dab36a2f
11:42:26.177 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/fc88f865-d74e-4b6c-8112-2d18dab36a2f/_tmp_space.db
11:42:26.191 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
11:42:26.321 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
11:42:26.336 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:42:27.016 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:42:27.016 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:42:27.024 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:42:27.024 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:42:27.248 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:42:27.249 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:42:27.330 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:27.376 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:27.376 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:27.378 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:27.379 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:27.379 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:27.379 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:27.380 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:27.380 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:27.380 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:42:27.478 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
11:42:28.469 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:42:28.489 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:42:28.490 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:42:28.492 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:42:28.493 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:42:28.493 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:42:28.494 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
11:42:28.521 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
11:42:28.567 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
11:42:28.570 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
11:42:28.571 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
11:42:28.573 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
11:42:28.574 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
11:42:28.579 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
11:42:28.580 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
11:42:28.581 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
11:42:28.582 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:42:28.583 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:42:28.589 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:42:28.590 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:42:28.602 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:42:28.603 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:42:28.614 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:42:28.614 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:42:28.623 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:42:28.623 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:42:28.632 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:42:28.632 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:42:28.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:42:28.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:42:28.658 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:42:28.658 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:42:28.665 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:42:28.666 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:42:28.677 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:42:28.678 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:42:28.693 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:42:28.694 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:42:28.721 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:42:28.721 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:42:28.746 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:42:28.746 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:42:28.763 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:42:28.764 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:42:28.782 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:42:28.783 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:42:29.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:42:29.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:42:29.660 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:42:29.660 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:42:29.697 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:42:29.697 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:42:29.730 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:29.731 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:29.732 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:29.733 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:29.733 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:29.733 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:29.734 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:29.734 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:29.735 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:42:29.735 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:42:29.833 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:42:29.833 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:42:29.873 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:42:29.874 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:42:30.305 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
11:42:30.307 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
11:42:30.311 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:42:30.421 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:42:30.431 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:42:31.364 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 366.8869 ms
11:42:31.561 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
11:42:32.268 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 161.0681 ms
11:42:32.716 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 264.7 KB, free 897.3 MB)
11:42:33.034 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.4 KB, free 897.3 MB)
11:42:33.041 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:14707 (size: 22.4 KB, free: 897.6 MB)
11:42:33.143 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:51
11:42:33.267 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:42:33.884 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:51
11:42:33.955 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:51)
11:42:33.959 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:51) with 1 output partitions
11:42:33.959 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:51)
11:42:33.960 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
11:42:33.972 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:42:34.003 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:51), which has no missing parents
11:42:34.095 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
11:42:34.107 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
11:42:34.109 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:14707 (size: 2.2 KB, free: 897.6 MB)
11:42:34.110 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
11:42:34.131 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:51) (first 15 tasks are for partitions Vector(0))
11:42:34.133 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
11:42:34.291 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:42:34.357 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
11:42:34.616 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:42:34.625 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 45 ms
11:42:34.689 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1268 bytes result sent to driver
11:42:34.721 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 472 ms on localhost (executor driver) (1/1)
11:42:34.738 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
11:42:34.746 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:51) finished in 0.554 s
11:42:34.763 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:51, took 0.878007 s
11:42:34.786 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:51
11:42:34.799 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at DWReleaseCustomer.scala:51) with 3 output partitions
11:42:34.799 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at DWReleaseCustomer.scala:51)
11:42:34.799 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
11:42:34.799 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:42:34.800 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:51), which has no missing parents
11:42:34.803 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
11:42:34.808 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
11:42:34.818 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:14707 (size: 2.2 KB, free: 897.6 MB)
11:42:34.821 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
11:42:34.831 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:51) (first 15 tasks are for partitions Vector(1, 2, 3))
11:42:34.831 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
11:42:34.834 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:42:34.836 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:42:34.836 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:42:34.839 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
11:42:34.853 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:42:34.854 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:42:34.856 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1225 bytes result sent to driver
11:42:34.868 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
11:42:34.869 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
11:42:34.870 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 37 ms on localhost (executor driver) (1/3)
11:42:34.883 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:42:34.883 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:42:34.893 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1182 bytes result sent to driver
11:42:34.895 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:42:34.895 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:42:34.897 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1182 bytes result sent to driver
11:42:34.901 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 66 ms on localhost (executor driver) (2/3)
11:42:34.903 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 67 ms on localhost (executor driver) (3/3)
11:42:34.904 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at DWReleaseCustomer.scala:51) finished in 0.072 s
11:42:34.906 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at DWReleaseCustomer.scala:51, took 0.120508 s
11:42:34.907 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
11:42:34.965 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
11:42:35.067 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@4fdf8f12{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:42:35.071 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
11:42:35.121 INFO  [dispatcher-event-loop-1] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
11:42:35.141 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
11:42:35.142 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
11:42:35.144 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
11:42:35.147 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
11:42:35.150 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
11:42:35.151 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
11:42:35.153 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-ba8553ba-5148-4f49-b52e-ed5e5f3758f2
11:47:47.717 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
11:47:48.399 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
11:47:48.440 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
11:47:48.441 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
11:47:48.442 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
11:47:48.443 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
11:47:48.444 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
11:47:50.390 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 14757.
11:47:50.413 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
11:47:50.438 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
11:47:50.442 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
11:47:50.442 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
11:47:50.454 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-32935f86-2a67-4c68-9bbb-3e01185884a7
11:47:50.480 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
11:47:50.552 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
11:47:50.743 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @8142ms
11:47:50.837 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
11:47:50.882 INFO  [main] org.spark_project.jetty.server.Server - Started @8281ms
11:47:50.923 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@5cf2ea8b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:47:50.924 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
11:47:50.970 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/jobs,null,AVAILABLE,@Spark}
11:47:50.971 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/jobs/json,null,AVAILABLE,@Spark}
11:47:50.972 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/jobs/job,null,AVAILABLE,@Spark}
11:47:50.973 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/jobs/job/json,null,AVAILABLE,@Spark}
11:47:50.975 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ea1fda{/stages,null,AVAILABLE,@Spark}
11:47:50.978 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@420745d7{/stages/json,null,AVAILABLE,@Spark}
11:47:50.980 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fa47fea{/stages/stage,null,AVAILABLE,@Spark}
11:47:50.981 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545f80bf{/stages/stage/json,null,AVAILABLE,@Spark}
11:47:50.983 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22fa55b2{/stages/pool,null,AVAILABLE,@Spark}
11:47:50.985 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6594402a{/stages/pool/json,null,AVAILABLE,@Spark}
11:47:50.990 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@405325cf{/storage,null,AVAILABLE,@Spark}
11:47:50.991 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c3f01f{/storage/json,null,AVAILABLE,@Spark}
11:47:50.992 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/storage/rdd,null,AVAILABLE,@Spark}
11:47:51.000 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/storage/rdd/json,null,AVAILABLE,@Spark}
11:47:51.001 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/environment,null,AVAILABLE,@Spark}
11:47:51.003 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5298dead{/environment/json,null,AVAILABLE,@Spark}
11:47:51.005 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/executors,null,AVAILABLE,@Spark}
11:47:51.006 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/executors/json,null,AVAILABLE,@Spark}
11:47:51.007 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/executors/threadDump,null,AVAILABLE,@Spark}
11:47:51.009 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b986cd{/executors/threadDump/json,null,AVAILABLE,@Spark}
11:47:51.021 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41c62850{/static,null,AVAILABLE,@Spark}
11:47:51.030 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f574cc2{/,null,AVAILABLE,@Spark}
11:47:51.032 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a9c84a5{/api,null,AVAILABLE,@Spark}
11:47:51.033 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f1249d{/jobs/job/kill,null,AVAILABLE,@Spark}
11:47:51.034 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a68df9{/stages/stage/kill,null,AVAILABLE,@Spark}
11:47:51.038 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
11:47:51.230 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
11:47:51.364 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14778.
11:47:51.365 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:14778
11:47:51.368 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
11:47:51.408 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 14778, None)
11:47:51.412 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:14778 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 14778, None)
11:47:51.420 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 14778, None)
11:47:51.421 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 14778, None)
11:47:51.855 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e850122{/metrics/json,null,AVAILABLE,@Spark}
11:47:52.118 ERROR [main] com.qf.bigdata.release.util.SparkHelper$ - Text '2019-09-24' could not be parsed at index 4
java.time.format.DateTimeParseException: Text '2019-09-24' could not be parsed at index 4
	at java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)
	at java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1851)
	at java.time.LocalDate.parse(LocalDate.java:400)
	at com.qf.bigdata.release.util.DateUtil$.dateFromat4StringDiff(DateUtil.scala:26)
	at com.qf.bigdata.release.util.SparkHelper$.rangeDates(SparkHelper.scala:69)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.handleJobs(DWReleaseCustomer.scala:86)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.main(DWReleaseCustomer.scala:105)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer.main(DWReleaseCustomer.scala)
11:47:56.184 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
11:47:56.226 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
11:47:56.226 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
11:47:56.252 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ad99cf3{/SQL,null,AVAILABLE,@Spark}
11:47:56.252 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c90b7b7{/SQL/json,null,AVAILABLE,@Spark}
11:47:56.253 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36bed37a{/SQL/execution,null,AVAILABLE,@Spark}
11:47:56.254 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b3ab4f9{/SQL/execution/json,null,AVAILABLE,@Spark}
11:47:56.256 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@729c8063{/static/sql,null,AVAILABLE,@Spark}
11:47:57.090 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
11:47:59.051 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
11:47:59.212 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
11:48:01.494 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
11:48:04.229 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
11:48:04.257 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
11:48:04.952 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
11:48:04.960 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
11:48:05.096 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
11:48:05.602 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
11:48:05.623 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
11:48:05.910 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
11:48:05.910 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
11:48:06.014 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
11:48:06.014 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
11:48:06.021 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
11:48:06.021 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
11:48:06.026 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
11:48:06.026 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
11:48:06.034 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
11:48:06.034 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
11:48:06.039 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
11:48:06.040 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
11:48:06.047 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
11:48:06.047 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
11:48:06.053 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
11:48:06.053 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
11:48:09.697 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/1ed5ba19-f7d7-4f20-9f82-2fa2cd5c1d74_resources
11:48:09.710 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/1ed5ba19-f7d7-4f20-9f82-2fa2cd5c1d74
11:48:09.714 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/1ed5ba19-f7d7-4f20-9f82-2fa2cd5c1d74
11:48:09.729 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/1ed5ba19-f7d7-4f20-9f82-2fa2cd5c1d74/_tmp_space.db
11:48:09.738 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
11:48:09.832 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:09.832 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:48:09.852 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
11:48:09.852 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
11:48:09.862 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
11:48:10.538 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/537e7a35-2183-41e2-a64d-c927f598b8eb_resources
11:48:10.882 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/537e7a35-2183-41e2-a64d-c927f598b8eb
11:48:10.887 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/537e7a35-2183-41e2-a64d-c927f598b8eb
11:48:10.970 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/537e7a35-2183-41e2-a64d-c927f598b8eb/_tmp_space.db
11:48:10.975 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
11:48:11.286 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
11:48:11.296 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:48:11.871 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:48:11.871 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:48:11.882 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:48:11.882 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:48:12.088 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:48:12.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:48:12.140 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.167 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.169 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.170 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.170 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.171 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.171 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.172 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.172 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.172 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:48:12.297 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
11:48:14.386 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:48:14.417 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:48:14.960 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:48:14.961 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:48:14.963 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:48:14.966 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:48:14.969 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
11:48:15.129 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
11:48:15.227 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
11:48:15.233 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
11:48:15.234 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
11:48:15.252 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
11:48:15.301 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
11:48:15.302 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
11:48:15.303 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
11:48:15.304 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
11:48:15.305 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:48:15.306 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:48:15.469 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:15.469 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:48:15.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:15.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:48:15.557 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:15.557 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:48:15.583 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:15.584 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:48:15.597 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:15.599 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:48:15.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:15.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:48:15.623 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:15.624 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:48:15.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:15.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:48:15.643 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:15.643 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:48:15.651 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:15.651 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:48:15.660 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:15.660 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:48:15.667 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:15.668 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:48:15.676 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:15.677 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:48:15.693 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:15.693 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:48:16.661 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:48:16.662 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:48:16.679 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:48:16.680 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:48:16.726 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:48:16.726 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:48:16.786 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:16.786 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:16.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:16.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:16.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:16.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:16.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:16.788 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:16.788 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:16.788 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:48:16.920 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:48:16.920 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:48:16.944 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:48:16.944 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:48:18.103 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-24)
11:48:18.107 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
11:48:18.111 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:48:18.201 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:48:18.265 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:48:20.455 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 1376.167 ms
11:48:21.638 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 132.4966 ms
11:48:22.301 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 264.7 KB, free 897.3 MB)
11:48:22.662 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.4 KB, free 897.3 MB)
11:48:22.667 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:14778 (size: 22.4 KB, free: 897.6 MB)
11:48:22.679 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:51
11:48:22.756 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
11:48:23.802 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:51
11:48:23.914 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:51)
11:48:23.919 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:51) with 1 output partitions
11:48:23.919 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:51)
11:48:23.920 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
11:48:23.922 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
11:48:23.963 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:51), which has no missing parents
11:48:24.420 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 24.1 KB, free 897.3 MB)
11:48:24.431 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.5 KB, free 897.3 MB)
11:48:24.432 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:14778 (size: 10.5 KB, free: 897.6 MB)
11:48:24.433 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
11:48:24.474 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:51) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:48:24.476 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
11:48:24.648 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5419 bytes)
11:48:24.651 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5419 bytes)
11:48:24.651 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5419 bytes)
11:48:24.652 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5419 bytes)
11:48:24.677 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
11:48:24.677 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
11:48:24.677 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
11:48:24.677 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
11:48:25.201 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 87.027 ms
11:48:25.320 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-24]
11:48:25.320 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-24]
11:48:25.320 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-24]
11:48:25.320 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-24]
11:48:30.259 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
11:48:30.718 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:30.748 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:31.325 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:44.128 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
11:48:44.961 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
11:48:45.010 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
11:48:45.011 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
11:48:45.012 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
11:48:45.014 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
11:48:45.016 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
11:48:46.628 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 14824.
11:48:46.653 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
11:48:46.677 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
11:48:46.681 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
11:48:46.681 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
11:48:46.694 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-56ddd618-9f25-4c86-b9c7-b617bb1c5313
11:48:46.722 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
11:48:46.794 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
11:48:46.995 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @8255ms
11:48:47.096 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
11:48:47.118 INFO  [main] org.spark_project.jetty.server.Server - Started @8382ms
11:48:47.161 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4fdf8f12{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:48:47.161 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
11:48:47.203 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1162410a{/jobs,null,AVAILABLE,@Spark}
11:48:47.206 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b0f7d9d{/jobs/json,null,AVAILABLE,@Spark}
11:48:47.211 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/jobs/job,null,AVAILABLE,@Spark}
11:48:47.216 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/jobs/job/json,null,AVAILABLE,@Spark}
11:48:47.221 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/stages,null,AVAILABLE,@Spark}
11:48:47.222 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/stages/json,null,AVAILABLE,@Spark}
11:48:47.223 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e11ab3d{/stages/stage,null,AVAILABLE,@Spark}
11:48:47.226 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28f8e165{/stages/stage/json,null,AVAILABLE,@Spark}
11:48:47.227 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66f66866{/stages/pool,null,AVAILABLE,@Spark}
11:48:47.235 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d666b41{/stages/pool/json,null,AVAILABLE,@Spark}
11:48:47.236 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30f4b1a6{/storage,null,AVAILABLE,@Spark}
11:48:47.238 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e1162e7{/storage/json,null,AVAILABLE,@Spark}
11:48:47.239 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2f1700{/storage/rdd,null,AVAILABLE,@Spark}
11:48:47.240 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/storage/rdd/json,null,AVAILABLE,@Spark}
11:48:47.241 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/environment,null,AVAILABLE,@Spark}
11:48:47.242 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/environment/json,null,AVAILABLE,@Spark}
11:48:47.243 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@553f3b6e{/executors,null,AVAILABLE,@Spark}
11:48:47.259 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/executors/json,null,AVAILABLE,@Spark}
11:48:47.261 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/executors/threadDump,null,AVAILABLE,@Spark}
11:48:47.262 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/executors/threadDump/json,null,AVAILABLE,@Spark}
11:48:47.279 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bb7cce7{/static,null,AVAILABLE,@Spark}
11:48:47.283 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23aae55{/,null,AVAILABLE,@Spark}
11:48:47.285 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@680bddf5{/api,null,AVAILABLE,@Spark}
11:48:47.287 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53bf7094{/jobs/job/kill,null,AVAILABLE,@Spark}
11:48:47.288 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@710b30ef{/stages/stage/kill,null,AVAILABLE,@Spark}
11:48:47.291 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
11:48:47.536 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
11:48:47.596 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14845.
11:48:47.597 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:14845
11:48:47.599 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
11:48:47.659 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 14845, None)
11:48:47.664 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:14845 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 14845, None)
11:48:47.669 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 14845, None)
11:48:47.670 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 14845, None)
11:48:48.077 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a7761b1{/metrics/json,null,AVAILABLE,@Spark}
11:48:51.762 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
11:48:51.796 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
11:48:51.797 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
11:48:51.807 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@422ad5e2{/SQL,null,AVAILABLE,@Spark}
11:48:51.810 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6048e26a{/SQL/json,null,AVAILABLE,@Spark}
11:48:51.811 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3249e278{/SQL/execution,null,AVAILABLE,@Spark}
11:48:51.812 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@456f7d9e{/SQL/execution/json,null,AVAILABLE,@Spark}
11:48:51.814 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ad99cf3{/static/sql,null,AVAILABLE,@Spark}
11:48:52.305 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
11:48:53.425 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
11:48:53.461 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
11:48:55.288 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
11:48:57.855 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
11:48:57.864 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
11:48:58.311 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
11:48:58.318 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
11:48:58.402 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
11:48:58.561 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
11:48:58.562 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
11:48:58.579 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
11:48:58.580 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
11:48:58.658 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
11:48:58.658 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
11:48:58.663 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
11:48:58.663 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
11:48:58.669 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
11:48:58.669 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
11:48:58.674 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
11:48:58.675 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
11:48:58.679 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
11:48:58.679 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
11:48:58.687 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
11:48:58.687 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
11:48:58.691 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
11:48:58.692 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
11:48:59.642 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/e0614584-5f17-47a5-a460-c3dcc7af12ce_resources
11:48:59.654 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/e0614584-5f17-47a5-a460-c3dcc7af12ce
11:48:59.659 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/e0614584-5f17-47a5-a460-c3dcc7af12ce
11:48:59.677 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/e0614584-5f17-47a5-a460-c3dcc7af12ce/_tmp_space.db
11:48:59.682 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
11:48:59.713 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:59.713 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:48:59.723 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
11:48:59.723 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
11:48:59.728 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
11:49:00.118 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/e548c76f-af3e-42d8-85a8-0f349a10a5c9_resources
11:49:00.221 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/e548c76f-af3e-42d8-85a8-0f349a10a5c9
11:49:00.224 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/e548c76f-af3e-42d8-85a8-0f349a10a5c9
11:49:00.232 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/e548c76f-af3e-42d8-85a8-0f349a10a5c9/_tmp_space.db
11:49:00.237 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
11:49:00.279 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
11:49:00.288 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:49:00.629 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:49:00.630 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:49:00.639 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:49:00.639 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:49:00.756 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:49:00.757 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:49:00.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:00.834 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:00.835 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:00.836 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:00.837 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:00.837 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:00.837 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:00.838 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:00.839 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:00.839 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:49:00.862 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
11:49:01.567 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:49:01.592 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:49:01.593 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:49:01.594 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:49:01.595 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:49:01.596 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:49:01.599 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
11:49:01.651 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
11:49:01.705 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
11:49:01.712 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
11:49:01.713 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
11:49:01.717 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
11:49:01.723 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
11:49:01.724 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
11:49:01.725 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
11:49:01.726 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
11:49:01.728 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:49:01.728 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:49:01.738 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:01.739 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:49:01.758 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:01.759 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:49:01.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:01.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:49:01.796 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:01.797 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:49:01.849 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:01.849 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:49:01.881 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:01.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:49:01.897 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:01.897 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:49:01.915 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:01.916 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:49:01.932 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:01.932 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:49:01.956 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:01.956 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:49:01.974 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:01.975 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:49:01.993 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:01.995 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:49:02.021 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:02.022 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:49:02.041 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:02.047 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:49:02.778 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:49:02.778 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:49:02.787 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:49:02.788 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:49:02.839 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:49:02.839 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:49:02.900 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:02.901 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:02.902 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:02.903 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:02.903 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:02.903 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:02.904 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:02.904 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:02.904 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:02.905 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:49:02.945 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:49:02.945 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:49:02.954 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:49:02.954 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:49:03.353 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-24)
11:49:03.356 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
11:49:03.360 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:49:03.378 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:49:03.385 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:49:04.159 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 501.3535 ms
11:49:04.256 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
11:49:05.567 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 304.6383 ms
11:49:05.697 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 264.7 KB, free 897.3 MB)
11:49:05.920 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.4 KB, free 897.3 MB)
11:49:05.924 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:14845 (size: 22.4 KB, free: 897.6 MB)
11:49:05.932 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:51
11:49:05.947 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
11:49:06.139 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:51
11:49:06.170 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:51)
11:49:06.173 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:51) with 1 output partitions
11:49:06.174 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:51)
11:49:06.174 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
11:49:06.177 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
11:49:06.183 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:51), which has no missing parents
11:49:06.361 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 24.1 KB, free 897.3 MB)
11:49:06.372 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.5 KB, free 897.3 MB)
11:49:06.380 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:14845 (size: 10.5 KB, free: 897.6 MB)
11:49:06.381 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
11:49:06.410 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:51) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:49:06.411 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
11:49:06.482 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5419 bytes)
11:49:06.485 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5419 bytes)
11:49:06.485 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5419 bytes)
11:49:06.488 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5419 bytes)
11:49:06.573 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
11:49:06.573 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
11:49:06.573 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
11:49:06.573 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
11:49:06.979 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 81.3572 ms
11:49:07.035 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-24]
11:49:07.038 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-24]
11:49:07.039 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-24]
11:49:07.039 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-24]
11:49:13.564 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:49:13.564 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:49:13.564 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:49:13.564 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:49:16.033 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1611 bytes result sent to driver
11:49:16.033 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1568 bytes result sent to driver
11:49:16.033 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1611 bytes result sent to driver
11:49:16.269 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 9713 ms on localhost (executor driver) (1/4)
11:49:16.315 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 9828 ms on localhost (executor driver) (2/4)
11:49:16.316 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 9846 ms on localhost (executor driver) (3/4)
11:49:40.523 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1826 bytes result sent to driver
11:49:40.600 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 34115 ms on localhost (executor driver) (4/4)
11:49:40.647 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DWReleaseCustomer.scala:51) finished in 34.161 s
11:49:40.647 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
11:49:40.661 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:49:40.676 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:49:40.683 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
11:49:40.688 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:49:40.736 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:51), which has no missing parents
11:49:40.855 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
11:49:40.866 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
11:49:40.869 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:14845 (size: 2.2 KB, free: 897.6 MB)
11:49:40.877 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
11:49:40.889 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:51) (first 15 tasks are for partitions Vector(0))
11:49:40.889 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
11:49:40.900 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:49:40.903 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
11:49:41.052 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
11:49:41.068 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 68 ms
11:49:41.273 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 2466 bytes result sent to driver
11:49:41.274 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 380 ms on localhost (executor driver) (1/1)
11:49:41.275 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
11:49:41.276 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:51) finished in 0.383 s
11:49:41.322 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:51, took 35.181563 s
11:49:41.514 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
11:49:41.554 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@4fdf8f12{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:49:41.581 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
11:49:41.696 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
11:49:41.799 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
11:49:41.813 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
11:49:41.815 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
11:49:41.844 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
11:49:41.858 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
11:49:41.859 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
11:49:41.867 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-bab0ab13-f7eb-4582-a81a-7f8ab6e6aa73
11:54:58.689 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
11:54:59.427 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
11:54:59.539 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
11:54:59.540 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
11:54:59.542 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
11:54:59.543 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
11:54:59.544 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
11:55:01.743 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 14910.
11:55:01.821 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
11:55:01.871 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
11:55:01.884 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
11:55:01.884 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
11:55:01.907 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-fe6b7a8e-312c-4bb4-b66b-c44b8857b1d2
11:55:01.980 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
11:55:02.160 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
11:55:02.461 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @13499ms
11:55:02.640 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
11:55:02.666 INFO  [main] org.spark_project.jetty.server.Server - Started @13704ms
11:55:02.722 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4fa0b449{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:55:02.722 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
11:55:02.766 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/jobs,null,AVAILABLE,@Spark}
11:55:02.767 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/jobs/json,null,AVAILABLE,@Spark}
11:55:02.769 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/jobs/job,null,AVAILABLE,@Spark}
11:55:02.771 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/jobs/job/json,null,AVAILABLE,@Spark}
11:55:02.772 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ea1fda{/stages,null,AVAILABLE,@Spark}
11:55:02.773 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@420745d7{/stages/json,null,AVAILABLE,@Spark}
11:55:02.774 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fa47fea{/stages/stage,null,AVAILABLE,@Spark}
11:55:02.776 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545f80bf{/stages/stage/json,null,AVAILABLE,@Spark}
11:55:02.777 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22fa55b2{/stages/pool,null,AVAILABLE,@Spark}
11:55:02.778 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6594402a{/stages/pool/json,null,AVAILABLE,@Spark}
11:55:02.782 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@405325cf{/storage,null,AVAILABLE,@Spark}
11:55:02.783 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c3f01f{/storage/json,null,AVAILABLE,@Spark}
11:55:02.784 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/storage/rdd,null,AVAILABLE,@Spark}
11:55:02.785 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/storage/rdd/json,null,AVAILABLE,@Spark}
11:55:02.786 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/environment,null,AVAILABLE,@Spark}
11:55:02.787 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5298dead{/environment/json,null,AVAILABLE,@Spark}
11:55:02.788 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/executors,null,AVAILABLE,@Spark}
11:55:02.789 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/executors/json,null,AVAILABLE,@Spark}
11:55:02.790 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/executors/threadDump,null,AVAILABLE,@Spark}
11:55:02.791 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b986cd{/executors/threadDump/json,null,AVAILABLE,@Spark}
11:55:02.806 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41c62850{/static,null,AVAILABLE,@Spark}
11:55:02.807 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f574cc2{/,null,AVAILABLE,@Spark}
11:55:02.808 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a9c84a5{/api,null,AVAILABLE,@Spark}
11:55:02.814 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f1249d{/jobs/job/kill,null,AVAILABLE,@Spark}
11:55:02.815 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a68df9{/stages/stage/kill,null,AVAILABLE,@Spark}
11:55:02.819 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
11:55:03.117 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
11:55:03.220 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14931.
11:55:03.228 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:14931
11:55:03.232 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
11:55:03.320 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 14931, None)
11:55:03.326 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:14931 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 14931, None)
11:55:03.347 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 14931, None)
11:55:03.348 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 14931, None)
11:55:03.716 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e850122{/metrics/json,null,AVAILABLE,@Spark}
11:55:04.086 ERROR [main] com.lzz.release.util.SparkHelper$ - Text '2019-09-24' could not be parsed at index 4
java.time.format.DateTimeParseException: Text '2019-09-24' could not be parsed at index 4
	at java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)
	at java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1851)
	at java.time.LocalDate.parse(LocalDate.java:400)
	at com.lzz.release.util.DateUtil$.dateFormat2StringDiff(DateUtil.scala:35)
	at com.lzz.release.util.SparkHelper$.rangeDates(SparkHelper.scala:83)
	at com.lzz.release.etl.release.dw.DWReleaseCustomer$.handleJobs(DWReleaseCustomer.scala:102)
	at com.lzz.release.etl.release.dw.DWReleaseCustomer$.main(DWReleaseCustomer.scala:120)
	at com.lzz.release.etl.release.dw.DWReleaseCustomer.main(DWReleaseCustomer.scala)
11:55:09.125 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
11:55:09.203 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
11:55:09.206 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
11:55:09.227 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@456f7d9e{/SQL,null,AVAILABLE,@Spark}
11:55:09.227 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8f40022{/SQL/json,null,AVAILABLE,@Spark}
11:55:09.228 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f31df32{/SQL/execution,null,AVAILABLE,@Spark}
11:55:09.229 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d7911d5{/SQL/execution/json,null,AVAILABLE,@Spark}
11:55:09.258 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b3ab4f9{/static/sql,null,AVAILABLE,@Spark}
11:55:10.262 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
11:55:13.910 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
11:55:15.195 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
11:55:23.596 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
11:55:25.954 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
11:55:25.958 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
11:55:26.777 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
11:55:26.784 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
11:55:26.954 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
11:55:27.242 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
11:55:27.244 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
11:55:27.303 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
11:55:27.303 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
11:55:27.432 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
11:55:27.433 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
11:55:27.441 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
11:55:27.442 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
11:55:27.451 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
11:55:27.452 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
11:55:27.465 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
11:55:27.466 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
11:55:27.475 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
11:55:27.475 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
11:55:27.485 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
11:55:27.485 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
11:55:27.496 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
11:55:27.497 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
11:55:29.007 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/46a1a794-a82e-424b-be25-9677923cd996_resources
11:55:29.043 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/46a1a794-a82e-424b-be25-9677923cd996
11:55:29.047 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/46a1a794-a82e-424b-be25-9677923cd996
11:55:29.065 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/46a1a794-a82e-424b-be25-9677923cd996/_tmp_space.db
11:55:29.071 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
11:55:29.167 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:55:29.167 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:55:29.178 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
11:55:29.178 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
11:55:29.184 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
11:55:29.755 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/ce20b1bc-589c-4583-a625-97d2402df44e_resources
11:55:29.765 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/ce20b1bc-589c-4583-a625-97d2402df44e
11:55:29.771 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/ce20b1bc-589c-4583-a625-97d2402df44e
11:55:29.809 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/ce20b1bc-589c-4583-a625-97d2402df44e/_tmp_space.db
11:55:29.814 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
11:55:30.083 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
11:55:30.114 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:55:30.541 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:55:30.542 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:55:30.550 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:55:30.551 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:55:31.050 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:55:31.050 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:55:31.110 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:31.142 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:31.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:31.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:31.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:31.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:31.148 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:31.148 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:31.148 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:31.148 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:55:31.214 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
11:55:32.493 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:55:32.516 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:55:32.518 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:55:32.522 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:55:32.522 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:55:32.523 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:55:32.524 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') idcard
11:55:32.555 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy') as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as age
11:55:32.608 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})',2) as int)%2 as gender
11:55:32.615 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') area_code
11:55:32.616 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') longitude
11:55:32.617 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') latitude
11:55:32.618 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') matter_id
11:55:32.619 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') model_code
11:55:32.620 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') model_version
11:55:32.622 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
11:55:32.627 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:55:32.628 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:55:32.652 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:55:32.652 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:55:32.664 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:55:32.664 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:55:32.676 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:55:32.676 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:55:32.698 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:55:32.698 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:55:32.725 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:55:32.726 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:55:32.738 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:55:32.739 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:55:32.756 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:55:32.756 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:55:32.764 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:55:32.764 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:55:32.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:55:32.778 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:55:32.797 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:55:32.798 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:55:32.811 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:55:32.812 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:55:32.821 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:55:32.822 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:55:32.836 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:55:32.837 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:55:32.850 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:55:32.850 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:55:34.006 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:55:34.006 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:55:34.016 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:55:34.016 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:55:34.072 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:55:34.073 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:55:34.117 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:34.118 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:34.118 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:34.119 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:34.119 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:34.119 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:34.119 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:34.120 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:34.120 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:55:34.120 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:55:34.208 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:55:34.209 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:55:34.233 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:55:34.233 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:55:35.072 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-24)
11:55:35.076 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
11:55:35.081 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:55:35.138 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:55:35.151 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:55:36.200 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 523.4872 ms
11:55:36.851 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 117.0387 ms
11:55:37.741 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 264.7 KB, free 897.3 MB)
11:55:38.015 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.4 KB, free 897.3 MB)
11:55:38.020 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:14931 (size: 22.4 KB, free: 897.6 MB)
11:55:38.036 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:62
11:55:38.083 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
11:55:38.624 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:62
11:55:38.719 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:62)
11:55:38.723 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:62) with 1 output partitions
11:55:38.723 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:62)
11:55:38.724 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
11:55:38.727 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
11:55:38.762 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:62), which has no missing parents
11:55:39.108 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 24.1 KB, free 897.3 MB)
11:55:39.114 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.5 KB, free 897.3 MB)
11:55:39.116 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:14931 (size: 10.5 KB, free: 897.6 MB)
11:55:39.117 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
11:55:39.163 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:62) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:55:39.166 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
11:55:39.306 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5419 bytes)
11:55:39.313 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5419 bytes)
11:55:39.314 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5419 bytes)
11:55:39.315 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5419 bytes)
11:55:39.343 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
11:55:39.343 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
11:55:39.345 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
11:55:39.345 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
11:55:39.931 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 144.6668 ms
11:55:40.040 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-24]
11:55:40.042 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-24]
11:55:40.042 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-24]
11:55:40.042 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-24]
11:55:42.714 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
11:55:42.836 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:55:42.842 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:55:42.843 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:55:42.844 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:55:43.152 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1611 bytes result sent to driver
11:55:43.152 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1611 bytes result sent to driver
11:55:43.152 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1611 bytes result sent to driver
11:55:43.202 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 3887 ms on localhost (executor driver) (1/4)
11:55:43.213 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 3899 ms on localhost (executor driver) (2/4)
11:55:43.216 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 3938 ms on localhost (executor driver) (3/4)
11:55:56.727 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1740 bytes result sent to driver
11:55:56.732 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 17420 ms on localhost (executor driver) (4/4)
11:55:57.326 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DWReleaseCustomer.scala:62) finished in 17.491 s
11:55:57.491 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
11:55:57.491 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:55:59.161 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:55:59.779 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
11:56:00.839 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:56:02.788 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62), which has no missing parents
11:56:04.471 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
11:56:04.474 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
11:56:04.476 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:14931 (size: 2.2 KB, free: 897.6 MB)
11:56:04.477 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
11:56:04.480 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62) (first 15 tasks are for partitions Vector(0))
11:56:04.480 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
11:56:04.484 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:56:04.485 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
11:56:06.010 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
11:56:06.294 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 730 ms
11:56:06.978 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 2509 bytes result sent to driver
11:56:06.980 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 2498 ms on localhost (executor driver) (1/1)
11:56:06.980 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
11:56:06.982 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:62) finished in 2.500 s
11:56:07.356 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:62, took 28.730921 s
11:56:08.891 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
11:56:09.270 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@4fa0b449{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:56:09.320 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
11:56:10.287 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
11:56:10.552 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
11:56:10.593 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
11:56:10.594 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
11:56:10.964 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
11:56:10.968 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
11:56:11.004 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
11:56:11.089 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-42e82ccb-b39b-41a9-951a-5dc2656edc23
11:56:37.502 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
11:56:38.537 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
11:56:38.666 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
11:56:38.668 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
11:56:38.670 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
11:56:38.671 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
11:56:38.672 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
11:56:40.698 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 14987.
11:56:40.787 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
11:56:40.877 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
11:56:40.882 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
11:56:40.883 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
11:56:40.902 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-97f6d611-773c-4bf9-9661-c577cae2b901
11:56:40.999 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
11:56:41.188 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
11:56:41.585 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @23071ms
11:56:42.214 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
11:56:42.279 INFO  [main] org.spark_project.jetty.server.Server - Started @23766ms
11:56:42.355 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@54f5f647{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:56:42.355 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
11:56:42.493 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62df0ff3{/jobs,null,AVAILABLE,@Spark}
11:56:42.495 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/jobs/json,null,AVAILABLE,@Spark}
11:56:42.496 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/jobs/job,null,AVAILABLE,@Spark}
11:56:42.497 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/jobs/job/json,null,AVAILABLE,@Spark}
11:56:42.506 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/stages,null,AVAILABLE,@Spark}
11:56:42.506 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e11ab3d{/stages/json,null,AVAILABLE,@Spark}
11:56:42.507 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2392212b{/stages/stage,null,AVAILABLE,@Spark}
11:56:42.512 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66f66866{/stages/stage/json,null,AVAILABLE,@Spark}
11:56:42.513 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d666b41{/stages/pool,null,AVAILABLE,@Spark}
11:56:42.515 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30f4b1a6{/stages/pool/json,null,AVAILABLE,@Spark}
11:56:42.516 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e1162e7{/storage,null,AVAILABLE,@Spark}
11:56:42.517 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2f1700{/storage/json,null,AVAILABLE,@Spark}
11:56:42.518 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/storage/rdd,null,AVAILABLE,@Spark}
11:56:42.522 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/storage/rdd/json,null,AVAILABLE,@Spark}
11:56:42.523 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/environment,null,AVAILABLE,@Spark}
11:56:42.525 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@553f3b6e{/environment/json,null,AVAILABLE,@Spark}
11:56:42.526 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/executors,null,AVAILABLE,@Spark}
11:56:42.528 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/executors/json,null,AVAILABLE,@Spark}
11:56:42.534 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/executors/threadDump,null,AVAILABLE,@Spark}
11:56:43.213 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bb7cce7{/executors/threadDump/json,null,AVAILABLE,@Spark}
11:56:43.260 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b530eb9{/static,null,AVAILABLE,@Spark}
11:56:43.263 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@680bddf5{/,null,AVAILABLE,@Spark}
11:56:43.264 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d83c5a5{/api,null,AVAILABLE,@Spark}
11:56:43.266 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@710b30ef{/jobs/job/kill,null,AVAILABLE,@Spark}
11:56:43.267 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28b576a9{/stages/stage/kill,null,AVAILABLE,@Spark}
11:56:43.272 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
11:56:43.644 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
11:56:43.776 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1031.
11:56:43.789 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:1031
11:56:43.798 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
11:56:43.914 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 1031, None)
11:56:43.926 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:1031 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 1031, None)
11:56:43.987 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 1031, None)
11:56:43.988 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 1031, None)
11:56:44.382 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27fde870{/metrics/json,null,AVAILABLE,@Spark}
11:56:49.740 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
11:56:49.834 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
11:56:49.835 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
11:56:49.865 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@422ad5e2{/SQL,null,AVAILABLE,@Spark}
11:56:49.866 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6048e26a{/SQL/json,null,AVAILABLE,@Spark}
11:56:49.867 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3249e278{/SQL/execution,null,AVAILABLE,@Spark}
11:56:49.868 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@456f7d9e{/SQL/execution/json,null,AVAILABLE,@Spark}
11:56:49.875 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ad99cf3{/static/sql,null,AVAILABLE,@Spark}
11:56:51.124 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
11:56:52.861 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
11:56:52.988 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
11:56:54.870 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
11:56:58.855 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
11:56:58.859 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
11:56:59.763 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
11:56:59.771 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
11:56:59.905 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
11:57:00.198 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
11:57:00.200 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
11:57:00.235 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
11:57:00.235 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
11:57:00.351 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
11:57:00.351 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
11:57:00.358 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
11:57:00.358 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
11:57:00.364 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
11:57:00.365 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
11:57:00.372 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
11:57:00.373 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
11:57:00.379 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
11:57:00.380 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
11:57:00.385 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
11:57:00.385 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
11:57:00.391 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
11:57:00.391 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
11:57:02.625 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/d0a76b63-cea2-4dc6-a25f-dc3e45664231_resources
11:57:02.636 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/d0a76b63-cea2-4dc6-a25f-dc3e45664231
11:57:02.640 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/d0a76b63-cea2-4dc6-a25f-dc3e45664231
11:57:02.662 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/d0a76b63-cea2-4dc6-a25f-dc3e45664231/_tmp_space.db
11:57:02.667 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
11:57:02.765 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:57:02.765 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:57:02.774 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
11:57:02.774 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
11:57:02.782 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
11:57:03.245 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/61961dd5-1123-4e8b-a8be-af6db74f8e21_resources
11:57:03.271 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/61961dd5-1123-4e8b-a8be-af6db74f8e21
11:57:03.278 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/61961dd5-1123-4e8b-a8be-af6db74f8e21
11:57:03.285 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/61961dd5-1123-4e8b-a8be-af6db74f8e21/_tmp_space.db
11:57:03.288 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
11:57:03.462 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
11:57:03.485 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:57:03.993 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:57:03.994 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:57:04.004 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:57:04.004 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:57:04.212 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:57:04.212 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:57:04.277 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:04.297 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:04.297 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:04.298 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:04.298 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:04.298 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:04.298 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:04.301 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:04.301 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:04.301 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:57:04.358 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
11:57:06.401 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:57:06.418 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:57:06.419 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:57:06.420 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:57:06.421 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:57:06.421 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:57:06.422 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') idcard
11:57:06.467 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy') as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as age
11:57:06.516 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})',2) as int)%2 as gender
11:57:06.521 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') area_code
11:57:06.522 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') longitude
11:57:06.524 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') latitude
11:57:06.526 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') matter_id
11:57:06.527 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') model_code
11:57:06.531 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') model_version
11:57:06.533 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
11:57:06.534 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:57:06.534 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:57:06.548 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:57:06.549 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:57:06.569 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:57:06.569 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:57:06.580 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:57:06.581 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:57:06.588 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:57:06.588 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:57:06.598 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:57:06.598 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:57:06.608 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:57:06.608 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:57:06.618 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:57:06.618 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:57:06.633 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:57:06.633 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:57:06.649 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:57:06.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:57:06.679 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:57:06.680 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:57:06.703 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:57:06.704 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:57:06.723 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:57:06.724 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:57:06.742 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:57:06.743 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:57:06.759 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:57:06.762 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
11:57:07.667 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:57:07.667 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:57:07.679 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:57:07.679 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:57:07.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:57:07.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:57:07.790 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:07.791 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:07.791 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:07.791 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:07.792 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:07.792 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:07.792 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:07.793 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:07.793 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:57:07.793 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:57:07.869 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:57:07.873 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:57:07.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:57:07.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:57:08.801 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-24)
11:57:08.803 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
11:57:08.806 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:57:08.878 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:57:08.890 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:57:09.896 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 418.2034 ms
11:57:10.545 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 102.2387 ms
11:57:10.884 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 264.7 KB, free 897.3 MB)
11:57:11.063 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.4 KB, free 897.3 MB)
11:57:11.067 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:1031 (size: 22.4 KB, free: 897.6 MB)
11:57:11.086 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:62
11:57:11.114 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
11:57:11.475 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:62
11:57:11.532 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:62)
11:57:11.536 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:62) with 1 output partitions
11:57:11.536 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:62)
11:57:11.537 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
11:57:11.539 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
11:57:11.560 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:62), which has no missing parents
11:57:11.867 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 24.1 KB, free 897.3 MB)
11:57:11.873 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.5 KB, free 897.3 MB)
11:57:11.874 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:1031 (size: 10.5 KB, free: 897.6 MB)
11:57:11.875 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
11:57:11.890 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:62) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:57:11.893 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
11:57:11.961 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
11:57:12.003 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5419 bytes)
11:57:12.006 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5419 bytes)
11:57:12.007 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5419 bytes)
11:57:12.016 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5419 bytes)
11:57:12.071 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
11:57:12.072 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
11:57:12.072 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
11:57:12.073 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
11:57:12.550 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 95.4115 ms
11:57:12.636 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-24]
11:57:12.636 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-24]
11:57:12.636 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-24]
11:57:12.636 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-24]
11:57:15.632 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:57:15.636 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:57:15.637 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:57:15.643 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:57:16.099 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1611 bytes result sent to driver
11:57:16.100 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1611 bytes result sent to driver
11:57:16.100 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1611 bytes result sent to driver
11:57:16.125 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 4112 ms on localhost (executor driver) (1/4)
11:57:16.141 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 4134 ms on localhost (executor driver) (2/4)
11:57:16.148 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 4164 ms on localhost (executor driver) (3/4)
11:57:25.938 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1826 bytes result sent to driver
11:57:25.982 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 13975 ms on localhost (executor driver) (4/4)
11:57:26.041 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DWReleaseCustomer.scala:62) finished in 14.027 s
11:57:26.041 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
11:57:26.046 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:57:26.058 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:57:26.061 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
11:57:26.062 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:57:26.195 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62), which has no missing parents
11:57:26.298 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
11:57:26.302 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
11:57:26.306 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:1031 (size: 2.2 KB, free: 897.6 MB)
11:57:26.309 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
11:57:26.314 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62) (first 15 tasks are for partitions Vector(0))
11:57:26.314 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
11:57:26.318 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:57:26.318 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
11:57:26.535 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
11:57:26.551 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 95 ms
11:57:26.937 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 2767 bytes result sent to driver
11:57:26.938 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 623 ms on localhost (executor driver) (1/1)
11:57:26.939 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
11:57:26.939 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:62) finished in 0.624 s
11:57:27.015 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:62, took 15.539941 s
11:57:27.220 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
11:57:27.257 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@54f5f647{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:57:27.276 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
11:57:27.411 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
11:57:27.515 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
11:57:27.519 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
11:57:27.520 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
11:57:27.527 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
11:57:27.532 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
11:57:27.533 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
11:57:27.539 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-019874e7-1881-4c79-ae7f-435f2d127d62
14:39:05.182 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
14:39:06.382 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
14:39:06.574 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
14:39:06.620 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
14:39:06.621 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
14:39:06.622 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
14:39:06.624 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
14:39:08.792 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 4570.
14:39:08.936 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
14:39:09.103 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
14:39:09.119 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
14:39:09.120 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
14:39:09.180 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-92d3be91-4793-414e-a241-05d6c8e46966
14:39:09.329 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
14:39:09.494 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
14:39:09.876 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @39459ms
14:39:10.071 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
14:39:10.104 INFO  [main] org.spark_project.jetty.server.Server - Started @39690ms
14:39:10.147 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@2dfc7c1a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
14:39:10.148 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
14:39:10.195 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62df0ff3{/jobs,null,AVAILABLE,@Spark}
14:39:10.196 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/jobs/json,null,AVAILABLE,@Spark}
14:39:10.200 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/jobs/job,null,AVAILABLE,@Spark}
14:39:10.201 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/jobs/job/json,null,AVAILABLE,@Spark}
14:39:10.202 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/stages,null,AVAILABLE,@Spark}
14:39:10.203 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e11ab3d{/stages/json,null,AVAILABLE,@Spark}
14:39:10.204 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2392212b{/stages/stage,null,AVAILABLE,@Spark}
14:39:10.206 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66f66866{/stages/stage/json,null,AVAILABLE,@Spark}
14:39:10.207 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d666b41{/stages/pool,null,AVAILABLE,@Spark}
14:39:10.212 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30f4b1a6{/stages/pool/json,null,AVAILABLE,@Spark}
14:39:10.219 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e1162e7{/storage,null,AVAILABLE,@Spark}
14:39:10.220 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2f1700{/storage/json,null,AVAILABLE,@Spark}
14:39:10.225 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/storage/rdd,null,AVAILABLE,@Spark}
14:39:10.228 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/storage/rdd/json,null,AVAILABLE,@Spark}
14:39:10.230 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/environment,null,AVAILABLE,@Spark}
14:39:10.233 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@553f3b6e{/environment/json,null,AVAILABLE,@Spark}
14:39:10.233 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/executors,null,AVAILABLE,@Spark}
14:39:10.234 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/executors/json,null,AVAILABLE,@Spark}
14:39:10.242 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/executors/threadDump,null,AVAILABLE,@Spark}
14:39:10.243 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bb7cce7{/executors/threadDump/json,null,AVAILABLE,@Spark}
14:39:10.257 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b530eb9{/static,null,AVAILABLE,@Spark}
14:39:10.259 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@680bddf5{/,null,AVAILABLE,@Spark}
14:39:10.264 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d83c5a5{/api,null,AVAILABLE,@Spark}
14:39:10.265 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@710b30ef{/jobs/job/kill,null,AVAILABLE,@Spark}
14:39:10.266 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28b576a9{/stages/stage/kill,null,AVAILABLE,@Spark}
14:39:10.269 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
14:39:10.730 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
14:39:10.926 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4592.
14:39:10.927 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:4592
14:39:10.942 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
14:39:11.013 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 4592, None)
14:39:11.050 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:4592 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 4592, None)
14:39:11.076 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 4592, None)
14:39:11.076 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 4592, None)
14:39:11.623 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27fde870{/metrics/json,null,AVAILABLE,@Spark}
14:39:17.576 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
14:39:17.645 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
14:39:17.645 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
14:39:17.690 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5382184b{/SQL,null,AVAILABLE,@Spark}
14:39:17.691 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2213639b{/SQL/json,null,AVAILABLE,@Spark}
14:39:17.692 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3a2f6{/SQL/execution,null,AVAILABLE,@Spark}
14:39:17.693 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@317890ea{/SQL/execution/json,null,AVAILABLE,@Spark}
14:39:17.694 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6048e26a{/static/sql,null,AVAILABLE,@Spark}
14:39:19.146 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
14:39:21.836 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
14:39:21.899 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
14:39:25.429 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
14:39:28.648 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
14:39:28.671 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
14:39:29.524 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
14:39:29.538 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
14:39:29.712 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
14:39:29.983 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
14:39:29.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
14:39:30.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
14:39:30.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
14:39:30.173 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
14:39:30.173 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
14:39:30.178 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
14:39:30.179 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
14:39:30.187 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
14:39:30.187 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
14:39:30.194 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
14:39:30.194 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
14:39:30.201 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
14:39:30.201 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
14:39:30.208 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
14:39:30.208 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
14:39:30.215 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
14:39:30.215 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
14:39:32.653 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/88568f30-0bea-4e0e-94d3-eda3f8ccdb81_resources
14:39:32.736 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/88568f30-0bea-4e0e-94d3-eda3f8ccdb81
14:39:32.740 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/88568f30-0bea-4e0e-94d3-eda3f8ccdb81
14:39:32.773 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/88568f30-0bea-4e0e-94d3-eda3f8ccdb81/_tmp_space.db
14:39:32.841 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
14:39:33.476 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:39:33.476 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:39:33.591 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
14:39:33.591 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
14:39:33.680 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
14:39:34.751 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/615f9210-82e7-42e9-9532-29eb52fcb8ff_resources
14:39:34.790 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/615f9210-82e7-42e9-9532-29eb52fcb8ff
14:39:34.796 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/615f9210-82e7-42e9-9532-29eb52fcb8ff
14:39:34.814 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/615f9210-82e7-42e9-9532-29eb52fcb8ff/_tmp_space.db
14:39:34.818 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
14:39:35.202 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
14:39:35.241 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
14:39:36.216 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
14:39:36.217 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
14:39:36.262 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
14:39:36.263 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
14:39:36.744 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
14:39:36.745 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
14:39:36.791 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:39:36.874 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:39:36.874 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:39:36.875 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:39:36.875 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:39:36.875 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:39:36.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:39:36.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:39:36.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
14:39:36.877 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:39:36.877 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:39:36.877 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:39:36.879 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:39:36.880 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:39:36.881 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:39:36.882 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:39:36.882 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:39:36.883 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
14:39:36.980 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
14:39:39.518 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
14:39:39.541 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
14:39:39.543 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
14:39:39.544 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
14:39:39.545 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
14:39:39.548 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
14:39:39.549 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') idcard
14:39:39.669 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy') as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as age
14:39:39.753 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})',2) as int)%2 as gender
14:39:39.769 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') area_code
14:39:39.774 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') longitude
14:39:39.787 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') latitude
14:39:39.788 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') matter_id
14:39:39.790 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') model_code
14:39:39.796 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') model_version
14:39:39.798 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
14:39:39.803 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
14:39:39.804 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
14:39:39.847 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:39:39.847 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:39:39.863 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:39:39.863 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:39:39.877 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:39:39.877 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:39:39.885 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:39:39.885 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:39:39.900 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:39:39.900 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:39:39.915 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:39:39.915 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:39:39.926 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:39:39.926 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:39:39.936 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:39:39.937 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:39:39.944 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:39:39.944 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:39:39.951 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:39:39.952 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:39:39.961 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:39:39.962 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:39:39.977 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:39:39.977 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:39:39.988 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:39:39.990 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:39:40.003 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:39:40.004 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:39:40.442 ERROR [main] com.lzz.release.etl.release.dm.DMReleaseCustomer$ - cannot resolve '`exts`' given input columns: [device_type, model_version, latitude, gender, release_session, idcard, release_status, area_code, sources, model_code, device_num, longitude, age, channels, aid, matter_id, ct, bdp_day]; line 1 pos 16;
'Project [release_session#0, release_status#1, device_num#2, device_type#3, sources#4, channels#5, 'get_json_object('exts, $.idcard) AS idcard#55, (cast(date_format(current_timestamp(), yyyy, Some(Asia/Shanghai)) as int) - cast('regexp_extract('get_json_object('exts, $.idcard), (\d{6})(\d{4}), 2) as int)) AS age#56, (cast('regexp_extract('get_json_object('exts, $.idcard), (\d{16})(\d{1}), 2) as int) % 2) AS gender#57, 'get_json_object('exts, $.area_code) AS area_code#58, 'get_json_object('exts, $.longitude) AS longitude#59, 'get_json_object('exts, $.latitude) AS latitude#60, 'get_json_object('exts, $.matter_id) AS matter_id#61, 'get_json_object('exts, $.model_code) AS model_code#62, 'get_json_object('exts, $.model_version) AS model_version#63, 'get_json_object('exts, $.aid) AS aid#64, ct#16L, bdp_day#17]
+- SubqueryAlias dw_release_customer
   +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,idcard#6,age#7,gender#8,area_code#9,longitude#10,latitude#11,matter_id#12,model_code#13,model_version#14,aid#15,ct#16L,bdp_day#17] parquet

org.apache.spark.sql.AnalysisException: cannot resolve '`exts`' given input columns: [device_type, model_version, latitude, gender, release_session, idcard, release_status, area_code, sources, model_code, device_num, longitude, age, channels, aid, matter_id, ct, bdp_day]; line 1 pos 16;
'Project [release_session#0, release_status#1, device_num#2, device_type#3, sources#4, channels#5, 'get_json_object('exts, $.idcard) AS idcard#55, (cast(date_format(current_timestamp(), yyyy, Some(Asia/Shanghai)) as int) - cast('regexp_extract('get_json_object('exts, $.idcard), (\d{6})(\d{4}), 2) as int)) AS age#56, (cast('regexp_extract('get_json_object('exts, $.idcard), (\d{16})(\d{1}), 2) as int) % 2) AS gender#57, 'get_json_object('exts, $.area_code) AS area_code#58, 'get_json_object('exts, $.longitude) AS longitude#59, 'get_json_object('exts, $.latitude) AS latitude#60, 'get_json_object('exts, $.matter_id) AS matter_id#61, 'get_json_object('exts, $.model_code) AS model_code#62, 'get_json_object('exts, $.model_version) AS model_version#63, 'get_json_object('exts, $.aid) AS aid#64, ct#16L, bdp_day#17]
+- SubqueryAlias dw_release_customer
   +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,idcard#6,age#7,gender#8,area_code#9,longitude#10,latitude#11,matter_id#12,model_code#13,model_version#14,aid#15,ct#16L,bdp_day#17] parquet

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2888)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1154)
	at org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1189)
	at com.lzz.release.util.SparkHelper$.readTableData(SparkHelper.scala:31)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.handleReleaseJob(DMReleaseCustomer.scala:46)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$$anonfun$handleJobs$1.apply(DMReleaseCustomer.scala:81)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$$anonfun$handleJobs$1.apply(DMReleaseCustomer.scala:79)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.handleJobs(DMReleaseCustomer.scala:79)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.main(DMReleaseCustomer.scala:100)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer.main(DMReleaseCustomer.scala)
14:39:40.491 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@2dfc7c1a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
14:39:40.501 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
14:39:40.593 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
14:39:40.662 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
14:39:40.679 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
14:39:40.688 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
14:39:40.724 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
14:39:40.730 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
14:39:40.760 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
14:39:40.762 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-90fa45b0-b222-4039-bfaa-355c45dd0ad3
14:47:36.210 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
14:47:37.568 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
14:47:37.676 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
14:47:37.678 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
14:47:37.679 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
14:47:37.680 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
14:47:37.681 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
14:47:39.883 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 4674.
14:47:39.984 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
14:47:40.073 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
14:47:40.091 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
14:47:40.092 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
14:47:40.114 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-9e57127c-ecee-43e7-8b65-7cb4ff5b9629
14:47:40.244 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
14:47:40.397 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
14:47:40.848 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @18895ms
14:47:41.245 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
14:47:41.303 INFO  [main] org.spark_project.jetty.server.Server - Started @19352ms
14:47:41.378 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@54f5f647{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
14:47:41.379 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
14:47:41.458 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62df0ff3{/jobs,null,AVAILABLE,@Spark}
14:47:41.460 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/jobs/json,null,AVAILABLE,@Spark}
14:47:41.461 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/jobs/job,null,AVAILABLE,@Spark}
14:47:41.473 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/jobs/job/json,null,AVAILABLE,@Spark}
14:47:41.475 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/stages,null,AVAILABLE,@Spark}
14:47:41.477 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e11ab3d{/stages/json,null,AVAILABLE,@Spark}
14:47:41.480 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2392212b{/stages/stage,null,AVAILABLE,@Spark}
14:47:41.506 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66f66866{/stages/stage/json,null,AVAILABLE,@Spark}
14:47:41.508 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d666b41{/stages/pool,null,AVAILABLE,@Spark}
14:47:41.515 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30f4b1a6{/stages/pool/json,null,AVAILABLE,@Spark}
14:47:41.523 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e1162e7{/storage,null,AVAILABLE,@Spark}
14:47:41.524 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2f1700{/storage/json,null,AVAILABLE,@Spark}
14:47:41.527 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/storage/rdd,null,AVAILABLE,@Spark}
14:47:41.540 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/storage/rdd/json,null,AVAILABLE,@Spark}
14:47:41.548 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/environment,null,AVAILABLE,@Spark}
14:47:41.565 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@553f3b6e{/environment/json,null,AVAILABLE,@Spark}
14:47:41.571 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/executors,null,AVAILABLE,@Spark}
14:47:41.572 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/executors/json,null,AVAILABLE,@Spark}
14:47:41.579 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/executors/threadDump,null,AVAILABLE,@Spark}
14:47:41.580 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bb7cce7{/executors/threadDump/json,null,AVAILABLE,@Spark}
14:47:41.759 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b530eb9{/static,null,AVAILABLE,@Spark}
14:47:41.763 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@680bddf5{/,null,AVAILABLE,@Spark}
14:47:41.766 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d83c5a5{/api,null,AVAILABLE,@Spark}
14:47:41.767 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@710b30ef{/jobs/job/kill,null,AVAILABLE,@Spark}
14:47:41.769 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28b576a9{/stages/stage/kill,null,AVAILABLE,@Spark}
14:47:41.773 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
14:47:42.420 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
14:47:42.812 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4695.
14:47:42.830 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:4695
14:47:42.871 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
14:47:43.181 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 4695, None)
14:47:43.218 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:4695 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 4695, None)
14:47:43.271 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 4695, None)
14:47:43.272 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 4695, None)
14:47:44.279 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27fde870{/metrics/json,null,AVAILABLE,@Spark}
14:47:48.859 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
14:47:48.921 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
14:47:48.922 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
14:47:48.960 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@422ad5e2{/SQL,null,AVAILABLE,@Spark}
14:47:48.961 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6048e26a{/SQL/json,null,AVAILABLE,@Spark}
14:47:48.963 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3249e278{/SQL/execution,null,AVAILABLE,@Spark}
14:47:48.964 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@456f7d9e{/SQL/execution/json,null,AVAILABLE,@Spark}
14:47:48.975 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ad99cf3{/static/sql,null,AVAILABLE,@Spark}
14:47:50.199 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
14:47:52.009 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
14:47:52.074 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
14:47:55.069 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
14:47:57.550 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
14:47:57.554 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
14:47:58.286 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
14:47:58.300 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
14:47:58.479 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
14:47:58.726 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
14:47:58.728 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
14:47:58.812 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
14:47:58.813 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
14:47:58.991 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
14:47:58.991 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
14:47:58.996 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
14:47:58.997 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
14:47:59.003 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
14:47:59.003 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
14:47:59.008 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
14:47:59.008 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
14:47:59.014 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
14:47:59.014 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
14:47:59.018 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
14:47:59.019 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
14:47:59.025 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
14:47:59.025 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
14:48:00.953 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/0191b727-e30f-4688-8a54-4e7325be348b_resources
14:48:00.973 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/0191b727-e30f-4688-8a54-4e7325be348b
14:48:00.978 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/0191b727-e30f-4688-8a54-4e7325be348b
14:48:00.982 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/0191b727-e30f-4688-8a54-4e7325be348b/_tmp_space.db
14:48:00.992 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
14:48:01.087 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:48:01.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:48:01.100 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
14:48:01.100 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
14:48:01.107 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
14:48:02.231 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/451f3cec-db1c-4a83-8ee2-fd3251b0ea8a_resources
14:48:02.239 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/451f3cec-db1c-4a83-8ee2-fd3251b0ea8a
14:48:02.243 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/451f3cec-db1c-4a83-8ee2-fd3251b0ea8a
14:48:02.258 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/451f3cec-db1c-4a83-8ee2-fd3251b0ea8a/_tmp_space.db
14:48:02.269 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
14:48:02.500 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
14:48:02.516 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
14:48:03.614 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
14:48:03.615 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
14:48:03.627 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
14:48:03.628 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
14:48:03.955 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
14:48:03.960 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
14:48:04.047 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:04.127 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:04.127 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:04.128 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:04.128 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:04.129 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:04.129 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:04.129 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:04.130 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:04.130 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
14:48:04.207 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
14:48:05.873 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
14:48:05.914 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
14:48:05.915 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
14:48:05.916 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
14:48:05.917 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
14:48:05.918 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
14:48:05.923 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') idcard
14:48:05.984 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy') as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as age
14:48:06.073 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})',2) as int)%2 as gender
14:48:06.076 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') area_code
14:48:06.077 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') longitude
14:48:06.083 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') latitude
14:48:06.085 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') matter_id
14:48:06.088 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') model_code
14:48:06.095 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') model_version
14:48:06.096 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
14:48:06.098 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
14:48:06.099 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
14:48:06.157 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:48:06.158 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:48:06.174 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:48:06.175 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:48:06.191 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:48:06.192 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:48:06.211 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:48:06.211 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:48:06.226 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:48:06.227 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:48:06.243 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:48:06.243 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:48:06.255 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:48:06.256 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:48:06.268 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:48:06.270 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:48:06.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:48:06.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:48:06.316 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:48:06.317 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:48:06.326 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:48:06.327 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:48:06.337 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:48:06.337 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:48:06.351 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:48:06.351 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:48:06.360 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
14:48:06.361 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
14:48:07.684 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
14:48:07.684 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
14:48:07.693 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
14:48:07.693 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
14:48:07.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
14:48:07.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
14:48:07.776 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:07.777 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:07.777 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:07.777 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:07.777 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:07.778 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:07.778 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:07.778 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:07.778 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
14:48:07.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
14:48:07.976 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
14:48:07.977 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
14:48:07.995 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
14:48:07.996 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
14:48:09.133 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-24)
14:48:09.136 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
14:48:09.140 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
14:48:09.210 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
14:48:09.267 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
14:48:10.493 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 522.9088 ms
14:48:11.391 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 189.5546 ms
14:48:11.985 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
14:48:16.977 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 264.7 KB, free 897.3 MB)
14:48:17.584 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.4 KB, free 897.3 MB)
14:48:17.589 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:4695 (size: 22.4 KB, free: 897.6 MB)
14:48:17.606 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:62
14:48:17.729 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
14:48:18.520 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:62
14:48:18.720 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:62)
14:48:18.764 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:62) with 1 output partitions
14:48:18.764 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:62)
14:48:18.766 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
14:48:18.781 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
14:48:18.841 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:62), which has no missing parents
14:48:19.441 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 24.1 KB, free 897.3 MB)
14:48:19.452 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.5 KB, free 897.3 MB)
14:48:19.453 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:4695 (size: 10.5 KB, free: 897.6 MB)
14:48:19.454 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
14:48:19.520 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:62) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
14:48:19.535 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
14:48:19.755 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5419 bytes)
14:48:19.758 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5419 bytes)
14:48:19.759 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5419 bytes)
14:48:19.760 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5419 bytes)
14:48:19.806 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
14:48:19.806 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
14:48:19.806 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
14:48:19.806 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
14:48:20.433 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 152.0613 ms
14:48:20.626 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-24]
14:48:20.626 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-24]
14:48:20.626 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-24]
14:48:20.626 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-24]
14:48:25.120 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
14:48:25.120 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
14:48:25.121 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
14:48:25.121 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
14:48:26.683 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1611 bytes result sent to driver
14:48:26.683 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1611 bytes result sent to driver
14:48:26.683 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1568 bytes result sent to driver
14:48:27.185 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 7343 ms on localhost (executor driver) (1/4)
14:48:27.314 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 7555 ms on localhost (executor driver) (2/4)
14:48:27.315 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 7587 ms on localhost (executor driver) (3/4)
14:48:39.776 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1826 bytes result sent to driver
14:48:39.850 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 20091 ms on localhost (executor driver) (4/4)
14:48:39.884 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DWReleaseCustomer.scala:62) finished in 20.214 s
14:48:39.884 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
14:48:39.889 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
14:48:39.902 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
14:48:39.914 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
14:48:39.915 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
14:48:39.968 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62), which has no missing parents
14:48:40.072 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
14:48:40.078 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
14:48:40.083 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:4695 (size: 2.2 KB, free: 897.6 MB)
14:48:40.088 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
14:48:40.097 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:62) (first 15 tasks are for partitions Vector(0))
14:48:40.098 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
14:48:40.113 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
14:48:40.113 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
14:48:40.222 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
14:48:40.238 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 45 ms
14:48:40.432 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 2466 bytes result sent to driver
14:48:40.434 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 327 ms on localhost (executor driver) (1/1)
14:48:40.434 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
14:48:40.434 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:62) finished in 0.329 s
14:48:40.504 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:62, took 21.983054 s
14:48:40.685 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
14:48:40.747 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@54f5f647{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
14:48:40.774 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
14:48:40.867 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
14:48:40.938 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
14:48:40.952 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
14:48:40.953 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
14:48:40.981 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
14:48:40.995 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
14:48:40.997 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
14:48:41.005 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-16689cd6-cafd-4ca7-b869-043a1153edd7
15:39:48.235 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
15:39:49.647 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
15:39:49.780 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
15:39:49.782 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
15:39:49.793 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
15:39:49.793 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
15:39:49.795 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
15:39:52.031 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5195.
15:39:52.141 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
15:39:52.320 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
15:39:52.337 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
15:39:52.338 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
15:39:52.431 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-6c61f6b7-79ec-4022-99c1-e1e2c96bc695
15:39:52.639 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
15:39:52.846 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
15:39:53.339 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @29809ms
15:39:54.165 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
15:39:54.394 INFO  [main] org.spark_project.jetty.server.Server - Started @30866ms
15:39:54.466 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@54f5f647{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
15:39:54.467 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
15:39:54.536 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62df0ff3{/jobs,null,AVAILABLE,@Spark}
15:39:54.538 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/jobs/json,null,AVAILABLE,@Spark}
15:39:54.544 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/jobs/job,null,AVAILABLE,@Spark}
15:39:54.546 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/jobs/job/json,null,AVAILABLE,@Spark}
15:39:54.547 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/stages,null,AVAILABLE,@Spark}
15:39:54.548 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e11ab3d{/stages/json,null,AVAILABLE,@Spark}
15:39:54.553 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2392212b{/stages/stage,null,AVAILABLE,@Spark}
15:39:54.556 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66f66866{/stages/stage/json,null,AVAILABLE,@Spark}
15:39:54.557 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d666b41{/stages/pool,null,AVAILABLE,@Spark}
15:39:54.558 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30f4b1a6{/stages/pool/json,null,AVAILABLE,@Spark}
15:39:54.563 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e1162e7{/storage,null,AVAILABLE,@Spark}
15:39:54.565 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2f1700{/storage/json,null,AVAILABLE,@Spark}
15:39:54.568 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/storage/rdd,null,AVAILABLE,@Spark}
15:39:54.569 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/storage/rdd/json,null,AVAILABLE,@Spark}
15:39:54.570 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/environment,null,AVAILABLE,@Spark}
15:39:54.572 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@553f3b6e{/environment/json,null,AVAILABLE,@Spark}
15:39:54.575 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/executors,null,AVAILABLE,@Spark}
15:39:54.576 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/executors/json,null,AVAILABLE,@Spark}
15:39:54.578 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/executors/threadDump,null,AVAILABLE,@Spark}
15:39:54.579 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bb7cce7{/executors/threadDump/json,null,AVAILABLE,@Spark}
15:39:54.601 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b530eb9{/static,null,AVAILABLE,@Spark}
15:39:54.603 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@680bddf5{/,null,AVAILABLE,@Spark}
15:39:54.606 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d83c5a5{/api,null,AVAILABLE,@Spark}
15:39:54.607 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@710b30ef{/jobs/job/kill,null,AVAILABLE,@Spark}
15:39:54.609 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28b576a9{/stages/stage/kill,null,AVAILABLE,@Spark}
15:39:54.613 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
15:39:55.131 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
15:39:55.289 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5216.
15:39:55.309 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:5216
15:39:55.326 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
15:39:55.487 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 5216, None)
15:39:55.517 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:5216 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 5216, None)
15:39:55.538 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 5216, None)
15:39:55.539 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 5216, None)
15:39:56.301 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27fde870{/metrics/json,null,AVAILABLE,@Spark}
15:40:00.703 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
15:40:00.775 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
15:40:00.776 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
15:40:00.821 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4262fdeb{/SQL,null,AVAILABLE,@Spark}
15:40:00.822 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5382184b{/SQL/json,null,AVAILABLE,@Spark}
15:40:00.823 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42ea7565{/SQL/execution,null,AVAILABLE,@Spark}
15:40:00.824 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3a2f6{/SQL/execution/json,null,AVAILABLE,@Spark}
15:40:00.830 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@422ad5e2{/static/sql,null,AVAILABLE,@Spark}
15:40:02.296 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
15:40:04.786 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15:40:04.874 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
15:40:07.048 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
15:40:09.845 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
15:40:09.888 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
15:40:10.505 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
15:40:10.512 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
15:40:10.632 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
15:40:10.910 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
15:40:10.912 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
15:40:10.970 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
15:40:10.971 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
15:40:11.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
15:40:11.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
15:40:11.073 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
15:40:11.074 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
15:40:11.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
15:40:11.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
15:40:11.086 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
15:40:11.086 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
15:40:11.091 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
15:40:11.091 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
15:40:11.097 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
15:40:11.097 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
15:40:11.102 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
15:40:11.103 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
15:40:13.424 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/1a568702-f706-4951-9697-0fadec039045_resources
15:40:13.439 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/1a568702-f706-4951-9697-0fadec039045
15:40:13.443 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/1a568702-f706-4951-9697-0fadec039045
15:40:13.464 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/1a568702-f706-4951-9697-0fadec039045/_tmp_space.db
15:40:13.514 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
15:40:13.660 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:40:13.660 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:40:13.697 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
15:40:13.698 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
15:40:13.712 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
15:40:14.405 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/7dbb3e50-7711-4d04-97e8-70fc5d477d00_resources
15:40:14.411 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/7dbb3e50-7711-4d04-97e8-70fc5d477d00
15:40:14.415 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/7dbb3e50-7711-4d04-97e8-70fc5d477d00
15:40:14.434 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/7dbb3e50-7711-4d04-97e8-70fc5d477d00/_tmp_space.db
15:40:14.440 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
15:40:14.779 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
15:40:14.814 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
15:40:15.680 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
15:40:15.681 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
15:40:15.706 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
15:40:15.706 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
15:40:16.001 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
15:40:16.001 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
15:40:16.062 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:40:16.140 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:40:16.142 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:40:16.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:40:16.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:40:16.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:40:16.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:40:16.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:40:16.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
15:40:16.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:40:16.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:40:16.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:40:16.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:40:16.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:40:16.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:40:16.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:40:16.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:40:16.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
15:40:16.278 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
15:40:18.208 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
15:40:18.228 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
15:40:18.228 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
15:40:18.229 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
15:40:18.229 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
15:40:18.230 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
15:40:18.232 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') idcard
15:40:18.303 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy') as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as age
15:40:18.363 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})',2) as int)%2 as gender
15:40:18.368 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') area_code
15:40:18.370 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') longitude
15:40:18.371 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') latitude
15:40:18.377 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') matter_id
15:40:18.379 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') model_code
15:40:18.382 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') model_version
15:40:18.383 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
15:40:18.384 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
15:40:18.385 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
15:40:18.444 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:40:18.445 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:40:18.478 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:40:18.478 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:40:18.508 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:40:18.508 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:40:18.528 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:40:18.529 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:40:18.559 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:40:18.560 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:40:18.577 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:40:18.578 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:40:18.592 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:40:18.593 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:40:18.601 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:40:18.601 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:40:18.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:40:18.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:40:18.617 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:40:18.617 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:40:18.628 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:40:18.628 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:40:18.634 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:40:18.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:40:18.644 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:40:18.645 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:40:18.651 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:40:18.652 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:40:19.178 ERROR [main] com.lzz.release.etl.release.dm.DMReleaseCustomer$ - cannot resolve '`exts`' given input columns: [release_session, ct, channels, model_code, model_version, aid, device_type, idcard, area_code, longitude, age, release_status, device_num, gender, sources, matter_id, latitude, bdp_day]; line 1 pos 16;
'Project [release_session#0, release_status#1, device_num#2, device_type#3, sources#4, channels#5, 'get_json_object('exts, $.idcard) AS idcard#55, (cast(date_format(current_timestamp(), yyyy, Some(Asia/Shanghai)) as int) - cast('regexp_extract('get_json_object('exts, $.idcard), (\d{6})(\d{4}), 2) as int)) AS age#56, (cast('regexp_extract('get_json_object('exts, $.idcard), (\d{16})(\d{1}), 2) as int) % 2) AS gender#57, 'get_json_object('exts, $.area_code) AS area_code#58, 'get_json_object('exts, $.longitude) AS longitude#59, 'get_json_object('exts, $.latitude) AS latitude#60, 'get_json_object('exts, $.matter_id) AS matter_id#61, 'get_json_object('exts, $.model_code) AS model_code#62, 'get_json_object('exts, $.model_version) AS model_version#63, 'get_json_object('exts, $.aid) AS aid#64, ct#16L, bdp_day#17]
+- SubqueryAlias dw_release_customer
   +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,idcard#6,age#7,gender#8,area_code#9,longitude#10,latitude#11,matter_id#12,model_code#13,model_version#14,aid#15,ct#16L,bdp_day#17] parquet

org.apache.spark.sql.AnalysisException: cannot resolve '`exts`' given input columns: [release_session, ct, channels, model_code, model_version, aid, device_type, idcard, area_code, longitude, age, release_status, device_num, gender, sources, matter_id, latitude, bdp_day]; line 1 pos 16;
'Project [release_session#0, release_status#1, device_num#2, device_type#3, sources#4, channels#5, 'get_json_object('exts, $.idcard) AS idcard#55, (cast(date_format(current_timestamp(), yyyy, Some(Asia/Shanghai)) as int) - cast('regexp_extract('get_json_object('exts, $.idcard), (\d{6})(\d{4}), 2) as int)) AS age#56, (cast('regexp_extract('get_json_object('exts, $.idcard), (\d{16})(\d{1}), 2) as int) % 2) AS gender#57, 'get_json_object('exts, $.area_code) AS area_code#58, 'get_json_object('exts, $.longitude) AS longitude#59, 'get_json_object('exts, $.latitude) AS latitude#60, 'get_json_object('exts, $.matter_id) AS matter_id#61, 'get_json_object('exts, $.model_code) AS model_code#62, 'get_json_object('exts, $.model_version) AS model_version#63, 'get_json_object('exts, $.aid) AS aid#64, ct#16L, bdp_day#17]
+- SubqueryAlias dw_release_customer
   +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,idcard#6,age#7,gender#8,area_code#9,longitude#10,latitude#11,matter_id#12,model_code#13,model_version#14,aid#15,ct#16L,bdp_day#17] parquet

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2888)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1154)
	at org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1189)
	at com.lzz.release.util.SparkHelper$.readTableData(SparkHelper.scala:31)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.handleReleaseJob(DMReleaseCustomer.scala:46)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$$anonfun$handleJobs$1.apply(DMReleaseCustomer.scala:103)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$$anonfun$handleJobs$1.apply(DMReleaseCustomer.scala:101)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.handleJobs(DMReleaseCustomer.scala:101)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.main(DMReleaseCustomer.scala:122)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer.main(DMReleaseCustomer.scala)
15:40:19.236 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@54f5f647{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
15:40:19.246 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
15:40:19.329 INFO  [dispatcher-event-loop-1] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
15:40:19.385 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
15:40:19.401 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
15:40:19.413 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
15:40:19.449 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
15:40:19.460 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
15:40:19.494 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
15:40:19.502 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-40a098a3-fd15-4baa-96e0-80813e85820d
15:42:43.891 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
15:42:44.787 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
15:42:44.911 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
15:42:44.913 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
15:42:44.914 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
15:42:44.915 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
15:42:44.916 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
15:42:47.514 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5260.
15:42:47.601 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
15:42:47.691 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
15:42:47.709 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
15:42:47.710 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
15:42:47.771 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-e6203a8d-6c72-4308-9eb9-d0c477443310
15:42:47.869 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
15:42:48.038 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
15:42:48.351 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @13183ms
15:42:48.567 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
15:42:48.592 INFO  [main] org.spark_project.jetty.server.Server - Started @13424ms
15:42:48.645 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@56180c22{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
15:42:48.645 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
15:42:48.694 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/jobs,null,AVAILABLE,@Spark}
15:42:48.695 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/jobs/json,null,AVAILABLE,@Spark}
15:42:48.696 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/jobs/job,null,AVAILABLE,@Spark}
15:42:48.697 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/jobs/job/json,null,AVAILABLE,@Spark}
15:42:48.698 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ea1fda{/stages,null,AVAILABLE,@Spark}
15:42:48.698 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@420745d7{/stages/json,null,AVAILABLE,@Spark}
15:42:48.699 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fa47fea{/stages/stage,null,AVAILABLE,@Spark}
15:42:48.701 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545f80bf{/stages/stage/json,null,AVAILABLE,@Spark}
15:42:48.705 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22fa55b2{/stages/pool,null,AVAILABLE,@Spark}
15:42:48.706 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6594402a{/stages/pool/json,null,AVAILABLE,@Spark}
15:42:48.707 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@405325cf{/storage,null,AVAILABLE,@Spark}
15:42:48.708 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c3f01f{/storage/json,null,AVAILABLE,@Spark}
15:42:48.711 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/storage/rdd,null,AVAILABLE,@Spark}
15:42:48.713 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/storage/rdd/json,null,AVAILABLE,@Spark}
15:42:48.716 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/environment,null,AVAILABLE,@Spark}
15:42:48.717 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5298dead{/environment/json,null,AVAILABLE,@Spark}
15:42:48.718 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/executors,null,AVAILABLE,@Spark}
15:42:48.719 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/executors/json,null,AVAILABLE,@Spark}
15:42:48.721 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/executors/threadDump,null,AVAILABLE,@Spark}
15:42:48.725 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b986cd{/executors/threadDump/json,null,AVAILABLE,@Spark}
15:42:48.783 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41c62850{/static,null,AVAILABLE,@Spark}
15:42:48.785 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f574cc2{/,null,AVAILABLE,@Spark}
15:42:48.787 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a9c84a5{/api,null,AVAILABLE,@Spark}
15:42:48.788 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f1249d{/jobs/job/kill,null,AVAILABLE,@Spark}
15:42:48.789 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a68df9{/stages/stage/kill,null,AVAILABLE,@Spark}
15:42:48.794 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
15:42:49.286 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
15:42:49.480 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5281.
15:42:49.497 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:5281
15:42:49.505 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
15:42:49.578 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 5281, None)
15:42:49.627 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:5281 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 5281, None)
15:42:49.666 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 5281, None)
15:42:49.667 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 5281, None)
15:42:50.162 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e850122{/metrics/json,null,AVAILABLE,@Spark}
15:42:54.893 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
15:42:54.944 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
15:42:54.945 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
15:42:54.974 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e046e39{/SQL,null,AVAILABLE,@Spark}
15:42:54.975 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46fc522d{/SQL/json,null,AVAILABLE,@Spark}
15:42:54.976 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2213639b{/SQL/execution,null,AVAILABLE,@Spark}
15:42:54.976 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f25bf88{/SQL/execution/json,null,AVAILABLE,@Spark}
15:42:54.984 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c719bd4{/static/sql,null,AVAILABLE,@Spark}
15:42:56.070 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
15:42:57.725 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15:42:57.833 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
15:42:59.826 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
15:43:02.529 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
15:43:02.537 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
15:43:03.542 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
15:43:03.555 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
15:43:03.713 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
15:43:04.002 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
15:43:04.004 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
15:43:04.061 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
15:43:04.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
15:43:04.168 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
15:43:04.168 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
15:43:04.173 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
15:43:04.174 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
15:43:04.180 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
15:43:04.181 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
15:43:04.188 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
15:43:04.188 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
15:43:04.193 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
15:43:04.194 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
15:43:04.199 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
15:43:04.199 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
15:43:04.204 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
15:43:04.204 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
15:43:05.602 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/7f5870b9-e185-4602-9198-30b4dcf55a5c_resources
15:43:05.614 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/7f5870b9-e185-4602-9198-30b4dcf55a5c
15:43:05.617 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/7f5870b9-e185-4602-9198-30b4dcf55a5c
15:43:05.634 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/7f5870b9-e185-4602-9198-30b4dcf55a5c/_tmp_space.db
15:43:05.639 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
15:43:05.726 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:43:05.726 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:43:05.736 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
15:43:05.736 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
15:43:05.741 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
15:43:06.158 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/72ab5f70-218d-45d4-9156-52e97eb7cd9c_resources
15:43:06.165 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/72ab5f70-218d-45d4-9156-52e97eb7cd9c
15:43:06.173 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/72ab5f70-218d-45d4-9156-52e97eb7cd9c
15:43:06.200 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/72ab5f70-218d-45d4-9156-52e97eb7cd9c/_tmp_space.db
15:43:06.207 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
15:43:06.384 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
15:43:06.400 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
15:43:06.966 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
15:43:06.967 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
15:43:06.978 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
15:43:06.978 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
15:43:07.243 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
15:43:07.244 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
15:43:07.305 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:43:07.334 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:43:07.335 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:43:07.335 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:43:07.336 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:43:07.336 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:43:07.336 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:43:07.336 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:43:07.337 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
15:43:07.337 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:43:07.337 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:43:07.338 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:43:07.338 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:43:07.338 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:43:07.338 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:43:07.339 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:43:07.339 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:43:07.339 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
15:43:07.408 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
15:43:08.598 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
15:43:08.617 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
15:43:08.618 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
15:43:08.619 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
15:43:08.623 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
15:43:08.625 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
15:43:08.625 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') idcard
15:43:08.654 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy') as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as age
15:43:08.707 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})',2) as int)%2 as gender
15:43:08.712 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') area_code
15:43:08.714 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') longitude
15:43:08.715 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') latitude
15:43:08.716 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') matter_id
15:43:08.722 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') model_code
15:43:08.724 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') model_version
15:43:08.725 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
15:43:08.726 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
15:43:08.727 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
15:43:08.737 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:43:08.737 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:43:08.771 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:43:08.772 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:43:08.797 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:43:08.797 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:43:08.826 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:43:08.826 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:43:08.891 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:43:08.893 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:43:08.957 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:43:08.957 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:43:08.974 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:43:08.975 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:43:08.986 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:43:08.987 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:43:08.995 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:43:08.996 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:43:09.013 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:43:09.014 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:43:09.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:43:09.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:43:09.051 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:43:09.051 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:43:09.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:43:09.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:43:09.085 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:43:09.085 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:43:09.481 ERROR [main] com.lzz.release.etl.release.dm.DMReleaseCustomer$ - cannot resolve '`exts`' given input columns: [device_num, device_type, aid, model_version, age, area_code, sources, release_status, gender, channels, longitude, bdp_day, release_session, matter_id, ct, latitude, model_code, idcard]; line 1 pos 16;
'Project [release_session#0, release_status#1, device_num#2, device_type#3, sources#4, channels#5, 'get_json_object('exts, $.idcard) AS idcard#55, (cast(date_format(current_timestamp(), yyyy, Some(Asia/Shanghai)) as int) - cast('regexp_extract('get_json_object('exts, $.idcard), (\d{6})(\d{4}), 2) as int)) AS age#56, (cast('regexp_extract('get_json_object('exts, $.idcard), (\d{16})(\d{1}), 2) as int) % 2) AS gender#57, 'get_json_object('exts, $.area_code) AS area_code#58, 'get_json_object('exts, $.longitude) AS longitude#59, 'get_json_object('exts, $.latitude) AS latitude#60, 'get_json_object('exts, $.matter_id) AS matter_id#61, 'get_json_object('exts, $.model_code) AS model_code#62, 'get_json_object('exts, $.model_version) AS model_version#63, 'get_json_object('exts, $.aid) AS aid#64, ct#16L, bdp_day#17]
+- SubqueryAlias dw_release_customer
   +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,idcard#6,age#7,gender#8,area_code#9,longitude#10,latitude#11,matter_id#12,model_code#13,model_version#14,aid#15,ct#16L,bdp_day#17] parquet

org.apache.spark.sql.AnalysisException: cannot resolve '`exts`' given input columns: [device_num, device_type, aid, model_version, age, area_code, sources, release_status, gender, channels, longitude, bdp_day, release_session, matter_id, ct, latitude, model_code, idcard]; line 1 pos 16;
'Project [release_session#0, release_status#1, device_num#2, device_type#3, sources#4, channels#5, 'get_json_object('exts, $.idcard) AS idcard#55, (cast(date_format(current_timestamp(), yyyy, Some(Asia/Shanghai)) as int) - cast('regexp_extract('get_json_object('exts, $.idcard), (\d{6})(\d{4}), 2) as int)) AS age#56, (cast('regexp_extract('get_json_object('exts, $.idcard), (\d{16})(\d{1}), 2) as int) % 2) AS gender#57, 'get_json_object('exts, $.area_code) AS area_code#58, 'get_json_object('exts, $.longitude) AS longitude#59, 'get_json_object('exts, $.latitude) AS latitude#60, 'get_json_object('exts, $.matter_id) AS matter_id#61, 'get_json_object('exts, $.model_code) AS model_code#62, 'get_json_object('exts, $.model_version) AS model_version#63, 'get_json_object('exts, $.aid) AS aid#64, ct#16L, bdp_day#17]
+- SubqueryAlias dw_release_customer
   +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,idcard#6,age#7,gender#8,area_code#9,longitude#10,latitude#11,matter_id#12,model_code#13,model_version#14,aid#15,ct#16L,bdp_day#17] parquet

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2888)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1154)
	at org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1189)
	at com.lzz.release.util.SparkHelper$.readTableData(SparkHelper.scala:31)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.handleReleaseJob(DMReleaseCustomer.scala:46)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$$anonfun$handleJobs$1.apply(DMReleaseCustomer.scala:103)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$$anonfun$handleJobs$1.apply(DMReleaseCustomer.scala:101)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.handleJobs(DMReleaseCustomer.scala:101)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.main(DMReleaseCustomer.scala:122)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer.main(DMReleaseCustomer.scala)
15:43:09.517 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@56180c22{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
15:43:09.603 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
15:43:09.693 INFO  [dispatcher-event-loop-2] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
15:43:09.759 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
15:43:09.760 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
15:43:09.801 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
15:43:09.815 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
15:43:09.821 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
15:43:09.837 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
15:43:09.839 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-c30aabfb-fcbc-4f03-b29d-a6c5e2f08679
15:47:12.499 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
15:47:13.611 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
15:47:13.741 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
15:47:13.743 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
15:47:13.744 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
15:47:13.744 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
15:47:13.746 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
15:47:16.404 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5350.
15:47:16.528 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
15:47:16.687 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
15:47:16.711 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
15:47:16.712 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
15:47:16.736 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-779af1d9-9f67-41e8-9df9-2f2b6147a138
15:47:16.884 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
15:47:17.023 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
15:47:17.589 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @15876ms
15:47:17.819 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
15:47:17.865 INFO  [main] org.spark_project.jetty.server.Server - Started @16153ms
15:47:17.918 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@17f9344b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
15:47:17.918 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
15:47:18.031 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/jobs,null,AVAILABLE,@Spark}
15:47:18.032 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/jobs/json,null,AVAILABLE,@Spark}
15:47:18.058 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/jobs/job,null,AVAILABLE,@Spark}
15:47:18.068 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fa47fea{/jobs/job/json,null,AVAILABLE,@Spark}
15:47:18.069 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b43e173{/stages,null,AVAILABLE,@Spark}
15:47:18.070 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545f80bf{/stages/json,null,AVAILABLE,@Spark}
15:47:18.072 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22fa55b2{/stages/stage,null,AVAILABLE,@Spark}
15:47:18.086 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@405325cf{/stages/stage/json,null,AVAILABLE,@Spark}
15:47:18.091 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c3f01f{/stages/pool,null,AVAILABLE,@Spark}
15:47:18.096 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/stages/pool/json,null,AVAILABLE,@Spark}
15:47:18.101 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/storage,null,AVAILABLE,@Spark}
15:47:18.103 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/storage/json,null,AVAILABLE,@Spark}
15:47:18.105 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5298dead{/storage/rdd,null,AVAILABLE,@Spark}
15:47:18.107 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/storage/rdd/json,null,AVAILABLE,@Spark}
15:47:18.112 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/environment,null,AVAILABLE,@Spark}
15:47:18.113 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/environment/json,null,AVAILABLE,@Spark}
15:47:18.114 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b986cd{/executors,null,AVAILABLE,@Spark}
15:47:18.115 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41c62850{/executors/json,null,AVAILABLE,@Spark}
15:47:18.128 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@328572f0{/executors/threadDump,null,AVAILABLE,@Spark}
15:47:18.137 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f460bb{/executors/threadDump/json,null,AVAILABLE,@Spark}
15:47:18.156 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d2a6eac{/static,null,AVAILABLE,@Spark}
15:47:18.158 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60222fd8{/,null,AVAILABLE,@Spark}
15:47:18.163 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f1249d{/api,null,AVAILABLE,@Spark}
15:47:18.164 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b1637e1{/jobs/job/kill,null,AVAILABLE,@Spark}
15:47:18.166 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64711bf2{/stages/stage/kill,null,AVAILABLE,@Spark}
15:47:18.173 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
15:47:18.524 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
15:47:18.663 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5373.
15:47:18.666 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:5373
15:47:18.673 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
15:47:18.754 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 5373, None)
15:47:18.785 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:5373 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 5373, None)
15:47:18.804 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 5373, None)
15:47:18.804 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 5373, None)
15:47:19.202 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e4c3a38{/metrics/json,null,AVAILABLE,@Spark}
15:47:19.482 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
15:47:19.592 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
15:47:19.592 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
15:47:19.618 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a344b65{/SQL,null,AVAILABLE,@Spark}
15:47:19.620 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b474074{/SQL/json,null,AVAILABLE,@Spark}
15:47:19.621 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bac9ba{/SQL/execution,null,AVAILABLE,@Spark}
15:47:19.622 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bdd5689{/SQL/execution/json,null,AVAILABLE,@Spark}
15:47:19.624 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e140e59{/static/sql,null,AVAILABLE,@Spark}
15:47:22.007 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
15:47:24.020 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15:47:24.188 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
15:47:26.678 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
15:47:29.250 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
15:47:29.253 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
15:47:30.171 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
15:47:30.181 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
15:47:30.368 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
15:47:30.725 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
15:47:30.727 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
15:47:30.782 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
15:47:30.783 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
15:47:30.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
15:47:30.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
15:47:30.893 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
15:47:30.893 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
15:47:30.902 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
15:47:30.902 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
15:47:30.910 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
15:47:30.910 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
15:47:30.916 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
15:47:30.917 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
15:47:30.923 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
15:47:30.924 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
15:47:30.931 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
15:47:30.931 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
15:47:32.634 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/f52d1cfe-6c77-41dc-8679-209a0f1ff76b_resources
15:47:32.646 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/f52d1cfe-6c77-41dc-8679-209a0f1ff76b
15:47:32.651 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/f52d1cfe-6c77-41dc-8679-209a0f1ff76b
15:47:32.665 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/f52d1cfe-6c77-41dc-8679-209a0f1ff76b/_tmp_space.db
15:47:32.672 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
15:47:32.787 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:47:32.788 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:47:32.805 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
15:47:32.805 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
15:47:32.812 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
15:47:33.492 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/f0a563f2-66fe-4d97-b4b5-167e48d23a98_resources
15:47:33.549 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/f0a563f2-66fe-4d97-b4b5-167e48d23a98
15:47:33.556 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/f0a563f2-66fe-4d97-b4b5-167e48d23a98
15:47:33.588 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/f0a563f2-66fe-4d97-b4b5-167e48d23a98/_tmp_space.db
15:47:33.592 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
15:47:33.815 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
15:47:37.802 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
15:47:38.470 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
15:47:38.470 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
15:47:38.484 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
15:47:38.484 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
15:47:38.758 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
15:47:38.758 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
15:47:38.809 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:47:38.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:47:38.878 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:47:38.879 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:47:38.879 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:47:38.879 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:47:38.879 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:47:38.879 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:47:38.880 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
15:47:38.881 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:47:38.881 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:47:38.881 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:47:38.882 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:47:38.882 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:47:38.882 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:47:38.883 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:47:38.883 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:47:38.884 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
15:47:38.983 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
15:47:40.631 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
15:47:40.654 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
15:47:40.655 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
15:47:40.655 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
15:47:40.656 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
15:47:40.657 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
15:47:40.657 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') idcard
15:47:40.696 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy') as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as age
15:47:40.855 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})',2) as int)%2 as gender
15:47:40.906 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') area_code
15:47:40.912 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') longitude
15:47:40.937 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') latitude
15:47:40.942 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') matter_id
15:47:40.976 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') model_code
15:47:40.983 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') model_version
15:47:40.983 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
15:47:40.988 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
15:47:40.989 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
15:47:41.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:47:41.032 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:47:41.046 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:47:41.046 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:47:41.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:47:41.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:47:41.088 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:47:41.088 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:47:41.106 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:47:41.106 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:47:41.124 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:47:41.124 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:47:41.147 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:47:41.149 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:47:41.228 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:47:41.229 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:47:41.273 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:47:41.274 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:47:41.300 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:47:41.300 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:47:41.318 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:47:41.318 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:47:41.334 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:47:41.335 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:47:41.437 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:47:41.446 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:47:41.483 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:47:41.484 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:47:42.061 ERROR [main] com.lzz.release.etl.release.dm.DMReleaseCustomer$ - cannot resolve '`exts`' given input columns: [area_code, ct, idcard, release_session, latitude, model_code, longitude, gender, model_version, release_status, channels, device_num, bdp_day, matter_id, aid, age, sources, device_type]; line 1 pos 16;
'Project [release_session#0, release_status#1, device_num#2, device_type#3, sources#4, channels#5, 'get_json_object('exts, $.idcard) AS idcard#55, (cast(date_format(current_timestamp(), yyyy, Some(Asia/Shanghai)) as int) - cast('regexp_extract('get_json_object('exts, $.idcard), (\d{6})(\d{4}), 2) as int)) AS age#56, (cast('regexp_extract('get_json_object('exts, $.idcard), (\d{16})(\d{1}), 2) as int) % 2) AS gender#57, 'get_json_object('exts, $.area_code) AS area_code#58, 'get_json_object('exts, $.longitude) AS longitude#59, 'get_json_object('exts, $.latitude) AS latitude#60, 'get_json_object('exts, $.matter_id) AS matter_id#61, 'get_json_object('exts, $.model_code) AS model_code#62, 'get_json_object('exts, $.model_version) AS model_version#63, 'get_json_object('exts, $.aid) AS aid#64, ct#16L, bdp_day#17]
+- SubqueryAlias dw_release_customer
   +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,idcard#6,age#7,gender#8,area_code#9,longitude#10,latitude#11,matter_id#12,model_code#13,model_version#14,aid#15,ct#16L,bdp_day#17] parquet

org.apache.spark.sql.AnalysisException: cannot resolve '`exts`' given input columns: [area_code, ct, idcard, release_session, latitude, model_code, longitude, gender, model_version, release_status, channels, device_num, bdp_day, matter_id, aid, age, sources, device_type]; line 1 pos 16;
'Project [release_session#0, release_status#1, device_num#2, device_type#3, sources#4, channels#5, 'get_json_object('exts, $.idcard) AS idcard#55, (cast(date_format(current_timestamp(), yyyy, Some(Asia/Shanghai)) as int) - cast('regexp_extract('get_json_object('exts, $.idcard), (\d{6})(\d{4}), 2) as int)) AS age#56, (cast('regexp_extract('get_json_object('exts, $.idcard), (\d{16})(\d{1}), 2) as int) % 2) AS gender#57, 'get_json_object('exts, $.area_code) AS area_code#58, 'get_json_object('exts, $.longitude) AS longitude#59, 'get_json_object('exts, $.latitude) AS latitude#60, 'get_json_object('exts, $.matter_id) AS matter_id#61, 'get_json_object('exts, $.model_code) AS model_code#62, 'get_json_object('exts, $.model_version) AS model_version#63, 'get_json_object('exts, $.aid) AS aid#64, ct#16L, bdp_day#17]
+- SubqueryAlias dw_release_customer
   +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,idcard#6,age#7,gender#8,area_code#9,longitude#10,latitude#11,matter_id#12,model_code#13,model_version#14,aid#15,ct#16L,bdp_day#17] parquet

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2888)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1154)
	at org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1189)
	at com.lzz.release.util.SparkHelper$.readTableData(SparkHelper.scala:32)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.handleReleaseJob(DMReleaseCustomer.scala:46)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$$anonfun$handleJobs$1.apply(DMReleaseCustomer.scala:103)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$$anonfun$handleJobs$1.apply(DMReleaseCustomer.scala:101)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.handleJobs(DMReleaseCustomer.scala:101)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.main(DMReleaseCustomer.scala:122)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer.main(DMReleaseCustomer.scala)
15:47:42.205 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@17f9344b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
15:47:42.248 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
15:47:42.374 INFO  [dispatcher-event-loop-1] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
15:47:42.442 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
15:47:42.457 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
15:47:42.467 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
15:47:42.494 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
15:47:42.505 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
15:47:42.524 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
15:47:42.527 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-b75ff814-8f31-43d8-8ff0-c70bc52abd73
15:56:40.426 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
15:56:41.395 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
15:56:41.511 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
15:56:41.514 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
15:56:41.516 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
15:56:41.516 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
15:56:41.518 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
15:56:43.734 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5493.
15:56:43.811 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
15:56:43.912 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
15:56:43.929 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
15:56:43.930 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
15:56:43.955 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-476b1ead-7ada-42bb-be89-bba6991012af
15:56:44.056 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
15:56:44.181 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
15:56:44.576 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @18702ms
15:56:44.897 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
15:56:44.951 INFO  [main] org.spark_project.jetty.server.Server - Started @19078ms
15:56:45.040 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@54e81b21{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
15:56:45.041 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
15:56:45.228 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/jobs,null,AVAILABLE,@Spark}
15:56:45.229 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/jobs/json,null,AVAILABLE,@Spark}
15:56:45.230 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e11ab3d{/jobs/job,null,AVAILABLE,@Spark}
15:56:45.232 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b43e173{/jobs/job/json,null,AVAILABLE,@Spark}
15:56:45.233 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545f80bf{/stages,null,AVAILABLE,@Spark}
15:56:45.234 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22fa55b2{/stages/json,null,AVAILABLE,@Spark}
15:56:45.235 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6594402a{/stages/stage,null,AVAILABLE,@Spark}
15:56:45.242 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c3f01f{/stages/stage/json,null,AVAILABLE,@Spark}
15:56:45.243 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/stages/pool,null,AVAILABLE,@Spark}
15:56:45.244 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/stages/pool/json,null,AVAILABLE,@Spark}
15:56:45.245 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/storage,null,AVAILABLE,@Spark}
15:56:45.246 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5298dead{/storage/json,null,AVAILABLE,@Spark}
15:56:45.248 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/storage/rdd,null,AVAILABLE,@Spark}
15:56:45.252 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/storage/rdd/json,null,AVAILABLE,@Spark}
15:56:45.253 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/environment,null,AVAILABLE,@Spark}
15:56:45.270 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b986cd{/environment/json,null,AVAILABLE,@Spark}
15:56:45.273 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41c62850{/executors,null,AVAILABLE,@Spark}
15:56:45.274 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@328572f0{/executors/json,null,AVAILABLE,@Spark}
15:56:45.274 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f460bb{/executors/threadDump,null,AVAILABLE,@Spark}
15:56:45.291 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d2a6eac{/executors/threadDump/json,null,AVAILABLE,@Spark}
15:56:45.314 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c0f7678{/static,null,AVAILABLE,@Spark}
15:56:45.316 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f1249d{/,null,AVAILABLE,@Spark}
15:56:45.318 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a68df9{/api,null,AVAILABLE,@Spark}
15:56:45.319 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64711bf2{/jobs/job/kill,null,AVAILABLE,@Spark}
15:56:45.321 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c1e23ff{/stages/stage/kill,null,AVAILABLE,@Spark}
15:56:45.327 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
15:56:45.895 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
15:56:46.274 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5514.
15:56:46.275 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:5514
15:56:46.322 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
15:56:46.431 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 5514, None)
15:56:46.457 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:5514 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 5514, None)
15:56:46.490 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 5514, None)
15:56:46.491 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 5514, None)
15:56:47.078 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c27d163{/metrics/json,null,AVAILABLE,@Spark}
15:56:47.382 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
15:56:47.456 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
15:56:47.456 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
15:56:47.500 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b08f438{/SQL,null,AVAILABLE,@Spark}
15:56:47.501 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b2f8ab6{/SQL/json,null,AVAILABLE,@Spark}
15:56:47.502 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65ae095c{/SQL/execution,null,AVAILABLE,@Spark}
15:56:47.503 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e140e59{/SQL/execution/json,null,AVAILABLE,@Spark}
15:56:47.505 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ae263bf{/static/sql,null,AVAILABLE,@Spark}
15:56:49.525 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
15:56:51.118 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15:56:51.240 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
15:56:53.895 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
15:56:56.726 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
15:56:56.730 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
15:56:57.366 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
15:56:57.372 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
15:56:57.473 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
15:56:57.763 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
15:56:57.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
15:56:57.829 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
15:56:57.830 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
15:56:58.535 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
15:56:58.536 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
15:56:58.550 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
15:56:58.550 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
15:56:58.558 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
15:56:58.559 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
15:56:58.573 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
15:56:58.573 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
15:56:58.584 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
15:56:58.584 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
15:56:58.596 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
15:56:58.596 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
15:56:58.603 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
15:56:58.603 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
15:57:03.284 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/24934c42-8925-4e01-8d92-16926d0bed93_resources
15:57:03.295 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/24934c42-8925-4e01-8d92-16926d0bed93
15:57:03.301 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/24934c42-8925-4e01-8d92-16926d0bed93
15:57:03.317 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/24934c42-8925-4e01-8d92-16926d0bed93/_tmp_space.db
15:57:03.322 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
15:57:03.419 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:57:03.419 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:57:03.444 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
15:57:03.444 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
15:57:03.449 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
15:57:04.329 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/68e1b0e3-ef9a-4450-8a3e-e08e70c01b25_resources
15:57:04.384 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/68e1b0e3-ef9a-4450-8a3e-e08e70c01b25
15:57:04.432 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/68e1b0e3-ef9a-4450-8a3e-e08e70c01b25
15:57:04.451 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/68e1b0e3-ef9a-4450-8a3e-e08e70c01b25/_tmp_space.db
15:57:04.456 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
15:57:04.821 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
15:57:09.217 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
15:57:09.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
15:57:09.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
15:57:09.842 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
15:57:09.842 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
15:57:10.138 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
15:57:10.138 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
15:57:10.200 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:10.257 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:10.258 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:10.258 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:10.258 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:10.259 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:10.259 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:10.259 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:10.260 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
15:57:10.260 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:10.261 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:10.261 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:10.261 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:10.261 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:10.262 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:10.262 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:10.263 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:10.263 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
15:57:10.324 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
15:57:12.005 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
15:57:12.041 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
15:57:12.042 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
15:57:12.049 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
15:57:12.049 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
15:57:12.050 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
15:57:12.051 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: idcard
15:57:12.052 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: age
15:57:12.053 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: getAgeRange(age) as age_range
15:57:12.111 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: gender
15:57:12.115 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: area_code
15:57:12.116 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
15:57:12.116 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
15:57:12.275 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:57:12.290 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
15:57:13.414 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
15:57:13.415 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
15:57:13.421 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
15:57:13.421 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
15:57:13.499 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
15:57:13.499 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
15:57:13.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:13.571 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:13.572 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:13.573 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:13.573 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:13.573 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:13.574 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:13.574 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:13.574 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
15:57:13.575 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:13.575 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:13.576 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:13.576 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:13.577 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:13.577 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:13.577 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:13.578 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:13.578 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
15:57:13.759 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
15:57:13.760 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
15:57:13.775 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=dw_release tbl=dw_release_customer
15:57:13.775 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=dw_release tbl=dw_release_customer	
15:57:15.173 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#17),(bdp_day#17 = 2019-09-24)
15:57:15.182 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
15:57:15.190 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 9 more fields>
15:57:15.461 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: 
15:57:15.534 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
15:57:18.461 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 1111.0931 ms
15:57:19.110 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 265.9 KB, free 897.3 MB)
15:57:20.594 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.7 KB, free 897.3 MB)
15:57:20.628 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:5514 (size: 22.7 KB, free: 897.6 MB)
15:57:20.907 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from persist at DMReleaseCustomer.scala:48
15:57:21.384 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
15:57:23.676 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 105.886 ms
15:57:23.797 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:50
15:57:23.923 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DMReleaseCustomer.scala:50) with 1 output partitions
15:57:23.924 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at DMReleaseCustomer.scala:50)
15:57:23.925 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
15:57:23.932 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
15:57:23.972 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at show at DMReleaseCustomer.scala:50), which has no missing parents
15:57:24.594 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 26.9 KB, free 897.3 MB)
15:57:24.601 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.4 KB, free 897.3 MB)
15:57:24.602 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:5514 (size: 10.4 KB, free: 897.6 MB)
15:57:24.603 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
15:57:24.654 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at show at DMReleaseCustomer.scala:50) (first 15 tasks are for partitions Vector(0))
15:57:24.658 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
15:57:24.928 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5413 bytes)
15:57:25.017 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
15:57:25.460 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/bdp_day=2019-09-24/000000_0, range: 0-4194304, partition values: [2019-09-24]
15:57:29.362 WARN  [Executor task launch worker for task 0] org.apache.parquet.CorruptStatistics - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
15:57:33.801 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.memory.MemoryStore - Block rdd_2_0 stored as values in memory (estimated size 5.6 MB, free 891.7 MB)
15:57:33.804 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added rdd_2_0 in memory on 192.168.237.1:5514 (size: 5.6 MB, free: 892.0 MB)
15:57:33.997 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.6841 ms
15:57:34.849 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 824.9083 ms
15:57:35.255 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - 1 block locks were not released by TID = 0:
[rdd_2_0]
15:57:35.287 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3016 bytes result sent to driver
15:57:35.368 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 10523 ms on localhost (executor driver) (1/1)
15:57:35.397 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
15:57:35.414 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (show at DMReleaseCustomer.scala:50) finished in 10.635 s
15:57:35.442 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DMReleaseCustomer.scala:50, took 11.644365 s
15:57:36.686 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
15:57:36.687 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
15:57:36.688 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
15:57:36.688 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: user_count
15:57:36.688 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: total_count
15:57:36.689 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
15:57:37.158 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.2013 ms
15:57:37.469 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 86.3223 ms
15:57:37.721 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 182.4207 ms
15:57:37.824 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 90.1671 ms
15:57:38.091 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:75
15:57:38.113 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 7 (show at DMReleaseCustomer.scala:75)
15:57:38.159 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (show at DMReleaseCustomer.scala:75)
15:57:38.160 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at DMReleaseCustomer.scala:75) with 1 output partitions
15:57:38.160 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at DMReleaseCustomer.scala:75)
15:57:38.162 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
15:57:38.241 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
15:57:38.248 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at show at DMReleaseCustomer.scala:75), which has no missing parents
15:57:38.328 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 45.7 KB, free 891.6 MB)
15:57:38.332 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 18.1 KB, free 891.6 MB)
15:57:38.335 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:5514 (size: 18.1 KB, free: 892.0 MB)
15:57:38.336 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
15:57:38.377 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(0, 1))
15:57:38.377 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 2 tasks
15:57:38.404 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5402 bytes)
15:57:38.415 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, ANY, 5402 bytes)
15:57:38.417 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
15:57:38.423 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 1.0 (TID 2)
15:57:38.450 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.BlockManager - Found block rdd_2_0 locally
15:57:38.458 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/bdp_day=2019-09-24/000000_0, range: 4194304-5574414, partition values: [2019-09-24]
15:57:38.484 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 32.4086 ms
15:57:38.621 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.7211 ms
15:57:38.813 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.memory.MemoryStore - Block rdd_2_1 stored as values in memory (estimated size 16.0 B, free 883.6 MB)
15:57:38.817 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added rdd_2_1 in memory on 192.168.237.1:5514 (size: 16.0 B, free: 892.0 MB)
15:57:38.883 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.4795 ms
15:57:38.995 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.7176 ms
15:57:39.048 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.5278 ms
15:57:39.146 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.6265 ms
15:57:39.241 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 1.0 (TID 2). 2686 bytes result sent to driver
15:57:39.257 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 1.0 (TID 2) in 840 ms on localhost (executor driver) (1/2)
15:57:39.971 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2134 bytes result sent to driver
15:57:39.977 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 1584 ms on localhost (executor driver) (2/2)
15:57:39.983 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
15:57:39.984 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (show at DMReleaseCustomer.scala:75) finished in 1.593 s
15:57:39.988 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
15:57:39.991 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
15:57:39.996 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
15:57:39.997 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
15:57:40.014 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at show at DMReleaseCustomer.scala:75), which has no missing parents
15:57:40.046 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 43.5 KB, free 891.6 MB)
15:57:40.144 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 14.5 KB, free 891.6 MB)
15:57:40.146 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.237.1:5514 (size: 14.5 KB, free: 891.9 MB)
15:57:40.149 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1004
15:57:40.150 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 32 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
15:57:40.151 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 32 tasks
15:57:40.153 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 4715 bytes)
15:57:40.153 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 4, localhost, executor driver, partition 1, ANY, 4715 bytes)
15:57:40.154 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 5, localhost, executor driver, partition 2, ANY, 4715 bytes)
15:57:40.154 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 6, localhost, executor driver, partition 3, ANY, 4715 bytes)
15:57:40.154 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 3)
15:57:40.155 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 4)
15:57:40.177 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 6)
15:57:40.177 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 5)
15:57:40.332 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:40.332 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:40.332 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:40.332 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:40.360 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 111 ms
15:57:40.360 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 111 ms
15:57:40.360 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 111 ms
15:57:40.360 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 111 ms
15:57:40.462 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.5658 ms
15:57:40.777 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 252.0127 ms
15:57:41.387 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 55
15:57:41.411 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 56
15:57:42.057 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 830.9578 ms
15:57:42.257 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 90.5974 ms
15:57:42.287 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 27.0749 ms
15:57:42.329 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.237.1:5514 in memory (size: 18.1 KB, free: 892.0 MB)
15:57:44.110 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 5). 2930 bytes result sent to driver
15:57:44.111 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 6). 2930 bytes result sent to driver
15:57:44.113 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 7, localhost, executor driver, partition 4, ANY, 4715 bytes)
15:57:44.114 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 3). 2887 bytes result sent to driver
15:57:44.114 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 8, localhost, executor driver, partition 5, ANY, 4715 bytes)
15:57:44.114 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 4.0 in stage 2.0 (TID 7)
15:57:44.115 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 5.0 in stage 2.0 (TID 8)
15:57:44.117 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 6) in 3963 ms on localhost (executor driver) (1/32)
15:57:44.125 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:44.125 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:44.130 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:44.131 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
15:57:44.164 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 2.0 (TID 9, localhost, executor driver, partition 6, ANY, 4715 bytes)
15:57:44.165 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 3) in 4014 ms on localhost (executor driver) (2/32)
15:57:44.167 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 6.0 in stage 2.0 (TID 9)
15:57:44.172 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:44.172 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:44.187 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 4). 2887 bytes result sent to driver
15:57:44.188 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 2.0 (TID 10, localhost, executor driver, partition 7, ANY, 4715 bytes)
15:57:44.189 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 4) in 4036 ms on localhost (executor driver) (3/32)
15:57:44.223 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 5) in 4070 ms on localhost (executor driver) (4/32)
15:57:44.239 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 7.0 in stage 2.0 (TID 10)
15:57:44.244 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:44.244 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:44.445 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 4.0 in stage 2.0 (TID 7). 2887 bytes result sent to driver
15:57:44.447 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 2.0 (TID 11, localhost, executor driver, partition 8, ANY, 4715 bytes)
15:57:44.448 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 8.0 in stage 2.0 (TID 11)
15:57:44.448 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 7) in 335 ms on localhost (executor driver) (5/32)
15:57:44.462 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:44.462 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:44.500 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 7.0 in stage 2.0 (TID 10). 2844 bytes result sent to driver
15:57:44.500 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 2.0 (TID 12, localhost, executor driver, partition 9, ANY, 4715 bytes)
15:57:44.501 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 9.0 in stage 2.0 (TID 12)
15:57:44.501 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 2.0 (TID 10) in 313 ms on localhost (executor driver) (6/32)
15:57:44.504 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:44.505 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
15:57:44.537 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 5.0 in stage 2.0 (TID 8). 2887 bytes result sent to driver
15:57:44.539 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 2.0 (TID 13, localhost, executor driver, partition 10, ANY, 4715 bytes)
15:57:44.540 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 8) in 426 ms on localhost (executor driver) (7/32)
15:57:44.540 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 10.0 in stage 2.0 (TID 13)
15:57:44.553 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 6.0 in stage 2.0 (TID 9). 2844 bytes result sent to driver
15:57:44.553 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:44.553 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:44.554 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 2.0 (TID 14, localhost, executor driver, partition 11, ANY, 4715 bytes)
15:57:44.558 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 2.0 (TID 9) in 394 ms on localhost (executor driver) (8/32)
15:57:44.570 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 11.0 in stage 2.0 (TID 14)
15:57:44.575 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:44.576 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
15:57:44.691 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 8.0 in stage 2.0 (TID 11). 2844 bytes result sent to driver
15:57:44.692 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 12.0 in stage 2.0 (TID 15, localhost, executor driver, partition 12, ANY, 4715 bytes)
15:57:44.693 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 2.0 (TID 11) in 246 ms on localhost (executor driver) (9/32)
15:57:44.694 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 12.0 in stage 2.0 (TID 15)
15:57:44.702 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:44.702 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:44.751 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 10.0 in stage 2.0 (TID 13). 2887 bytes result sent to driver
15:57:44.752 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 13.0 in stage 2.0 (TID 16, localhost, executor driver, partition 13, ANY, 4715 bytes)
15:57:44.753 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 2.0 (TID 13) in 215 ms on localhost (executor driver) (10/32)
15:57:44.754 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 13.0 in stage 2.0 (TID 16)
15:57:44.764 INFO  [Executor task launch worker for task 16] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:44.764 INFO  [Executor task launch worker for task 16] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:44.789 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 11.0 in stage 2.0 (TID 14). 2887 bytes result sent to driver
15:57:44.798 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 14.0 in stage 2.0 (TID 17, localhost, executor driver, partition 14, ANY, 4715 bytes)
15:57:44.803 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 2.0 (TID 14) in 248 ms on localhost (executor driver) (11/32)
15:57:44.806 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 14.0 in stage 2.0 (TID 17)
15:57:44.815 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:44.815 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
15:57:44.902 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 9.0 in stage 2.0 (TID 12). 2844 bytes result sent to driver
15:57:44.906 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 15.0 in stage 2.0 (TID 18, localhost, executor driver, partition 15, ANY, 4715 bytes)
15:57:44.907 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 2.0 (TID 12) in 407 ms on localhost (executor driver) (12/32)
15:57:44.907 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 15.0 in stage 2.0 (TID 18)
15:57:44.911 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:44.912 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
15:57:45.097 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 12.0 in stage 2.0 (TID 15). 2844 bytes result sent to driver
15:57:45.098 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 16.0 in stage 2.0 (TID 19, localhost, executor driver, partition 16, ANY, 4715 bytes)
15:57:45.099 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 12.0 in stage 2.0 (TID 15) in 408 ms on localhost (executor driver) (13/32)
15:57:45.099 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 16.0 in stage 2.0 (TID 19)
15:57:45.102 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:45.102 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:45.158 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 14.0 in stage 2.0 (TID 17). 2844 bytes result sent to driver
15:57:45.159 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 17.0 in stage 2.0 (TID 20, localhost, executor driver, partition 17, ANY, 4715 bytes)
15:57:45.160 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 15.0 in stage 2.0 (TID 18). 2844 bytes result sent to driver
15:57:45.160 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 17.0 in stage 2.0 (TID 20)
15:57:45.161 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 18.0 in stage 2.0 (TID 21, localhost, executor driver, partition 18, ANY, 4715 bytes)
15:57:45.161 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 14.0 in stage 2.0 (TID 17) in 364 ms on localhost (executor driver) (14/32)
15:57:45.162 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 15.0 in stage 2.0 (TID 18) in 256 ms on localhost (executor driver) (15/32)
15:57:45.163 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Running task 18.0 in stage 2.0 (TID 21)
15:57:45.163 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:45.164 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
15:57:45.167 INFO  [Executor task launch worker for task 21] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:45.167 INFO  [Executor task launch worker for task 21] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:45.176 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 13.0 in stage 2.0 (TID 16). 2844 bytes result sent to driver
15:57:45.177 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 19.0 in stage 2.0 (TID 22, localhost, executor driver, partition 19, ANY, 4715 bytes)
15:57:45.178 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 13.0 in stage 2.0 (TID 16) in 426 ms on localhost (executor driver) (16/32)
15:57:45.178 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Running task 19.0 in stage 2.0 (TID 22)
15:57:45.185 INFO  [Executor task launch worker for task 22] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:45.186 INFO  [Executor task launch worker for task 22] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
15:57:45.282 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 16.0 in stage 2.0 (TID 19). 2844 bytes result sent to driver
15:57:45.291 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 20.0 in stage 2.0 (TID 23, localhost, executor driver, partition 20, ANY, 4715 bytes)
15:57:45.292 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 16.0 in stage 2.0 (TID 19) in 195 ms on localhost (executor driver) (17/32)
15:57:45.299 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Running task 20.0 in stage 2.0 (TID 23)
15:57:45.303 INFO  [Executor task launch worker for task 23] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:45.316 INFO  [Executor task launch worker for task 23] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 13 ms
15:57:45.378 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Finished task 19.0 in stage 2.0 (TID 22). 2887 bytes result sent to driver
15:57:45.379 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 21.0 in stage 2.0 (TID 24, localhost, executor driver, partition 21, ANY, 4715 bytes)
15:57:45.380 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Running task 21.0 in stage 2.0 (TID 24)
15:57:45.381 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 19.0 in stage 2.0 (TID 22) in 204 ms on localhost (executor driver) (18/32)
15:57:45.383 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:45.384 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
15:57:45.433 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 17.0 in stage 2.0 (TID 20). 2844 bytes result sent to driver
15:57:45.438 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 22.0 in stage 2.0 (TID 25, localhost, executor driver, partition 22, ANY, 4715 bytes)
15:57:45.441 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 17.0 in stage 2.0 (TID 20) in 282 ms on localhost (executor driver) (19/32)
15:57:45.441 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Running task 22.0 in stage 2.0 (TID 25)
15:57:45.451 INFO  [Executor task launch worker for task 25] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:45.451 INFO  [Executor task launch worker for task 25] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:45.473 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Finished task 18.0 in stage 2.0 (TID 21). 2887 bytes result sent to driver
15:57:45.476 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 23.0 in stage 2.0 (TID 26, localhost, executor driver, partition 23, ANY, 4715 bytes)
15:57:45.476 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Running task 23.0 in stage 2.0 (TID 26)
15:57:45.476 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 18.0 in stage 2.0 (TID 21) in 315 ms on localhost (executor driver) (20/32)
15:57:45.483 INFO  [Executor task launch worker for task 26] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:45.483 INFO  [Executor task launch worker for task 26] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:45.528 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Finished task 21.0 in stage 2.0 (TID 24). 2844 bytes result sent to driver
15:57:45.533 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 24.0 in stage 2.0 (TID 27, localhost, executor driver, partition 24, ANY, 4715 bytes)
15:57:45.534 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 21.0 in stage 2.0 (TID 24) in 155 ms on localhost (executor driver) (21/32)
15:57:45.534 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Running task 24.0 in stage 2.0 (TID 27)
15:57:45.537 INFO  [Executor task launch worker for task 27] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:45.538 INFO  [Executor task launch worker for task 27] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
15:57:45.704 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Finished task 20.0 in stage 2.0 (TID 23). 2844 bytes result sent to driver
15:57:45.704 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 25.0 in stage 2.0 (TID 28, localhost, executor driver, partition 25, ANY, 4715 bytes)
15:57:45.705 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Running task 25.0 in stage 2.0 (TID 28)
15:57:45.705 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 20.0 in stage 2.0 (TID 23) in 419 ms on localhost (executor driver) (22/32)
15:57:45.710 INFO  [Executor task launch worker for task 28] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:45.710 INFO  [Executor task launch worker for task 28] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:45.723 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Finished task 24.0 in stage 2.0 (TID 27). 2844 bytes result sent to driver
15:57:45.724 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 26.0 in stage 2.0 (TID 29, localhost, executor driver, partition 26, ANY, 4715 bytes)
15:57:45.725 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 24.0 in stage 2.0 (TID 27) in 192 ms on localhost (executor driver) (23/32)
15:57:45.725 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Running task 26.0 in stage 2.0 (TID 29)
15:57:45.739 INFO  [Executor task launch worker for task 29] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:45.739 INFO  [Executor task launch worker for task 29] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:45.756 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Finished task 23.0 in stage 2.0 (TID 26). 2887 bytes result sent to driver
15:57:45.759 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 27.0 in stage 2.0 (TID 30, localhost, executor driver, partition 27, ANY, 4715 bytes)
15:57:45.760 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 23.0 in stage 2.0 (TID 26) in 285 ms on localhost (executor driver) (24/32)
15:57:45.761 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Running task 27.0 in stage 2.0 (TID 30)
15:57:45.769 INFO  [Executor task launch worker for task 30] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:45.769 INFO  [Executor task launch worker for task 30] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:45.775 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Finished task 22.0 in stage 2.0 (TID 25). 2887 bytes result sent to driver
15:57:45.776 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 28.0 in stage 2.0 (TID 31, localhost, executor driver, partition 28, ANY, 4715 bytes)
15:57:45.776 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Running task 28.0 in stage 2.0 (TID 31)
15:57:45.776 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 22.0 in stage 2.0 (TID 25) in 339 ms on localhost (executor driver) (25/32)
15:57:45.783 INFO  [Executor task launch worker for task 31] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:45.784 INFO  [Executor task launch worker for task 31] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
15:57:45.898 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Finished task 26.0 in stage 2.0 (TID 29). 2844 bytes result sent to driver
15:57:45.899 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 29.0 in stage 2.0 (TID 32, localhost, executor driver, partition 29, ANY, 4715 bytes)
15:57:45.965 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 26.0 in stage 2.0 (TID 29) in 242 ms on localhost (executor driver) (26/32)
15:57:45.966 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Running task 29.0 in stage 2.0 (TID 32)
15:57:45.968 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Finished task 25.0 in stage 2.0 (TID 28). 2887 bytes result sent to driver
15:57:45.969 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 30.0 in stage 2.0 (TID 33, localhost, executor driver, partition 30, ANY, 4715 bytes)
15:57:45.969 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 25.0 in stage 2.0 (TID 28) in 265 ms on localhost (executor driver) (27/32)
15:57:45.969 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Running task 30.0 in stage 2.0 (TID 33)
15:57:45.974 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:45.975 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
15:57:45.993 INFO  [Executor task launch worker for task 32] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:45.993 INFO  [Executor task launch worker for task 32] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:46.024 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Finished task 27.0 in stage 2.0 (TID 30). 2887 bytes result sent to driver
15:57:46.026 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 31.0 in stage 2.0 (TID 34, localhost, executor driver, partition 31, ANY, 4715 bytes)
15:57:46.027 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 27.0 in stage 2.0 (TID 30) in 268 ms on localhost (executor driver) (28/32)
15:57:46.027 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Running task 31.0 in stage 2.0 (TID 34)
15:57:46.032 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
15:57:46.032 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:46.036 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Finished task 28.0 in stage 2.0 (TID 31). 2887 bytes result sent to driver
15:57:46.041 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 28.0 in stage 2.0 (TID 31) in 266 ms on localhost (executor driver) (29/32)
15:57:46.129 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Finished task 31.0 in stage 2.0 (TID 34). 2844 bytes result sent to driver
15:57:46.131 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 31.0 in stage 2.0 (TID 34) in 105 ms on localhost (executor driver) (30/32)
15:57:46.140 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Finished task 30.0 in stage 2.0 (TID 33). 2844 bytes result sent to driver
15:57:46.140 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 30.0 in stage 2.0 (TID 33) in 171 ms on localhost (executor driver) (31/32)
15:57:46.151 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Finished task 29.0 in stage 2.0 (TID 32). 2887 bytes result sent to driver
15:57:46.152 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 29.0 in stage 2.0 (TID 32) in 253 ms on localhost (executor driver) (32/32)
15:57:46.152 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (show at DMReleaseCustomer.scala:75) finished in 6.001 s
15:57:46.152 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
15:57:46.153 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
15:57:46.153 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
15:57:46.153 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
15:57:46.153 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
15:57:46.161 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75), which has no missing parents
15:57:46.170 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 23.9 KB, free 891.6 MB)
15:57:46.172 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.1 KB, free 891.6 MB)
15:57:46.172 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.237.1:5514 (size: 10.1 KB, free: 892.0 MB)
15:57:46.176 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
15:57:46.178 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(0))
15:57:46.178 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
15:57:46.180 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 35, localhost, executor driver, partition 0, ANY, 4726 bytes)
15:57:46.181 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 35)
15:57:46.189 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
15:57:46.189 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:46.229 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 35). 3173 bytes result sent to driver
15:57:46.232 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 35) in 53 ms on localhost (executor driver) (1/1)
15:57:46.232 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
15:57:46.232 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at DMReleaseCustomer.scala:75) finished in 0.053 s
15:57:46.237 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at DMReleaseCustomer.scala:75, took 8.141990 s
15:57:46.295 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:75
15:57:46.299 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (show at DMReleaseCustomer.scala:75) with 4 output partitions
15:57:46.299 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (show at DMReleaseCustomer.scala:75)
15:57:46.299 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
15:57:46.300 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
15:57:46.300 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75), which has no missing parents
15:57:46.303 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 23.9 KB, free 891.6 MB)
15:57:46.305 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.1 KB, free 891.6 MB)
15:57:46.306 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.237.1:5514 (size: 10.1 KB, free: 891.9 MB)
15:57:46.307 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
15:57:46.307 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
15:57:46.308 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 4 tasks
15:57:46.309 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 6.0 (TID 36, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
15:57:46.309 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 37, localhost, executor driver, partition 1, ANY, 4726 bytes)
15:57:46.309 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 6.0 (TID 38, localhost, executor driver, partition 3, ANY, 4726 bytes)
15:57:46.310 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 6.0 (TID 39, localhost, executor driver, partition 4, ANY, 4726 bytes)
15:57:46.312 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 37)
15:57:46.312 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Running task 1.0 in stage 6.0 (TID 36)
15:57:46.312 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Running task 2.0 in stage 6.0 (TID 38)
15:57:46.312 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Running task 3.0 in stage 6.0 (TID 39)
15:57:46.316 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 32 blocks
15:57:46.316 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:46.317 INFO  [Executor task launch worker for task 37] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
15:57:46.318 INFO  [Executor task launch worker for task 37] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
15:57:46.319 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Finished task 1.0 in stage 6.0 (TID 36). 3087 bytes result sent to driver
15:57:46.320 INFO  [Executor task launch worker for task 39] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
15:57:46.321 INFO  [Executor task launch worker for task 39] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
15:57:46.323 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 6.0 (TID 36) in 15 ms on localhost (executor driver) (1/4)
15:57:46.325 INFO  [Executor task launch worker for task 38] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
15:57:46.325 INFO  [Executor task launch worker for task 38] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:46.357 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 37). 3174 bytes result sent to driver
15:57:46.358 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 37) in 49 ms on localhost (executor driver) (2/4)
15:57:46.364 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Finished task 3.0 in stage 6.0 (TID 39). 3207 bytes result sent to driver
15:57:46.365 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 6.0 (TID 39) in 56 ms on localhost (executor driver) (3/4)
15:57:46.392 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Finished task 2.0 in stage 6.0 (TID 38). 3218 bytes result sent to driver
15:57:46.394 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 6.0 (TID 38) in 85 ms on localhost (executor driver) (4/4)
15:57:46.395 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
15:57:46.397 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (show at DMReleaseCustomer.scala:75) finished in 0.089 s
15:57:46.398 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: show at DMReleaseCustomer.scala:75, took 0.103269 s
15:57:46.406 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:75
15:57:46.407 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 3 (show at DMReleaseCustomer.scala:75) with 4 output partitions
15:57:46.407 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (show at DMReleaseCustomer.scala:75)
15:57:46.407 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 8)
15:57:46.407 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
15:57:46.408 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75), which has no missing parents
15:57:46.411 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 23.9 KB, free 891.5 MB)
15:57:46.416 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.0 KB, free 891.5 MB)
15:57:46.416 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.237.1:5514 (size: 10.0 KB, free: 891.9 MB)
15:57:46.416 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
15:57:46.417 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 9 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(5, 6, 7, 8))
15:57:46.417 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 4 tasks
15:57:46.418 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 40, localhost, executor driver, partition 7, PROCESS_LOCAL, 4726 bytes)
15:57:46.419 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 9.0 (TID 41, localhost, executor driver, partition 8, PROCESS_LOCAL, 4726 bytes)
15:57:46.419 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 42, localhost, executor driver, partition 5, ANY, 4726 bytes)
15:57:46.419 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 43, localhost, executor driver, partition 6, ANY, 4726 bytes)
15:57:46.420 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 40)
15:57:46.420 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Running task 3.0 in stage 9.0 (TID 41)
15:57:46.420 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 42)
15:57:46.420 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 43)
15:57:46.425 INFO  [Executor task launch worker for task 43] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
15:57:46.426 INFO  [Executor task launch worker for task 43] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
15:57:46.427 INFO  [Executor task launch worker for task 40] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 32 blocks
15:57:46.427 INFO  [Executor task launch worker for task 40] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:46.432 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 40). 3087 bytes result sent to driver
15:57:46.434 INFO  [Executor task launch worker for task 41] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 32 blocks
15:57:46.434 INFO  [Executor task launch worker for task 41] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
15:57:46.444 INFO  [Executor task launch worker for task 42] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
15:57:46.444 INFO  [Executor task launch worker for task 42] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:57:46.458 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 40) in 40 ms on localhost (executor driver) (1/4)
15:57:46.467 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Finished task 3.0 in stage 9.0 (TID 41). 3087 bytes result sent to driver
15:57:46.468 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 43). 3175 bytes result sent to driver
15:57:46.470 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 43) in 51 ms on localhost (executor driver) (2/4)
15:57:46.470 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 9.0 (TID 41) in 52 ms on localhost (executor driver) (3/4)
15:57:46.502 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 42). 3239 bytes result sent to driver
15:57:46.503 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 42) in 84 ms on localhost (executor driver) (4/4)
15:57:46.503 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
15:57:46.503 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 9 (show at DMReleaseCustomer.scala:75) finished in 0.085 s
15:57:46.504 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 3 finished: show at DMReleaseCustomer.scala:75, took 0.098594 s
15:57:46.821 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@54e81b21{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
15:57:46.883 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
15:57:50.041 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
15:57:50.327 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
15:57:50.329 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
15:57:50.330 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
15:57:50.354 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
15:57:50.402 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
15:57:50.503 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
15:57:50.506 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-ebcf71ef-1381-432b-9b53-d562db16d8a8
16:23:10.657 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:23:12.094 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
16:23:12.242 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
16:23:12.243 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
16:23:12.244 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:23:12.245 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:23:12.246 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
16:23:14.510 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5879.
16:23:14.687 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:23:14.821 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:23:14.838 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:23:14.839 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:23:14.867 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-9bf4bbe2-3215-4511-8599-796f445b5c5c
16:23:14.970 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
16:23:15.135 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:23:15.803 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @26627ms
16:23:16.062 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:23:16.092 INFO  [main] org.spark_project.jetty.server.Server - Started @26979ms
16:23:16.140 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@54e81b21{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:23:16.143 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:23:16.194 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/jobs,null,AVAILABLE,@Spark}
16:23:16.195 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/jobs/json,null,AVAILABLE,@Spark}
16:23:16.195 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e11ab3d{/jobs/job,null,AVAILABLE,@Spark}
16:23:16.197 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b43e173{/jobs/job/json,null,AVAILABLE,@Spark}
16:23:16.197 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545f80bf{/stages,null,AVAILABLE,@Spark}
16:23:16.198 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22fa55b2{/stages/json,null,AVAILABLE,@Spark}
16:23:16.199 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6594402a{/stages/stage,null,AVAILABLE,@Spark}
16:23:16.202 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c3f01f{/stages/stage/json,null,AVAILABLE,@Spark}
16:23:16.203 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/stages/pool,null,AVAILABLE,@Spark}
16:23:16.207 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/stages/pool/json,null,AVAILABLE,@Spark}
16:23:16.209 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/storage,null,AVAILABLE,@Spark}
16:23:16.210 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5298dead{/storage/json,null,AVAILABLE,@Spark}
16:23:16.211 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/storage/rdd,null,AVAILABLE,@Spark}
16:23:16.211 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/storage/rdd/json,null,AVAILABLE,@Spark}
16:23:16.212 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/environment,null,AVAILABLE,@Spark}
16:23:16.213 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b986cd{/environment/json,null,AVAILABLE,@Spark}
16:23:16.214 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41c62850{/executors,null,AVAILABLE,@Spark}
16:23:16.217 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@328572f0{/executors/json,null,AVAILABLE,@Spark}
16:23:16.219 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f460bb{/executors/threadDump,null,AVAILABLE,@Spark}
16:23:16.220 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d2a6eac{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:23:16.237 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c0f7678{/static,null,AVAILABLE,@Spark}
16:23:16.238 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f1249d{/,null,AVAILABLE,@Spark}
16:23:16.240 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a68df9{/api,null,AVAILABLE,@Spark}
16:23:16.241 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64711bf2{/jobs/job/kill,null,AVAILABLE,@Spark}
16:23:16.242 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c1e23ff{/stages/stage/kill,null,AVAILABLE,@Spark}
16:23:16.246 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
16:23:16.647 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:23:16.776 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5902.
16:23:16.778 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:5902
16:23:16.790 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:23:16.956 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 5902, None)
16:23:17.002 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:5902 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 5902, None)
16:23:17.045 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 5902, None)
16:23:17.047 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 5902, None)
16:23:17.773 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c27d163{/metrics/json,null,AVAILABLE,@Spark}
16:23:18.229 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
16:23:18.332 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
16:23:18.333 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
16:23:18.377 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b08f438{/SQL,null,AVAILABLE,@Spark}
16:23:18.378 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b2f8ab6{/SQL/json,null,AVAILABLE,@Spark}
16:23:18.380 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65ae095c{/SQL/execution,null,AVAILABLE,@Spark}
16:23:18.381 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e140e59{/SQL/execution/json,null,AVAILABLE,@Spark}
16:23:18.417 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ae263bf{/static/sql,null,AVAILABLE,@Spark}
16:23:20.335 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:23:22.261 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:23:22.394 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:23:25.328 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:23:29.077 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:23:29.154 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:23:30.205 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:23:30.220 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:23:30.413 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:23:31.226 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:23:31.230 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
16:23:31.310 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
16:23:31.311 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
16:23:31.407 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:23:31.407 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:23:31.412 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:23:31.413 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:23:31.418 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:23:31.419 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:23:31.425 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
16:23:31.425 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
16:23:31.430 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:23:31.430 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:23:31.437 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
16:23:31.438 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
16:23:31.448 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
16:23:31.448 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
16:23:33.931 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/114b44cf-e1d2-441d-aeb8-cdce6b5339f6_resources
16:23:33.945 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/114b44cf-e1d2-441d-aeb8-cdce6b5339f6
16:23:33.949 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/114b44cf-e1d2-441d-aeb8-cdce6b5339f6
16:23:33.974 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/114b44cf-e1d2-441d-aeb8-cdce6b5339f6/_tmp_space.db
16:23:34.000 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
16:23:34.210 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:23:34.211 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
16:23:34.260 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:23:34.261 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:23:34.270 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:23:35.346 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/e74ec38a-2a57-49c4-aea0-b20118d52ec9_resources
16:23:35.362 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/e74ec38a-2a57-49c4-aea0-b20118d52ec9
16:23:35.371 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/e74ec38a-2a57-49c4-aea0-b20118d52ec9
16:23:35.400 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/e74ec38a-2a57-49c4-aea0-b20118d52ec9/_tmp_space.db
16:23:35.406 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
16:23:35.740 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:23:40.601 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
16:23:41.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
16:23:41.348 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
16:23:41.394 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:23:41.395 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:23:41.665 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:23:41.667 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:23:41.718 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:41.786 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:41.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:41.788 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:41.788 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:41.789 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:41.789 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:41.789 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:41.790 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:23:41.791 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:41.791 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:41.792 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:41.792 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:41.799 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:41.800 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:41.800 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:41.801 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:41.801 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:23:41.904 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:23:43.934 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:23:43.957 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:23:43.958 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
16:23:43.959 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:23:43.959 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:23:43.960 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:23:43.961 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: idcard
16:23:43.961 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: age
16:23:43.961 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: getAgeRange(age) as age_range
16:23:43.994 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: gender
16:23:43.995 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: area_code
16:23:43.996 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:23:43.996 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:23:44.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:23:44.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
16:23:45.120 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
16:23:45.120 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
16:23:45.130 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:23:45.130 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:23:45.182 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:23:45.182 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:23:45.224 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:45.224 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:45.225 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:45.225 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:45.226 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:45.226 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:45.226 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:45.226 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:45.227 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:23:45.227 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:45.227 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:45.227 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:45.228 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:45.228 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:45.228 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:45.228 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:45.228 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:23:45.228 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:23:45.476 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
16:23:45.476 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
16:23:45.525 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=dw_release tbl=dw_release_customer
16:23:45.525 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=dw_release tbl=dw_release_customer	
16:23:46.636 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#17),(bdp_day#17 = 2019-09-24)
16:23:46.639 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
16:23:46.642 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 9 more fields>
16:23:46.697 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: 
16:23:46.716 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
16:23:48.563 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 537.6482 ms
16:23:49.057 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 265.9 KB, free 897.3 MB)
16:23:50.018 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.7 KB, free 897.3 MB)
16:23:50.021 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:5902 (size: 22.7 KB, free: 897.6 MB)
16:23:50.100 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from persist at DMReleaseCustomer.scala:48
16:23:50.251 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
16:23:51.103 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 63.7212 ms
16:23:51.186 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:50
16:23:51.346 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DMReleaseCustomer.scala:50) with 1 output partitions
16:23:51.347 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at DMReleaseCustomer.scala:50)
16:23:51.349 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
16:23:51.359 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:23:51.426 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at show at DMReleaseCustomer.scala:50), which has no missing parents
16:23:51.837 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 26.9 KB, free 897.3 MB)
16:23:51.841 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.4 KB, free 897.3 MB)
16:23:51.844 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:5902 (size: 10.4 KB, free: 897.6 MB)
16:23:51.845 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
16:23:51.898 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at show at DMReleaseCustomer.scala:50) (first 15 tasks are for partitions Vector(0))
16:23:51.914 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
16:23:52.140 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5413 bytes)
16:23:52.176 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
16:23:52.549 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/bdp_day=2019-09-24/000000_0, range: 0-4194304, partition values: [2019-09-24]
16:23:54.384 WARN  [Executor task launch worker for task 0] org.apache.parquet.CorruptStatistics - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
16:23:57.525 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.memory.MemoryStore - Block rdd_2_0 stored as values in memory (estimated size 5.6 MB, free 891.7 MB)
16:23:57.527 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added rdd_2_0 in memory on 192.168.237.1:5902 (size: 5.6 MB, free: 892.0 MB)
16:23:57.592 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 23.7995 ms
16:23:57.693 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 75.7375 ms
16:23:58.039 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - 1 block locks were not released by TID = 0:
[rdd_2_0]
16:23:58.068 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3016 bytes result sent to driver
16:23:58.155 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 6057 ms on localhost (executor driver) (1/1)
16:23:58.170 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
16:23:58.187 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (show at DMReleaseCustomer.scala:50) finished in 6.158 s
16:23:58.214 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DMReleaseCustomer.scala:50, took 7.027493 s
16:23:58.570 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:23:58.571 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:23:58.573 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:23:58.574 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: user_count
16:23:58.575 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: total_count
16:23:58.579 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:23:59.647 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.0066 ms
16:23:59.894 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 95.295 ms
16:24:00.134 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 205.3449 ms
16:24:00.216 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 69.6634 ms
16:24:00.364 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:75
16:24:00.374 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 7 (show at DMReleaseCustomer.scala:75)
16:24:00.410 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (show at DMReleaseCustomer.scala:75)
16:24:00.410 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at DMReleaseCustomer.scala:75) with 1 output partitions
16:24:00.410 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at DMReleaseCustomer.scala:75)
16:24:00.410 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
16:24:00.415 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
16:24:00.421 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:24:00.446 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 45.7 KB, free 891.6 MB)
16:24:00.456 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 18.1 KB, free 891.6 MB)
16:24:00.457 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:5902 (size: 18.1 KB, free: 892.0 MB)
16:24:00.458 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
16:24:00.476 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(0, 1))
16:24:00.476 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 2 tasks
16:24:00.508 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5402 bytes)
16:24:00.550 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, ANY, 5402 bytes)
16:24:00.554 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
16:24:00.622 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.BlockManager - Found block rdd_2_0 locally
16:24:00.760 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 137.0364 ms
16:24:00.773 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 1.0 (TID 2)
16:24:00.787 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/bdp_day=2019-09-24/000000_0, range: 4194304-5574414, partition values: [2019-09-24]
16:24:00.808 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.1955 ms
16:24:01.007 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.8639 ms
16:24:01.064 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 25.0493 ms
16:24:01.136 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 30.1192 ms
16:24:01.268 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 28.3832 ms
16:24:02.741 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2177 bytes result sent to driver
16:24:02.744 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 2251 ms on localhost (executor driver) (1/2)
16:24:03.280 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.memory.MemoryStore - Block rdd_2_1 stored as values in memory (estimated size 16.0 B, free 891.6 MB)
16:24:03.281 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added rdd_2_1 in memory on 192.168.237.1:5902 (size: 16.0 B, free: 892.0 MB)
16:24:03.295 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 1.0 (TID 2). 2600 bytes result sent to driver
16:24:03.300 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 1.0 (TID 2) in 2758 ms on localhost (executor driver) (2/2)
16:24:03.302 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
16:24:03.302 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (show at DMReleaseCustomer.scala:75) finished in 2.809 s
16:24:03.303 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:24:03.304 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
16:24:03.324 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
16:24:03.325 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:24:03.341 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:24:03.359 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 43.5 KB, free 891.6 MB)
16:24:03.363 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 14.5 KB, free 891.6 MB)
16:24:03.364 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.237.1:5902 (size: 14.5 KB, free: 891.9 MB)
16:24:03.365 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1004
16:24:03.366 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 32 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
16:24:03.366 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 32 tasks
16:24:03.368 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 4715 bytes)
16:24:03.369 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 4, localhost, executor driver, partition 1, ANY, 4715 bytes)
16:24:03.369 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 5, localhost, executor driver, partition 2, ANY, 4715 bytes)
16:24:03.370 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 6, localhost, executor driver, partition 3, ANY, 4715 bytes)
16:24:03.371 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 3)
16:24:03.371 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 4)
16:24:03.388 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 5)
16:24:03.389 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 6)
16:24:03.489 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:03.489 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:03.489 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:03.489 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:03.507 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 53 ms
16:24:03.508 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 57 ms
16:24:03.509 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 58 ms
16:24:03.512 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 60 ms
16:24:03.566 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.52 ms
16:24:03.583 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.2258 ms
16:24:03.697 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 50.3822 ms
16:24:03.759 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.237.1:5902 in memory (size: 18.1 KB, free: 892.0 MB)
16:24:03.763 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 46.9564 ms
16:24:03.780 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.2576 ms
16:24:04.243 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 6). 2887 bytes result sent to driver
16:24:04.246 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 3). 2887 bytes result sent to driver
16:24:04.247 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 7, localhost, executor driver, partition 4, ANY, 4715 bytes)
16:24:04.247 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 8, localhost, executor driver, partition 5, ANY, 4715 bytes)
16:24:04.248 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 4.0 in stage 2.0 (TID 7)
16:24:04.249 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 5.0 in stage 2.0 (TID 8)
16:24:04.249 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 3) in 882 ms on localhost (executor driver) (1/32)
16:24:04.254 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:04.254 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:04.254 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:04.254 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:04.311 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 4). 2887 bytes result sent to driver
16:24:04.313 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 2.0 (TID 9, localhost, executor driver, partition 6, ANY, 4715 bytes)
16:24:04.314 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 4) in 945 ms on localhost (executor driver) (2/32)
16:24:04.318 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 6.0 in stage 2.0 (TID 9)
16:24:04.324 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 5). 2887 bytes result sent to driver
16:24:04.325 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 2.0 (TID 10, localhost, executor driver, partition 7, ANY, 4715 bytes)
16:24:04.326 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 5) in 957 ms on localhost (executor driver) (3/32)
16:24:04.326 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 7.0 in stage 2.0 (TID 10)
16:24:04.335 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:04.336 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:24:04.389 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:04.390 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
16:24:04.436 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 6) in 1066 ms on localhost (executor driver) (4/32)
16:24:04.527 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 4.0 in stage 2.0 (TID 7). 2844 bytes result sent to driver
16:24:04.530 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 2.0 (TID 11, localhost, executor driver, partition 8, ANY, 4715 bytes)
16:24:04.531 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 7) in 285 ms on localhost (executor driver) (5/32)
16:24:04.532 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 8.0 in stage 2.0 (TID 11)
16:24:04.549 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:04.549 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:04.598 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 6.0 in stage 2.0 (TID 9). 2887 bytes result sent to driver
16:24:04.598 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 5.0 in stage 2.0 (TID 8). 2844 bytes result sent to driver
16:24:04.601 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 2.0 (TID 12, localhost, executor driver, partition 9, ANY, 4715 bytes)
16:24:04.608 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 2.0 (TID 13, localhost, executor driver, partition 10, ANY, 4715 bytes)
16:24:04.611 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 2.0 (TID 9) in 299 ms on localhost (executor driver) (6/32)
16:24:04.613 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 8) in 366 ms on localhost (executor driver) (7/32)
16:24:04.617 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 9.0 in stage 2.0 (TID 12)
16:24:04.618 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 10.0 in stage 2.0 (TID 13)
16:24:04.624 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:04.624 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:04.626 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:04.627 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:24:04.685 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 7.0 in stage 2.0 (TID 10). 2844 bytes result sent to driver
16:24:04.687 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 2.0 (TID 14, localhost, executor driver, partition 11, ANY, 4715 bytes)
16:24:04.687 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 11.0 in stage 2.0 (TID 14)
16:24:04.689 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 2.0 (TID 10) in 364 ms on localhost (executor driver) (8/32)
16:24:04.694 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:04.694 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:04.735 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 8.0 in stage 2.0 (TID 11). 2887 bytes result sent to driver
16:24:04.739 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 12.0 in stage 2.0 (TID 15, localhost, executor driver, partition 12, ANY, 4715 bytes)
16:24:04.741 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 2.0 (TID 11) in 210 ms on localhost (executor driver) (9/32)
16:24:04.745 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 12.0 in stage 2.0 (TID 15)
16:24:04.751 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:04.752 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:24:04.814 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 9.0 in stage 2.0 (TID 12). 2887 bytes result sent to driver
16:24:04.816 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 13.0 in stage 2.0 (TID 16, localhost, executor driver, partition 13, ANY, 4715 bytes)
16:24:04.816 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 13.0 in stage 2.0 (TID 16)
16:24:04.817 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 2.0 (TID 12) in 217 ms on localhost (executor driver) (10/32)
16:24:04.821 INFO  [Executor task launch worker for task 16] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:04.821 INFO  [Executor task launch worker for task 16] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:04.955 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 10.0 in stage 2.0 (TID 13). 2844 bytes result sent to driver
16:24:04.957 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 14.0 in stage 2.0 (TID 17, localhost, executor driver, partition 14, ANY, 4715 bytes)
16:24:04.960 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 14.0 in stage 2.0 (TID 17)
16:24:04.967 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:04.967 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:24:04.994 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 11.0 in stage 2.0 (TID 14). 2844 bytes result sent to driver
16:24:04.995 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 15.0 in stage 2.0 (TID 18, localhost, executor driver, partition 15, ANY, 4715 bytes)
16:24:04.997 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 2.0 (TID 13) in 395 ms on localhost (executor driver) (11/32)
16:24:04.998 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 2.0 (TID 14) in 312 ms on localhost (executor driver) (12/32)
16:24:04.999 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 15.0 in stage 2.0 (TID 18)
16:24:05.009 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:05.009 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:24:05.045 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 12.0 in stage 2.0 (TID 15). 2887 bytes result sent to driver
16:24:05.047 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 16.0 in stage 2.0 (TID 19, localhost, executor driver, partition 16, ANY, 4715 bytes)
16:24:05.049 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 12.0 in stage 2.0 (TID 15) in 310 ms on localhost (executor driver) (13/32)
16:24:05.050 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 16.0 in stage 2.0 (TID 19)
16:24:05.058 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:05.058 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:05.128 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 13.0 in stage 2.0 (TID 16). 2844 bytes result sent to driver
16:24:05.129 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 17.0 in stage 2.0 (TID 20, localhost, executor driver, partition 17, ANY, 4715 bytes)
16:24:05.130 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 17.0 in stage 2.0 (TID 20)
16:24:05.131 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 13.0 in stage 2.0 (TID 16) in 314 ms on localhost (executor driver) (14/32)
16:24:05.136 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:05.139 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
16:24:05.141 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 14.0 in stage 2.0 (TID 17). 2844 bytes result sent to driver
16:24:05.144 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 18.0 in stage 2.0 (TID 21, localhost, executor driver, partition 18, ANY, 4715 bytes)
16:24:05.146 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Running task 18.0 in stage 2.0 (TID 21)
16:24:05.146 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 14.0 in stage 2.0 (TID 17) in 189 ms on localhost (executor driver) (15/32)
16:24:05.163 INFO  [Executor task launch worker for task 21] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:05.163 INFO  [Executor task launch worker for task 21] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:24:05.173 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 15.0 in stage 2.0 (TID 18). 2844 bytes result sent to driver
16:24:05.173 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 19.0 in stage 2.0 (TID 22, localhost, executor driver, partition 19, ANY, 4715 bytes)
16:24:05.174 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 15.0 in stage 2.0 (TID 18) in 179 ms on localhost (executor driver) (16/32)
16:24:05.175 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Running task 19.0 in stage 2.0 (TID 22)
16:24:05.178 INFO  [Executor task launch worker for task 22] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:05.178 INFO  [Executor task launch worker for task 22] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:05.243 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 16.0 in stage 2.0 (TID 19). 2887 bytes result sent to driver
16:24:05.245 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 20.0 in stage 2.0 (TID 23, localhost, executor driver, partition 20, ANY, 4715 bytes)
16:24:05.245 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 16.0 in stage 2.0 (TID 19) in 198 ms on localhost (executor driver) (17/32)
16:24:05.246 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Running task 20.0 in stage 2.0 (TID 23)
16:24:05.254 INFO  [Executor task launch worker for task 23] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:05.254 INFO  [Executor task launch worker for task 23] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:05.340 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Finished task 19.0 in stage 2.0 (TID 22). 2844 bytes result sent to driver
16:24:05.341 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 21.0 in stage 2.0 (TID 24, localhost, executor driver, partition 21, ANY, 4715 bytes)
16:24:05.342 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 19.0 in stage 2.0 (TID 22) in 169 ms on localhost (executor driver) (18/32)
16:24:05.343 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Running task 21.0 in stage 2.0 (TID 24)
16:24:05.348 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:05.348 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:05.410 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 17.0 in stage 2.0 (TID 20). 2887 bytes result sent to driver
16:24:05.410 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 22.0 in stage 2.0 (TID 25, localhost, executor driver, partition 22, ANY, 4715 bytes)
16:24:05.411 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Running task 22.0 in stage 2.0 (TID 25)
16:24:05.411 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 17.0 in stage 2.0 (TID 20) in 282 ms on localhost (executor driver) (19/32)
16:24:05.415 INFO  [Executor task launch worker for task 25] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:05.415 INFO  [Executor task launch worker for task 25] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:05.427 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Finished task 18.0 in stage 2.0 (TID 21). 2844 bytes result sent to driver
16:24:05.431 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 23.0 in stage 2.0 (TID 26, localhost, executor driver, partition 23, ANY, 4715 bytes)
16:24:05.432 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Running task 23.0 in stage 2.0 (TID 26)
16:24:05.442 INFO  [Executor task launch worker for task 26] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:05.443 INFO  [Executor task launch worker for task 26] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:24:05.465 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 18.0 in stage 2.0 (TID 21) in 322 ms on localhost (executor driver) (20/32)
16:24:06.526 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 55
16:24:06.530 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.237.1:5902 in memory (size: 10.4 KB, free: 892.0 MB)
16:24:06.531 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
16:24:06.531 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 56
16:24:06.558 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Finished task 20.0 in stage 2.0 (TID 23). 2887 bytes result sent to driver
16:24:06.559 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 24.0 in stage 2.0 (TID 27, localhost, executor driver, partition 24, ANY, 4715 bytes)
16:24:06.564 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Finished task 21.0 in stage 2.0 (TID 24). 2887 bytes result sent to driver
16:24:06.586 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 25.0 in stage 2.0 (TID 28, localhost, executor driver, partition 25, ANY, 4715 bytes)
16:24:06.586 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Running task 24.0 in stage 2.0 (TID 27)
16:24:06.587 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 21.0 in stage 2.0 (TID 24) in 1245 ms on localhost (executor driver) (21/32)
16:24:06.587 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 20.0 in stage 2.0 (TID 23) in 1343 ms on localhost (executor driver) (22/32)
16:24:06.587 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Running task 25.0 in stage 2.0 (TID 28)
16:24:06.591 INFO  [Executor task launch worker for task 28] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:06.591 INFO  [Executor task launch worker for task 27] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:06.591 INFO  [Executor task launch worker for task 28] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:06.591 INFO  [Executor task launch worker for task 27] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:06.636 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Finished task 22.0 in stage 2.0 (TID 25). 2887 bytes result sent to driver
16:24:06.637 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 26.0 in stage 2.0 (TID 29, localhost, executor driver, partition 26, ANY, 4715 bytes)
16:24:06.637 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 22.0 in stage 2.0 (TID 25) in 1227 ms on localhost (executor driver) (23/32)
16:24:06.637 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Running task 26.0 in stage 2.0 (TID 29)
16:24:06.671 INFO  [Executor task launch worker for task 29] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:06.671 INFO  [Executor task launch worker for task 29] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
16:24:06.708 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Finished task 23.0 in stage 2.0 (TID 26). 2887 bytes result sent to driver
16:24:06.709 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 27.0 in stage 2.0 (TID 30, localhost, executor driver, partition 27, ANY, 4715 bytes)
16:24:06.710 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 23.0 in stage 2.0 (TID 26) in 1280 ms on localhost (executor driver) (24/32)
16:24:06.719 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Finished task 25.0 in stage 2.0 (TID 28). 2887 bytes result sent to driver
16:24:06.723 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 28.0 in stage 2.0 (TID 31, localhost, executor driver, partition 28, ANY, 4715 bytes)
16:24:06.725 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Running task 28.0 in stage 2.0 (TID 31)
16:24:06.725 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 25.0 in stage 2.0 (TID 28) in 140 ms on localhost (executor driver) (25/32)
16:24:06.732 INFO  [Executor task launch worker for task 31] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:06.733 INFO  [Executor task launch worker for task 31] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:24:06.743 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Finished task 24.0 in stage 2.0 (TID 27). 2844 bytes result sent to driver
16:24:06.745 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 29.0 in stage 2.0 (TID 32, localhost, executor driver, partition 29, ANY, 4715 bytes)
16:24:06.746 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 24.0 in stage 2.0 (TID 27) in 187 ms on localhost (executor driver) (26/32)
16:24:06.749 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Running task 29.0 in stage 2.0 (TID 32)
16:24:06.749 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Running task 27.0 in stage 2.0 (TID 30)
16:24:06.758 INFO  [Executor task launch worker for task 30] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:06.758 INFO  [Executor task launch worker for task 30] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:06.787 INFO  [Executor task launch worker for task 32] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:06.787 INFO  [Executor task launch worker for task 32] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:06.812 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Finished task 26.0 in stage 2.0 (TID 29). 2844 bytes result sent to driver
16:24:06.812 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 30.0 in stage 2.0 (TID 33, localhost, executor driver, partition 30, ANY, 4715 bytes)
16:24:06.815 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Running task 30.0 in stage 2.0 (TID 33)
16:24:06.815 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 26.0 in stage 2.0 (TID 29) in 179 ms on localhost (executor driver) (27/32)
16:24:06.821 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:06.822 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:24:06.918 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Finished task 27.0 in stage 2.0 (TID 30). 2844 bytes result sent to driver
16:24:06.921 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 31.0 in stage 2.0 (TID 34, localhost, executor driver, partition 31, ANY, 4715 bytes)
16:24:06.921 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Running task 31.0 in stage 2.0 (TID 34)
16:24:06.922 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 27.0 in stage 2.0 (TID 30) in 213 ms on localhost (executor driver) (28/32)
16:24:06.925 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:24:06.925 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:06.964 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Finished task 29.0 in stage 2.0 (TID 32). 2844 bytes result sent to driver
16:24:06.967 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 29.0 in stage 2.0 (TID 32) in 222 ms on localhost (executor driver) (29/32)
16:24:06.975 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Finished task 28.0 in stage 2.0 (TID 31). 2887 bytes result sent to driver
16:24:06.975 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 28.0 in stage 2.0 (TID 31) in 253 ms on localhost (executor driver) (30/32)
16:24:06.991 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Finished task 30.0 in stage 2.0 (TID 33). 2844 bytes result sent to driver
16:24:06.992 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 30.0 in stage 2.0 (TID 33) in 179 ms on localhost (executor driver) (31/32)
16:24:07.002 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Finished task 31.0 in stage 2.0 (TID 34). 2887 bytes result sent to driver
16:24:07.003 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 31.0 in stage 2.0 (TID 34) in 83 ms on localhost (executor driver) (32/32)
16:24:07.003 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
16:24:07.004 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (show at DMReleaseCustomer.scala:75) finished in 3.637 s
16:24:07.004 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:24:07.004 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
16:24:07.004 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
16:24:07.004 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:24:07.005 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:24:07.008 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 23.9 KB, free 891.7 MB)
16:24:07.010 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.1 KB, free 891.6 MB)
16:24:07.011 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.237.1:5902 (size: 10.1 KB, free: 892.0 MB)
16:24:07.012 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
16:24:07.012 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(0))
16:24:07.012 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
16:24:07.014 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 35, localhost, executor driver, partition 0, ANY, 4726 bytes)
16:24:07.015 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 35)
16:24:07.019 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:24:07.019 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:07.065 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 35). 3216 bytes result sent to driver
16:24:07.066 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 35) in 52 ms on localhost (executor driver) (1/1)
16:24:07.066 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
16:24:07.068 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at DMReleaseCustomer.scala:75) finished in 0.053 s
16:24:07.069 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at DMReleaseCustomer.scala:75, took 6.703753 s
16:24:07.082 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:75
16:24:07.084 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (show at DMReleaseCustomer.scala:75) with 4 output partitions
16:24:07.084 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (show at DMReleaseCustomer.scala:75)
16:24:07.084 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
16:24:07.084 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:24:07.085 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:24:07.094 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 23.9 KB, free 891.6 MB)
16:24:07.097 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.1 KB, free 891.6 MB)
16:24:07.097 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.237.1:5902 (size: 10.1 KB, free: 892.0 MB)
16:24:07.098 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
16:24:07.099 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
16:24:07.099 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 4 tasks
16:24:07.100 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 6.0 (TID 36, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
16:24:07.100 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 37, localhost, executor driver, partition 1, ANY, 4726 bytes)
16:24:07.101 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 6.0 (TID 38, localhost, executor driver, partition 3, ANY, 4726 bytes)
16:24:07.101 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 6.0 (TID 39, localhost, executor driver, partition 4, ANY, 4726 bytes)
16:24:07.101 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Running task 1.0 in stage 6.0 (TID 36)
16:24:07.101 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 37)
16:24:07.102 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Running task 2.0 in stage 6.0 (TID 38)
16:24:07.102 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Running task 3.0 in stage 6.0 (TID 39)
16:24:07.112 INFO  [Executor task launch worker for task 37] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:24:07.112 INFO  [Executor task launch worker for task 37] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:07.113 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 32 blocks
16:24:07.113 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:07.113 INFO  [Executor task launch worker for task 38] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:24:07.115 INFO  [Executor task launch worker for task 38] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
16:24:07.123 INFO  [Executor task launch worker for task 39] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:24:07.125 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Finished task 1.0 in stage 6.0 (TID 36). 3087 bytes result sent to driver
16:24:07.126 INFO  [Executor task launch worker for task 39] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
16:24:07.144 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 6.0 (TID 36) in 44 ms on localhost (executor driver) (1/4)
16:24:07.151 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Finished task 2.0 in stage 6.0 (TID 38). 3218 bytes result sent to driver
16:24:07.163 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 6.0 (TID 38) in 62 ms on localhost (executor driver) (2/4)
16:24:07.165 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 37). 3217 bytes result sent to driver
16:24:07.167 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 37) in 67 ms on localhost (executor driver) (3/4)
16:24:07.181 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Finished task 3.0 in stage 6.0 (TID 39). 3207 bytes result sent to driver
16:24:07.183 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 6.0 (TID 39) in 82 ms on localhost (executor driver) (4/4)
16:24:07.183 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
16:24:07.184 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (show at DMReleaseCustomer.scala:75) finished in 0.085 s
16:24:07.184 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: show at DMReleaseCustomer.scala:75, took 0.101372 s
16:24:07.192 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:75
16:24:07.194 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 3 (show at DMReleaseCustomer.scala:75) with 4 output partitions
16:24:07.194 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (show at DMReleaseCustomer.scala:75)
16:24:07.194 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 8)
16:24:07.194 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:24:07.195 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:24:07.197 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 23.9 KB, free 891.6 MB)
16:24:07.199 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.0 KB, free 891.6 MB)
16:24:07.200 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.237.1:5902 (size: 10.0 KB, free: 891.9 MB)
16:24:07.200 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
16:24:07.204 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 9 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(5, 6, 7, 8))
16:24:07.204 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 4 tasks
16:24:07.205 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 40, localhost, executor driver, partition 7, PROCESS_LOCAL, 4726 bytes)
16:24:07.206 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 9.0 (TID 41, localhost, executor driver, partition 8, PROCESS_LOCAL, 4726 bytes)
16:24:07.206 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 42, localhost, executor driver, partition 5, ANY, 4726 bytes)
16:24:07.206 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 43, localhost, executor driver, partition 6, ANY, 4726 bytes)
16:24:07.207 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 40)
16:24:07.207 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Running task 3.0 in stage 9.0 (TID 41)
16:24:07.207 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 42)
16:24:07.207 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 43)
16:24:07.209 INFO  [Executor task launch worker for task 40] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 32 blocks
16:24:07.210 INFO  [Executor task launch worker for task 40] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:24:07.210 INFO  [Executor task launch worker for task 43] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:24:07.210 INFO  [Executor task launch worker for task 43] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:07.214 INFO  [Executor task launch worker for task 41] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 32 blocks
16:24:07.214 INFO  [Executor task launch worker for task 41] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:07.221 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Finished task 3.0 in stage 9.0 (TID 41). 3044 bytes result sent to driver
16:24:07.230 INFO  [Executor task launch worker for task 42] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:24:07.230 INFO  [Executor task launch worker for task 42] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:24:07.231 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 40). 3044 bytes result sent to driver
16:24:07.232 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 9.0 (TID 41) in 26 ms on localhost (executor driver) (1/4)
16:24:07.237 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 40) in 32 ms on localhost (executor driver) (2/4)
16:24:07.239 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 43). 3175 bytes result sent to driver
16:24:07.240 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 43) in 33 ms on localhost (executor driver) (3/4)
16:24:07.256 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 42). 3239 bytes result sent to driver
16:24:07.256 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 42) in 50 ms on localhost (executor driver) (4/4)
16:24:07.257 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
16:24:07.257 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 9 (show at DMReleaseCustomer.scala:75) finished in 0.052 s
16:24:07.258 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 3 finished: show at DMReleaseCustomer.scala:75, took 0.065312 s
16:24:07.364 ERROR [main] com.lzz.release.etl.release.dm.DMReleaseCustomer$ - cannot resolve '`user_count`' given input columns: [release_session, bdp_day, idcard, device_num, release_status, age, device_type, sources, age_range, gender, channels, ct, area_code];;
'Aggregate [sources#4, channels#5, device_type#3, age_range#55, 'user_count, 'total_count], [sources#4, channels#5, device_type#3, age_range#55, 'user_count, 'total_count, count(distinct 'user_count) AS user_count#349, count('total_count) AS total_count#351]
+- Filter (bdp_day#17 = 2019-09-24)
   +- Project [release_session#0, release_status#1, device_num#2, device_type#3, sources#4, channels#5, idcard#6, age#7, UDF:getAgeRange(cast(age#7 as string)) AS age_range#55, gender#8, area_code#9, ct#16L, bdp_day#17]
      +- SubqueryAlias dw_release_customer
         +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,idcard#6,age#7,gender#8,area_code#9,longitude#10,latitude#11,matter_id#12,model_code#13,model_version#14,aid#15,ct#16L,bdp_day#17] parquet

org.apache.spark.sql.AnalysisException: cannot resolve '`user_count`' given input columns: [release_session, bdp_day, idcard, device_num, release_status, age, device_type, sources, age_range, gender, channels, ct, area_code];;
'Aggregate [sources#4, channels#5, device_type#3, age_range#55, 'user_count, 'total_count], [sources#4, channels#5, device_type#3, age_range#55, 'user_count, 'total_count, count(distinct 'user_count) AS user_count#349, count('total_count) AS total_count#351]
+- Filter (bdp_day#17 = 2019-09-24)
   +- Project [release_session#0, release_status#1, device_num#2, device_type#3, sources#4, channels#5, idcard#6, age#7, UDF:getAgeRange(cast(age#7 as string)) AS age_range#55, gender#8, area_code#9, ct#16L, bdp_day#17]
      +- SubqueryAlias dw_release_customer
         +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,idcard#6,age#7,gender#8,area_code#9,longitude#10,latitude#11,matter_id#12,model_code#13,model_version#14,aid#15,ct#16L,bdp_day#17] parquet

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:64)
	at org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:224)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.handleReleaseJob(DMReleaseCustomer.scala:95)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$$anonfun$handleJobs$1.apply(DMReleaseCustomer.scala:131)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$$anonfun$handleJobs$1.apply(DMReleaseCustomer.scala:129)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.handleJobs(DMReleaseCustomer.scala:129)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.main(DMReleaseCustomer.scala:150)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer.main(DMReleaseCustomer.scala)
16:24:07.414 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@54e81b21{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:24:07.430 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
16:24:07.544 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:24:07.894 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:24:07.917 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
16:24:07.920 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:24:07.926 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:24:07.929 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:24:07.961 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:24:07.964 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-4284dcd9-028d-4e21-ab08-4d48bda626f8
16:26:37.281 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:26:38.063 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
16:26:38.257 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
16:26:38.258 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
16:26:38.259 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:26:38.259 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:26:38.261 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
16:26:40.535 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 6011.
16:26:40.645 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:26:40.708 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:26:40.731 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:26:40.733 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:26:40.766 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-b62f125d-a365-40f3-b3ce-b8cbbef4a86a
16:26:40.835 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
16:26:40.951 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:26:41.241 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @13046ms
16:26:41.418 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:26:41.469 INFO  [main] org.spark_project.jetty.server.Server - Started @13275ms
16:26:41.564 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@54e81b21{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:26:41.564 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:26:41.637 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/jobs,null,AVAILABLE,@Spark}
16:26:41.638 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/jobs/json,null,AVAILABLE,@Spark}
16:26:41.639 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e11ab3d{/jobs/job,null,AVAILABLE,@Spark}
16:26:41.640 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b43e173{/jobs/job/json,null,AVAILABLE,@Spark}
16:26:41.644 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545f80bf{/stages,null,AVAILABLE,@Spark}
16:26:41.646 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22fa55b2{/stages/json,null,AVAILABLE,@Spark}
16:26:41.647 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6594402a{/stages/stage,null,AVAILABLE,@Spark}
16:26:41.649 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c3f01f{/stages/stage/json,null,AVAILABLE,@Spark}
16:26:41.650 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/stages/pool,null,AVAILABLE,@Spark}
16:26:41.651 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/stages/pool/json,null,AVAILABLE,@Spark}
16:26:41.654 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/storage,null,AVAILABLE,@Spark}
16:26:41.655 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5298dead{/storage/json,null,AVAILABLE,@Spark}
16:26:41.656 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/storage/rdd,null,AVAILABLE,@Spark}
16:26:41.657 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/storage/rdd/json,null,AVAILABLE,@Spark}
16:26:41.658 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/environment,null,AVAILABLE,@Spark}
16:26:41.659 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b986cd{/environment/json,null,AVAILABLE,@Spark}
16:26:41.660 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41c62850{/executors,null,AVAILABLE,@Spark}
16:26:41.661 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@328572f0{/executors/json,null,AVAILABLE,@Spark}
16:26:41.662 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f460bb{/executors/threadDump,null,AVAILABLE,@Spark}
16:26:41.663 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d2a6eac{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:26:41.679 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c0f7678{/static,null,AVAILABLE,@Spark}
16:26:41.684 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f1249d{/,null,AVAILABLE,@Spark}
16:26:41.689 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a68df9{/api,null,AVAILABLE,@Spark}
16:26:41.693 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64711bf2{/jobs/job/kill,null,AVAILABLE,@Spark}
16:26:41.694 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c1e23ff{/stages/stage/kill,null,AVAILABLE,@Spark}
16:26:41.704 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
16:26:42.224 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:26:42.482 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 6032.
16:26:42.495 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:6032
16:26:42.529 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:26:42.613 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 6032, None)
16:26:42.639 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:6032 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 6032, None)
16:26:42.654 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 6032, None)
16:26:42.655 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 6032, None)
16:26:43.332 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c27d163{/metrics/json,null,AVAILABLE,@Spark}
16:26:43.707 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
16:26:43.770 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
16:26:43.771 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
16:26:43.810 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b08f438{/SQL,null,AVAILABLE,@Spark}
16:26:43.811 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b2f8ab6{/SQL/json,null,AVAILABLE,@Spark}
16:26:43.811 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65ae095c{/SQL/execution,null,AVAILABLE,@Spark}
16:26:43.812 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e140e59{/SQL/execution/json,null,AVAILABLE,@Spark}
16:26:43.814 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ae263bf{/static/sql,null,AVAILABLE,@Spark}
16:26:45.381 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:26:46.882 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:26:46.946 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:26:48.594 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:26:51.071 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:26:51.075 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:26:51.743 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:26:51.748 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:26:51.846 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:26:52.083 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:26:52.085 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
16:26:52.105 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
16:26:52.105 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
16:26:52.189 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:26:52.189 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:26:52.194 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:26:52.195 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:26:52.200 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:26:52.200 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:26:52.207 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
16:26:52.207 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
16:26:52.212 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:26:52.212 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:26:52.217 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
16:26:52.217 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
16:26:52.223 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
16:26:52.223 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
16:26:54.772 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/407a0422-3964-4025-b1ac-4e8cddf7e928_resources
16:26:54.788 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/407a0422-3964-4025-b1ac-4e8cddf7e928
16:26:54.824 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/407a0422-3964-4025-b1ac-4e8cddf7e928
16:26:54.837 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/407a0422-3964-4025-b1ac-4e8cddf7e928/_tmp_space.db
16:26:54.845 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
16:26:54.974 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:26:54.974 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
16:26:54.987 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:26:54.987 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:26:54.996 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:26:55.475 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/265e9ccb-1915-450c-b111-942e1a8e47ac_resources
16:26:55.487 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/265e9ccb-1915-450c-b111-942e1a8e47ac
16:26:55.491 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/265e9ccb-1915-450c-b111-942e1a8e47ac
16:26:55.504 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/265e9ccb-1915-450c-b111-942e1a8e47ac/_tmp_space.db
16:26:55.507 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
16:26:55.620 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:26:59.374 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
16:26:59.888 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
16:26:59.889 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
16:26:59.900 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:26:59.901 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:27:00.186 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:27:00.187 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:27:00.249 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:00.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:00.305 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:00.305 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:00.305 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:00.306 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:00.307 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:00.311 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:00.312 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:27:00.315 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:00.316 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:00.320 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:00.322 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:00.326 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:00.328 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:00.329 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:00.329 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:00.329 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:27:00.428 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:27:01.569 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:27:01.591 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:27:01.592 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
16:27:01.593 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:27:01.593 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:27:01.594 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:27:01.595 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: idcard
16:27:01.595 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: age
16:27:01.596 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: getAgeRange(age) as age_range
16:27:01.619 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: gender
16:27:01.620 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: area_code
16:27:01.620 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:27:01.621 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:27:01.630 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:27:01.630 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
16:27:03.120 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
16:27:03.120 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
16:27:03.358 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:27:03.359 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:27:03.405 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:27:03.406 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:27:03.450 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:03.451 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:03.452 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:03.452 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:03.452 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:03.453 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:03.455 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:03.456 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:03.457 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:27:03.457 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:03.458 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:03.458 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:03.458 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:03.459 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:03.459 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:03.459 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:03.459 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:27:03.460 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:27:03.548 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
16:27:03.548 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
16:27:03.570 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=dw_release tbl=dw_release_customer
16:27:03.579 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=dw_release tbl=dw_release_customer	
16:27:04.336 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#17),(bdp_day#17 = 2019-09-24)
16:27:04.339 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
16:27:04.341 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 9 more fields>
16:27:04.425 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: 
16:27:04.434 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
16:27:05.582 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 442.5068 ms
16:27:06.015 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 265.9 KB, free 897.3 MB)
16:27:06.274 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.7 KB, free 897.3 MB)
16:27:06.279 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:6032 (size: 22.7 KB, free: 897.6 MB)
16:27:06.306 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from persist at DMReleaseCustomer.scala:48
16:27:06.362 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
16:27:07.071 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 80.8602 ms
16:27:07.172 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:50
16:27:07.248 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DMReleaseCustomer.scala:50) with 1 output partitions
16:27:07.249 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at DMReleaseCustomer.scala:50)
16:27:07.249 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
16:27:07.290 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:27:07.354 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at show at DMReleaseCustomer.scala:50), which has no missing parents
16:27:07.568 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 26.9 KB, free 897.3 MB)
16:27:07.573 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.4 KB, free 897.3 MB)
16:27:07.574 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:6032 (size: 10.4 KB, free: 897.6 MB)
16:27:07.578 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
16:27:07.605 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at show at DMReleaseCustomer.scala:50) (first 15 tasks are for partitions Vector(0))
16:27:07.607 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
16:27:07.742 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5413 bytes)
16:27:07.808 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
16:27:08.038 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/bdp_day=2019-09-24/000000_0, range: 0-4194304, partition values: [2019-09-24]
16:27:09.616 WARN  [Executor task launch worker for task 0] org.apache.parquet.CorruptStatistics - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
16:27:13.769 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.memory.MemoryStore - Block rdd_2_0 stored as values in memory (estimated size 5.6 MB, free 891.7 MB)
16:27:13.771 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added rdd_2_0 in memory on 192.168.237.1:6032 (size: 5.6 MB, free: 892.0 MB)
16:27:13.922 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.196 ms
16:27:14.259 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 292.6913 ms
16:27:14.456 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - 1 block locks were not released by TID = 0:
[rdd_2_0]
16:27:14.490 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2973 bytes result sent to driver
16:27:14.510 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 6789 ms on localhost (executor driver) (1/1)
16:27:14.530 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
16:27:14.541 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (show at DMReleaseCustomer.scala:50) finished in 6.863 s
16:27:14.563 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DMReleaseCustomer.scala:50, took 7.390239 s
16:27:14.788 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:27:14.788 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:27:14.788 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:27:14.789 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: user_count
16:27:14.790 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: total_count
16:27:14.791 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:27:15.076 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.0844 ms
16:27:15.211 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 58.3368 ms
16:27:15.411 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 168.6017 ms
16:27:15.507 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 83.9196 ms
16:27:15.662 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:75
16:27:15.672 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 7 (show at DMReleaseCustomer.scala:75)
16:27:15.674 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (show at DMReleaseCustomer.scala:75)
16:27:15.675 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at DMReleaseCustomer.scala:75) with 1 output partitions
16:27:15.675 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at DMReleaseCustomer.scala:75)
16:27:15.675 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
16:27:15.681 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
16:27:15.687 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:27:15.715 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 45.7 KB, free 891.6 MB)
16:27:15.718 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 18.1 KB, free 891.6 MB)
16:27:15.723 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:6032 (size: 18.1 KB, free: 892.0 MB)
16:27:15.723 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
16:27:15.738 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(0, 1))
16:27:15.738 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 2 tasks
16:27:15.754 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5402 bytes)
16:27:15.755 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, ANY, 5402 bytes)
16:27:15.756 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
16:27:15.756 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 1.0 (TID 2)
16:27:15.781 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/bdp_day=2019-09-24/000000_0, range: 4194304-5574414, partition values: [2019-09-24]
16:27:15.784 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.BlockManager - Found block rdd_2_0 locally
16:27:15.810 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 25.0983 ms
16:27:15.891 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 29.6632 ms
16:27:15.943 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.1374 ms
16:27:15.972 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.384 ms
16:27:16.023 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 43.1995 ms
16:27:16.121 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.1883 ms
16:27:17.221 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2134 bytes result sent to driver
16:27:17.225 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 1486 ms on localhost (executor driver) (1/2)
16:27:17.425 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.memory.MemoryStore - Block rdd_2_1 stored as values in memory (estimated size 16.0 B, free 891.6 MB)
16:27:17.429 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added rdd_2_1 in memory on 192.168.237.1:6032 (size: 16.0 B, free: 892.0 MB)
16:27:17.452 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 1.0 (TID 2). 2600 bytes result sent to driver
16:27:17.455 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 1.0 (TID 2) in 1700 ms on localhost (executor driver) (2/2)
16:27:17.455 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
16:27:17.456 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (show at DMReleaseCustomer.scala:75) finished in 1.717 s
16:27:17.457 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:27:17.457 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
16:27:17.458 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
16:27:17.459 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:27:17.469 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:27:17.480 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 43.5 KB, free 891.6 MB)
16:27:17.574 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 14.5 KB, free 891.6 MB)
16:27:17.578 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.237.1:6032 (size: 14.5 KB, free: 891.9 MB)
16:27:17.579 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1004
16:27:17.581 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 32 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
16:27:17.581 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 32 tasks
16:27:17.584 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 4715 bytes)
16:27:17.584 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 4, localhost, executor driver, partition 1, ANY, 4715 bytes)
16:27:17.584 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 5, localhost, executor driver, partition 2, ANY, 4715 bytes)
16:27:17.585 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 6, localhost, executor driver, partition 3, ANY, 4715 bytes)
16:27:17.585 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 3)
16:27:17.586 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 4)
16:27:17.606 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 5)
16:27:17.617 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 6)
16:27:17.656 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:17.657 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:17.657 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:17.662 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:17.676 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 25 ms
16:27:17.684 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 40 ms
16:27:17.686 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 40 ms
16:27:17.704 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 43 ms
16:27:17.769 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 33.1519 ms
16:27:17.791 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.1446 ms
16:27:17.828 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.8348 ms
16:27:17.854 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.6351 ms
16:27:17.899 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 37.6052 ms
16:27:18.058 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 56
16:27:18.261 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.237.1:6032 in memory (size: 18.1 KB, free: 892.0 MB)
16:27:18.304 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 55
16:27:18.379 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 5). 2930 bytes result sent to driver
16:27:18.382 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 7, localhost, executor driver, partition 4, ANY, 4715 bytes)
16:27:18.382 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 4.0 in stage 2.0 (TID 7)
16:27:18.387 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:18.387 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:27:18.441 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 6). 2930 bytes result sent to driver
16:27:18.442 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 8, localhost, executor driver, partition 5, ANY, 4715 bytes)
16:27:18.443 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 6) in 858 ms on localhost (executor driver) (1/32)
16:27:18.456 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 4). 2930 bytes result sent to driver
16:27:18.456 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 2.0 (TID 9, localhost, executor driver, partition 6, ANY, 4715 bytes)
16:27:18.457 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 6.0 in stage 2.0 (TID 9)
16:27:18.458 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 4) in 874 ms on localhost (executor driver) (2/32)
16:27:18.462 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:18.462 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:27:18.591 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 3). 2930 bytes result sent to driver
16:27:18.594 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 2.0 (TID 10, localhost, executor driver, partition 7, ANY, 4715 bytes)
16:27:18.595 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 7.0 in stage 2.0 (TID 10)
16:27:18.595 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 3) in 1013 ms on localhost (executor driver) (3/32)
16:27:18.603 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:18.606 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
16:27:18.633 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 5) in 1049 ms on localhost (executor driver) (4/32)
16:27:18.642 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 5.0 in stage 2.0 (TID 8)
16:27:18.649 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:18.649 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:27:18.898 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 4.0 in stage 2.0 (TID 7). 2844 bytes result sent to driver
16:27:18.899 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 2.0 (TID 11, localhost, executor driver, partition 8, ANY, 4715 bytes)
16:27:18.899 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 7) in 518 ms on localhost (executor driver) (5/32)
16:27:18.899 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 8.0 in stage 2.0 (TID 11)
16:27:18.907 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:18.907 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:27:18.990 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 6.0 in stage 2.0 (TID 9). 2844 bytes result sent to driver
16:27:19.005 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 2.0 (TID 12, localhost, executor driver, partition 9, ANY, 4715 bytes)
16:27:19.007 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 2.0 (TID 9) in 551 ms on localhost (executor driver) (6/32)
16:27:19.012 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 9.0 in stage 2.0 (TID 12)
16:27:19.050 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:19.050 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:27:19.094 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 5.0 in stage 2.0 (TID 8). 2844 bytes result sent to driver
16:27:19.096 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 2.0 (TID 13, localhost, executor driver, partition 10, ANY, 4715 bytes)
16:27:19.097 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 10.0 in stage 2.0 (TID 13)
16:27:19.097 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 8) in 656 ms on localhost (executor driver) (7/32)
16:27:19.124 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:19.125 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:27:19.355 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 8.0 in stage 2.0 (TID 11). 2930 bytes result sent to driver
16:27:19.356 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 2.0 (TID 14, localhost, executor driver, partition 11, ANY, 4715 bytes)
16:27:19.357 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 2.0 (TID 11) in 459 ms on localhost (executor driver) (8/32)
16:27:19.357 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 11.0 in stage 2.0 (TID 14)
16:27:19.372 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:19.373 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:27:19.432 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 7.0 in stage 2.0 (TID 10). 2844 bytes result sent to driver
16:27:19.434 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 12.0 in stage 2.0 (TID 15, localhost, executor driver, partition 12, ANY, 4715 bytes)
16:27:19.434 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 2.0 (TID 10) in 840 ms on localhost (executor driver) (9/32)
16:27:19.434 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 12.0 in stage 2.0 (TID 15)
16:27:19.441 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:19.441 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
16:27:19.645 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 11.0 in stage 2.0 (TID 14). 2844 bytes result sent to driver
16:27:19.653 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 13.0 in stage 2.0 (TID 16, localhost, executor driver, partition 13, ANY, 4715 bytes)
16:27:19.654 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 13.0 in stage 2.0 (TID 16)
16:27:19.655 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 2.0 (TID 14) in 299 ms on localhost (executor driver) (10/32)
16:27:19.678 INFO  [Executor task launch worker for task 16] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:19.678 INFO  [Executor task launch worker for task 16] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:27:19.760 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 9.0 in stage 2.0 (TID 12). 2887 bytes result sent to driver
16:27:19.760 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 14.0 in stage 2.0 (TID 17, localhost, executor driver, partition 14, ANY, 4715 bytes)
16:27:19.762 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 2.0 (TID 12) in 769 ms on localhost (executor driver) (11/32)
16:27:19.763 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 14.0 in stage 2.0 (TID 17)
16:27:19.768 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:19.769 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:27:19.872 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 10.0 in stage 2.0 (TID 13). 2887 bytes result sent to driver
16:27:19.876 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 15.0 in stage 2.0 (TID 18, localhost, executor driver, partition 15, ANY, 4715 bytes)
16:27:19.891 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 2.0 (TID 13) in 796 ms on localhost (executor driver) (12/32)
16:27:19.894 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 15.0 in stage 2.0 (TID 18)
16:27:19.904 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:19.904 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:27:20.018 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 12.0 in stage 2.0 (TID 15). 2887 bytes result sent to driver
16:27:20.019 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 16.0 in stage 2.0 (TID 19, localhost, executor driver, partition 16, ANY, 4715 bytes)
16:27:20.022 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 16.0 in stage 2.0 (TID 19)
16:27:20.022 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 14.0 in stage 2.0 (TID 17). 2844 bytes result sent to driver
16:27:20.023 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 12.0 in stage 2.0 (TID 15) in 590 ms on localhost (executor driver) (13/32)
16:27:20.025 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 17.0 in stage 2.0 (TID 20, localhost, executor driver, partition 17, ANY, 4715 bytes)
16:27:20.026 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 14.0 in stage 2.0 (TID 17) in 266 ms on localhost (executor driver) (14/32)
16:27:20.027 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:20.028 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:27:20.033 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 17.0 in stage 2.0 (TID 20)
16:27:20.039 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:20.040 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:27:20.157 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 13.0 in stage 2.0 (TID 16). 2844 bytes result sent to driver
16:27:20.158 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 18.0 in stage 2.0 (TID 21, localhost, executor driver, partition 18, ANY, 4715 bytes)
16:27:20.159 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 13.0 in stage 2.0 (TID 16) in 506 ms on localhost (executor driver) (15/32)
16:27:20.160 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Running task 18.0 in stage 2.0 (TID 21)
16:27:20.169 INFO  [Executor task launch worker for task 21] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:20.171 INFO  [Executor task launch worker for task 21] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
16:27:20.187 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 16.0 in stage 2.0 (TID 19). 2887 bytes result sent to driver
16:27:20.188 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 19.0 in stage 2.0 (TID 22, localhost, executor driver, partition 19, ANY, 4715 bytes)
16:27:20.192 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 16.0 in stage 2.0 (TID 19) in 173 ms on localhost (executor driver) (16/32)
16:27:20.192 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Running task 19.0 in stage 2.0 (TID 22)
16:27:20.207 INFO  [Executor task launch worker for task 22] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:20.207 INFO  [Executor task launch worker for task 22] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:27:20.233 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 15.0 in stage 2.0 (TID 18). 2887 bytes result sent to driver
16:27:20.243 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 20.0 in stage 2.0 (TID 23, localhost, executor driver, partition 20, ANY, 4715 bytes)
16:27:20.244 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Running task 20.0 in stage 2.0 (TID 23)
16:27:20.244 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 15.0 in stage 2.0 (TID 18) in 368 ms on localhost (executor driver) (17/32)
16:27:20.258 INFO  [Executor task launch worker for task 23] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:20.270 INFO  [Executor task launch worker for task 23] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 12 ms
16:27:20.349 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Finished task 18.0 in stage 2.0 (TID 21). 2887 bytes result sent to driver
16:27:20.349 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 21.0 in stage 2.0 (TID 24, localhost, executor driver, partition 21, ANY, 4715 bytes)
16:27:20.350 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 18.0 in stage 2.0 (TID 21) in 192 ms on localhost (executor driver) (18/32)
16:27:20.351 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Running task 21.0 in stage 2.0 (TID 24)
16:27:20.355 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:20.355 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:27:20.456 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 17.0 in stage 2.0 (TID 20). 2844 bytes result sent to driver
16:27:20.459 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 22.0 in stage 2.0 (TID 25, localhost, executor driver, partition 22, ANY, 4715 bytes)
16:27:20.460 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Running task 22.0 in stage 2.0 (TID 25)
16:27:20.460 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 17.0 in stage 2.0 (TID 20) in 436 ms on localhost (executor driver) (19/32)
16:27:20.466 INFO  [Executor task launch worker for task 25] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:20.468 INFO  [Executor task launch worker for task 25] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
16:27:20.469 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Finished task 19.0 in stage 2.0 (TID 22). 2844 bytes result sent to driver
16:27:20.470 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 23.0 in stage 2.0 (TID 26, localhost, executor driver, partition 23, ANY, 4715 bytes)
16:27:20.474 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 19.0 in stage 2.0 (TID 22) in 286 ms on localhost (executor driver) (20/32)
16:27:20.476 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Running task 23.0 in stage 2.0 (TID 26)
16:27:20.485 INFO  [Executor task launch worker for task 26] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:20.486 INFO  [Executor task launch worker for task 26] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
16:27:20.535 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Finished task 20.0 in stage 2.0 (TID 23). 2844 bytes result sent to driver
16:27:20.538 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 24.0 in stage 2.0 (TID 27, localhost, executor driver, partition 24, ANY, 4715 bytes)
16:27:20.539 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Running task 24.0 in stage 2.0 (TID 27)
16:27:20.539 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 20.0 in stage 2.0 (TID 23) in 296 ms on localhost (executor driver) (21/32)
16:27:20.542 INFO  [Executor task launch worker for task 27] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:20.542 INFO  [Executor task launch worker for task 27] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:27:20.616 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Finished task 21.0 in stage 2.0 (TID 24). 2887 bytes result sent to driver
16:27:20.617 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 25.0 in stage 2.0 (TID 28, localhost, executor driver, partition 25, ANY, 4715 bytes)
16:27:20.618 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 21.0 in stage 2.0 (TID 24) in 269 ms on localhost (executor driver) (22/32)
16:27:20.619 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Running task 25.0 in stage 2.0 (TID 28)
16:27:20.624 INFO  [Executor task launch worker for task 28] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:20.624 INFO  [Executor task launch worker for task 28] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:27:20.628 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Finished task 23.0 in stage 2.0 (TID 26). 2887 bytes result sent to driver
16:27:20.629 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 26.0 in stage 2.0 (TID 29, localhost, executor driver, partition 26, ANY, 4715 bytes)
16:27:20.632 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 23.0 in stage 2.0 (TID 26) in 161 ms on localhost (executor driver) (23/32)
16:27:20.633 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Running task 26.0 in stage 2.0 (TID 29)
16:27:20.636 INFO  [Executor task launch worker for task 29] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:20.636 INFO  [Executor task launch worker for task 29] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:27:20.659 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Finished task 22.0 in stage 2.0 (TID 25). 2887 bytes result sent to driver
16:27:20.660 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 27.0 in stage 2.0 (TID 30, localhost, executor driver, partition 27, ANY, 4715 bytes)
16:27:20.661 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 22.0 in stage 2.0 (TID 25) in 202 ms on localhost (executor driver) (24/32)
16:27:20.662 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Running task 27.0 in stage 2.0 (TID 30)
16:27:20.671 INFO  [Executor task launch worker for task 30] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:20.671 INFO  [Executor task launch worker for task 30] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:27:20.828 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Finished task 24.0 in stage 2.0 (TID 27). 2887 bytes result sent to driver
16:27:20.829 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 28.0 in stage 2.0 (TID 31, localhost, executor driver, partition 28, ANY, 4715 bytes)
16:27:20.829 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 24.0 in stage 2.0 (TID 27) in 294 ms on localhost (executor driver) (25/32)
16:27:20.830 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Running task 28.0 in stage 2.0 (TID 31)
16:27:20.835 INFO  [Executor task launch worker for task 31] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:20.835 INFO  [Executor task launch worker for task 31] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:27:20.837 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Finished task 26.0 in stage 2.0 (TID 29). 2887 bytes result sent to driver
16:27:20.846 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 29.0 in stage 2.0 (TID 32, localhost, executor driver, partition 29, ANY, 4715 bytes)
16:27:20.856 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 26.0 in stage 2.0 (TID 29) in 228 ms on localhost (executor driver) (26/32)
16:27:20.857 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Running task 29.0 in stage 2.0 (TID 32)
16:27:20.860 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Finished task 27.0 in stage 2.0 (TID 30). 2844 bytes result sent to driver
16:27:20.862 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 30.0 in stage 2.0 (TID 33, localhost, executor driver, partition 30, ANY, 4715 bytes)
16:27:20.864 INFO  [Executor task launch worker for task 32] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:20.864 INFO  [Executor task launch worker for task 32] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:27:20.864 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 27.0 in stage 2.0 (TID 30) in 205 ms on localhost (executor driver) (27/32)
16:27:20.864 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Running task 30.0 in stage 2.0 (TID 33)
16:27:20.875 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:20.875 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:27:21.223 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Finished task 25.0 in stage 2.0 (TID 28). 2887 bytes result sent to driver
16:27:21.224 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 31.0 in stage 2.0 (TID 34, localhost, executor driver, partition 31, ANY, 4715 bytes)
16:27:21.228 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 25.0 in stage 2.0 (TID 28) in 611 ms on localhost (executor driver) (28/32)
16:27:21.234 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Running task 31.0 in stage 2.0 (TID 34)
16:27:21.238 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:27:21.238 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:27:21.241 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.237.1:6032 in memory (size: 10.4 KB, free: 892.0 MB)
16:27:21.249 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
16:27:21.322 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Finished task 30.0 in stage 2.0 (TID 33). 2930 bytes result sent to driver
16:27:21.323 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 30.0 in stage 2.0 (TID 33) in 461 ms on localhost (executor driver) (29/32)
16:27:21.347 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Finished task 28.0 in stage 2.0 (TID 31). 2887 bytes result sent to driver
16:27:21.349 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 28.0 in stage 2.0 (TID 31) in 521 ms on localhost (executor driver) (30/32)
16:27:21.354 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Finished task 31.0 in stage 2.0 (TID 34). 2844 bytes result sent to driver
16:27:21.358 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 31.0 in stage 2.0 (TID 34) in 134 ms on localhost (executor driver) (31/32)
16:27:21.413 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Finished task 29.0 in stage 2.0 (TID 32). 2930 bytes result sent to driver
16:27:21.417 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 29.0 in stage 2.0 (TID 32) in 571 ms on localhost (executor driver) (32/32)
16:27:21.417 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
16:27:21.421 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (show at DMReleaseCustomer.scala:75) finished in 3.839 s
16:27:21.422 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:27:21.422 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
16:27:21.422 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
16:27:21.422 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:27:21.423 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:27:21.426 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 23.9 KB, free 891.7 MB)
16:27:21.428 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.1 KB, free 891.6 MB)
16:27:21.428 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.237.1:6032 (size: 10.1 KB, free: 892.0 MB)
16:27:21.429 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
16:27:21.432 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(0))
16:27:21.432 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
16:27:21.436 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 35, localhost, executor driver, partition 0, ANY, 4726 bytes)
16:27:21.436 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 35)
16:27:21.440 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:27:21.441 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:27:21.540 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 35). 3173 bytes result sent to driver
16:27:21.542 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 35) in 108 ms on localhost (executor driver) (1/1)
16:27:21.542 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
16:27:21.547 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at DMReleaseCustomer.scala:75) finished in 0.112 s
16:27:21.548 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at DMReleaseCustomer.scala:75, took 5.886042 s
16:27:21.559 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:75
16:27:21.561 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (show at DMReleaseCustomer.scala:75) with 4 output partitions
16:27:21.561 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (show at DMReleaseCustomer.scala:75)
16:27:21.561 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
16:27:21.561 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:27:21.561 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:27:21.567 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 23.9 KB, free 891.6 MB)
16:27:21.568 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.1 KB, free 891.6 MB)
16:27:21.569 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.237.1:6032 (size: 10.1 KB, free: 892.0 MB)
16:27:21.570 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
16:27:21.570 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
16:27:21.570 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 4 tasks
16:27:21.572 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 6.0 (TID 36, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
16:27:21.572 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 37, localhost, executor driver, partition 1, ANY, 4726 bytes)
16:27:21.572 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 6.0 (TID 38, localhost, executor driver, partition 3, ANY, 4726 bytes)
16:27:21.573 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 6.0 (TID 39, localhost, executor driver, partition 4, ANY, 4726 bytes)
16:27:21.573 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 37)
16:27:21.573 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Running task 1.0 in stage 6.0 (TID 36)
16:27:21.573 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Running task 2.0 in stage 6.0 (TID 38)
16:27:21.573 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Running task 3.0 in stage 6.0 (TID 39)
16:27:21.578 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 32 blocks
16:27:21.579 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:27:21.579 INFO  [Executor task launch worker for task 39] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:27:21.579 INFO  [Executor task launch worker for task 39] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:27:21.584 INFO  [Executor task launch worker for task 38] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:27:21.584 INFO  [Executor task launch worker for task 38] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:27:21.587 INFO  [Executor task launch worker for task 37] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:27:21.587 INFO  [Executor task launch worker for task 37] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
16:27:21.599 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Finished task 1.0 in stage 6.0 (TID 36). 3044 bytes result sent to driver
16:27:21.601 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 6.0 (TID 36) in 30 ms on localhost (executor driver) (1/4)
16:27:21.606 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Finished task 2.0 in stage 6.0 (TID 38). 3175 bytes result sent to driver
16:27:21.611 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 6.0 (TID 38) in 38 ms on localhost (executor driver) (2/4)
16:27:21.620 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 37). 3174 bytes result sent to driver
16:27:21.621 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 37) in 49 ms on localhost (executor driver) (3/4)
16:27:21.660 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Finished task 3.0 in stage 6.0 (TID 39). 3207 bytes result sent to driver
16:27:21.663 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 6.0 (TID 39) in 91 ms on localhost (executor driver) (4/4)
16:27:21.663 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
16:27:21.664 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (show at DMReleaseCustomer.scala:75) finished in 0.093 s
16:27:21.671 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: show at DMReleaseCustomer.scala:75, took 0.111350 s
16:27:21.679 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:75
16:27:21.680 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 3 (show at DMReleaseCustomer.scala:75) with 4 output partitions
16:27:21.680 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (show at DMReleaseCustomer.scala:75)
16:27:21.681 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 8)
16:27:21.682 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:27:21.685 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:27:21.689 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 23.9 KB, free 891.6 MB)
16:27:21.691 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.1 KB, free 891.6 MB)
16:27:21.694 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.237.1:6032 (size: 10.1 KB, free: 891.9 MB)
16:27:21.696 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
16:27:21.697 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 9 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(5, 6, 7, 8))
16:27:21.697 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 4 tasks
16:27:21.700 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 40, localhost, executor driver, partition 7, PROCESS_LOCAL, 4726 bytes)
16:27:21.700 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 9.0 (TID 41, localhost, executor driver, partition 8, PROCESS_LOCAL, 4726 bytes)
16:27:21.701 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 42, localhost, executor driver, partition 5, ANY, 4726 bytes)
16:27:21.702 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 43, localhost, executor driver, partition 6, ANY, 4726 bytes)
16:27:21.702 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 40)
16:27:21.702 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Running task 3.0 in stage 9.0 (TID 41)
16:27:21.705 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 42)
16:27:21.705 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 43)
16:27:21.707 INFO  [Executor task launch worker for task 41] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 32 blocks
16:27:21.708 INFO  [Executor task launch worker for task 41] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:27:21.710 INFO  [Executor task launch worker for task 43] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:27:21.710 INFO  [Executor task launch worker for task 43] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:27:21.727 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Finished task 3.0 in stage 9.0 (TID 41). 3044 bytes result sent to driver
16:27:21.738 INFO  [Executor task launch worker for task 42] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:27:21.738 INFO  [Executor task launch worker for task 42] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:27:21.741 INFO  [Executor task launch worker for task 40] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 32 blocks
16:27:21.742 INFO  [Executor task launch worker for task 40] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:27:21.744 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 9.0 (TID 41) in 44 ms on localhost (executor driver) (1/4)
16:27:21.745 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 40). 3087 bytes result sent to driver
16:27:21.759 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 40) in 60 ms on localhost (executor driver) (2/4)
16:27:21.774 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 43). 3218 bytes result sent to driver
16:27:21.775 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 43) in 73 ms on localhost (executor driver) (3/4)
16:27:21.788 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 42). 3239 bytes result sent to driver
16:27:21.788 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 42) in 87 ms on localhost (executor driver) (4/4)
16:27:21.788 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
16:27:21.789 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 9 (show at DMReleaseCustomer.scala:75) finished in 0.092 s
16:27:21.790 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 3 finished: show at DMReleaseCustomer.scala:75, took 0.111858 s
16:27:21.847 ERROR [main] com.lzz.release.etl.release.dm.DMReleaseCustomer$ - cannot resolve '`user_count`' given input columns: [release_session, age_range, channels, age, gender, ct, device_num, idcard, bdp_day, device_type, sources, area_code, release_status];;
'Aggregate [sources#4, channels#5, device_type#3, age_range#55, gender#8, age_range#55], [sources#4, channels#5, device_type#3, age_range#55, gender#8, age_range#55, count(distinct 'user_count) AS user_count#349, count('total_count) AS total_count#351]
+- Filter (bdp_day#17 = 2019-09-24)
   +- Project [release_session#0, release_status#1, device_num#2, device_type#3, sources#4, channels#5, idcard#6, age#7, UDF:getAgeRange(cast(age#7 as string)) AS age_range#55, gender#8, area_code#9, ct#16L, bdp_day#17]
      +- SubqueryAlias dw_release_customer
         +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,idcard#6,age#7,gender#8,area_code#9,longitude#10,latitude#11,matter_id#12,model_code#13,model_version#14,aid#15,ct#16L,bdp_day#17] parquet

org.apache.spark.sql.AnalysisException: cannot resolve '`user_count`' given input columns: [release_session, age_range, channels, age, gender, ct, device_num, idcard, bdp_day, device_type, sources, area_code, release_status];;
'Aggregate [sources#4, channels#5, device_type#3, age_range#55, gender#8, age_range#55], [sources#4, channels#5, device_type#3, age_range#55, gender#8, age_range#55, count(distinct 'user_count) AS user_count#349, count('total_count) AS total_count#351]
+- Filter (bdp_day#17 = 2019-09-24)
   +- Project [release_session#0, release_status#1, device_num#2, device_type#3, sources#4, channels#5, idcard#6, age#7, UDF:getAgeRange(cast(age#7 as string)) AS age_range#55, gender#8, area_code#9, ct#16L, bdp_day#17]
      +- SubqueryAlias dw_release_customer
         +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,idcard#6,age#7,gender#8,area_code#9,longitude#10,latitude#11,matter_id#12,model_code#13,model_version#14,aid#15,ct#16L,bdp_day#17] parquet

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:64)
	at org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:224)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.handleReleaseJob(DMReleaseCustomer.scala:95)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$$anonfun$handleJobs$1.apply(DMReleaseCustomer.scala:131)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$$anonfun$handleJobs$1.apply(DMReleaseCustomer.scala:129)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.handleJobs(DMReleaseCustomer.scala:129)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.main(DMReleaseCustomer.scala:150)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer.main(DMReleaseCustomer.scala)
16:27:21.877 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@54e81b21{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:27:21.884 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
16:27:21.953 INFO  [dispatcher-event-loop-1] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:27:22.236 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:27:22.236 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
16:27:22.238 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:27:22.243 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:27:22.251 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:27:22.352 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:27:22.353 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-aab2b043-dc58-471b-8575-feda82c02389
16:29:52.096 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:29:52.668 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
16:29:52.696 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
16:29:52.696 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
16:29:52.697 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:29:52.698 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:29:52.699 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
16:29:54.287 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 6173.
16:29:54.311 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:29:54.335 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:29:54.339 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:29:54.341 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:29:54.351 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-37aab13d-5447-4c94-bf3c-1346c8ad9000
16:29:54.383 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
16:29:54.470 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:29:54.665 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @7862ms
16:29:54.853 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:29:54.892 INFO  [main] org.spark_project.jetty.server.Server - Started @8090ms
16:29:54.941 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@17f9344b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:29:54.942 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:29:54.976 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/jobs,null,AVAILABLE,@Spark}
16:29:54.977 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/jobs/json,null,AVAILABLE,@Spark}
16:29:54.977 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/jobs/job,null,AVAILABLE,@Spark}
16:29:54.978 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fa47fea{/jobs/job/json,null,AVAILABLE,@Spark}
16:29:54.979 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b43e173{/stages,null,AVAILABLE,@Spark}
16:29:54.980 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545f80bf{/stages/json,null,AVAILABLE,@Spark}
16:29:54.987 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22fa55b2{/stages/stage,null,AVAILABLE,@Spark}
16:29:54.991 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@405325cf{/stages/stage/json,null,AVAILABLE,@Spark}
16:29:54.992 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c3f01f{/stages/pool,null,AVAILABLE,@Spark}
16:29:54.994 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/stages/pool/json,null,AVAILABLE,@Spark}
16:29:54.995 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/storage,null,AVAILABLE,@Spark}
16:29:54.996 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/storage/json,null,AVAILABLE,@Spark}
16:29:55.000 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5298dead{/storage/rdd,null,AVAILABLE,@Spark}
16:29:55.001 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/storage/rdd/json,null,AVAILABLE,@Spark}
16:29:55.002 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/environment,null,AVAILABLE,@Spark}
16:29:55.003 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/environment/json,null,AVAILABLE,@Spark}
16:29:55.004 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b986cd{/executors,null,AVAILABLE,@Spark}
16:29:55.005 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41c62850{/executors/json,null,AVAILABLE,@Spark}
16:29:55.007 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@328572f0{/executors/threadDump,null,AVAILABLE,@Spark}
16:29:55.009 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f460bb{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:29:55.024 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d2a6eac{/static,null,AVAILABLE,@Spark}
16:29:55.026 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60222fd8{/,null,AVAILABLE,@Spark}
16:29:55.028 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f1249d{/api,null,AVAILABLE,@Spark}
16:29:55.032 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b1637e1{/jobs/job/kill,null,AVAILABLE,@Spark}
16:29:55.033 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64711bf2{/stages/stage/kill,null,AVAILABLE,@Spark}
16:29:55.036 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
16:29:55.271 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:29:55.367 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 6194.
16:29:55.368 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:6194
16:29:55.371 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:29:55.424 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 6194, None)
16:29:55.429 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:6194 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 6194, None)
16:29:55.436 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 6194, None)
16:29:55.437 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 6194, None)
16:29:55.911 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e4c3a38{/metrics/json,null,AVAILABLE,@Spark}
16:29:55.986 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
16:29:56.028 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
16:29:56.029 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
16:29:56.044 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48b22fd4{/SQL,null,AVAILABLE,@Spark}
16:29:56.045 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b08f438{/SQL/json,null,AVAILABLE,@Spark}
16:29:56.047 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73ab3aac{/SQL/execution,null,AVAILABLE,@Spark}
16:29:56.047 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65ae095c{/SQL/execution/json,null,AVAILABLE,@Spark}
16:29:56.049 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57aa341b{/static/sql,null,AVAILABLE,@Spark}
16:29:58.257 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:29:59.609 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:29:59.649 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:30:01.255 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:30:03.312 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:30:03.316 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:30:03.934 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:30:03.945 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:30:04.074 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:30:04.439 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:30:04.444 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
16:30:04.469 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
16:30:04.470 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
16:30:04.554 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:30:04.555 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:30:04.563 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:30:04.563 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:30:04.567 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:30:04.568 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:30:04.573 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
16:30:04.573 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
16:30:04.578 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:30:04.578 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:30:04.583 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
16:30:04.584 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
16:30:04.589 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
16:30:04.589 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
16:30:05.725 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/8268d092-318b-4e10-a6f3-75b9be847560_resources
16:30:05.739 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/8268d092-318b-4e10-a6f3-75b9be847560
16:30:05.743 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/8268d092-318b-4e10-a6f3-75b9be847560
16:30:05.747 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/8268d092-318b-4e10-a6f3-75b9be847560/_tmp_space.db
16:30:05.758 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
16:30:05.814 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:30:05.814 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
16:30:05.863 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:30:05.864 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:30:05.869 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:30:06.511 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/16448261-9ed0-4202-973c-0ab5d995e9fa_resources
16:30:06.519 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/16448261-9ed0-4202-973c-0ab5d995e9fa
16:30:06.526 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/16448261-9ed0-4202-973c-0ab5d995e9fa
16:30:06.818 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/16448261-9ed0-4202-973c-0ab5d995e9fa/_tmp_space.db
16:30:06.820 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
16:30:07.192 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:30:10.643 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
16:30:11.340 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
16:30:11.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
16:30:11.354 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:30:11.355 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:30:11.666 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:30:11.666 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:30:11.728 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:11.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:11.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:11.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:11.769 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:11.769 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:11.769 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:11.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:11.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:30:11.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:11.771 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:11.771 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:11.771 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:11.772 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:11.772 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:11.772 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:11.772 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:11.773 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:30:11.818 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:30:13.087 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:30:13.111 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:30:13.112 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
16:30:13.113 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:30:13.114 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:30:13.115 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:30:13.115 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: idcard
16:30:13.116 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: age
16:30:13.117 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: getAgeRange(age) as age_range
16:30:13.161 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: gender
16:30:13.161 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: area_code
16:30:13.162 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:30:13.163 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:30:13.180 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:30:13.181 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
16:30:14.255 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
16:30:14.255 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
16:30:14.263 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:30:14.263 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:30:14.302 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:30:14.303 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:30:14.340 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:14.341 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:14.342 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:14.342 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:14.343 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:14.343 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:14.343 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:14.344 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:14.345 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:30:14.345 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:14.345 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:14.346 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:14.346 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:14.346 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:14.346 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:14.346 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:14.347 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:30:14.347 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:30:14.436 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
16:30:14.436 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
16:30:14.461 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=dw_release tbl=dw_release_customer
16:30:14.461 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=dw_release tbl=dw_release_customer	
16:30:15.482 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#17),(bdp_day#17 = 2019-09-24)
16:30:15.488 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
16:30:15.492 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 9 more fields>
16:30:15.527 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: 
16:30:15.540 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
16:30:17.046 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 483.4282 ms
16:30:17.405 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 265.9 KB, free 897.3 MB)
16:30:17.665 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.7 KB, free 897.3 MB)
16:30:17.671 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:6194 (size: 22.7 KB, free: 897.6 MB)
16:30:17.718 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from persist at DMReleaseCustomer.scala:48
16:30:17.797 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
16:30:18.927 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 77.3143 ms
16:30:19.038 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:50
16:30:19.159 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DMReleaseCustomer.scala:50) with 1 output partitions
16:30:19.160 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at DMReleaseCustomer.scala:50)
16:30:19.161 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
16:30:19.168 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:30:19.215 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at show at DMReleaseCustomer.scala:50), which has no missing parents
16:30:19.513 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 26.9 KB, free 897.3 MB)
16:30:19.524 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.4 KB, free 897.3 MB)
16:30:19.526 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:6194 (size: 10.4 KB, free: 897.6 MB)
16:30:19.528 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
16:30:19.582 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at show at DMReleaseCustomer.scala:50) (first 15 tasks are for partitions Vector(0))
16:30:19.586 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
16:30:19.882 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5413 bytes)
16:30:19.924 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
16:30:20.333 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/bdp_day=2019-09-24/000000_0, range: 0-4194304, partition values: [2019-09-24]
16:30:22.241 WARN  [Executor task launch worker for task 0] org.apache.parquet.CorruptStatistics - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
16:30:25.382 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.memory.MemoryStore - Block rdd_2_0 stored as values in memory (estimated size 5.6 MB, free 891.7 MB)
16:30:25.388 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added rdd_2_0 in memory on 192.168.237.1:6194 (size: 5.6 MB, free: 892.0 MB)
16:30:25.579 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 26.8064 ms
16:30:25.953 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 338.1355 ms
16:30:26.077 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - 1 block locks were not released by TID = 0:
[rdd_2_0]
16:30:26.118 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2973 bytes result sent to driver
16:30:26.146 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 6346 ms on localhost (executor driver) (1/1)
16:30:26.165 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
16:30:26.182 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (show at DMReleaseCustomer.scala:50) finished in 6.463 s
16:30:26.206 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DMReleaseCustomer.scala:50, took 7.167155 s
16:30:26.622 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:30:26.623 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:30:26.624 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:30:26.624 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: user_count
16:30:26.624 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: total_count
16:30:26.625 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:30:27.022 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.6833 ms
16:30:27.271 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 117.1371 ms
16:30:27.638 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 292.5702 ms
16:30:27.747 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 96.8554 ms
16:30:27.969 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:75
16:30:27.975 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 7 (show at DMReleaseCustomer.scala:75)
16:30:27.986 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (show at DMReleaseCustomer.scala:75)
16:30:27.986 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at DMReleaseCustomer.scala:75) with 1 output partitions
16:30:27.986 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at DMReleaseCustomer.scala:75)
16:30:27.987 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
16:30:28.000 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
16:30:28.007 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:30:28.035 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 45.7 KB, free 891.6 MB)
16:30:28.040 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 18.1 KB, free 891.6 MB)
16:30:28.043 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:6194 (size: 18.1 KB, free: 892.0 MB)
16:30:28.044 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
16:30:28.062 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(0, 1))
16:30:28.062 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 2 tasks
16:30:28.075 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5402 bytes)
16:30:28.077 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, ANY, 5402 bytes)
16:30:28.077 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
16:30:28.095 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.BlockManager - Found block rdd_2_0 locally
16:30:28.100 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 1.0 (TID 2)
16:30:28.116 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/bdp_day=2019-09-24/000000_0, range: 4194304-5574414, partition values: [2019-09-24]
16:30:28.131 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 34.9837 ms
16:30:28.196 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.5119 ms
16:30:28.389 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.574 ms
16:30:28.429 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 28.8976 ms
16:30:28.469 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 27.3831 ms
16:30:28.577 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.9388 ms
16:30:29.695 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2177 bytes result sent to driver
16:30:29.699 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 1634 ms on localhost (executor driver) (1/2)
16:30:29.982 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.memory.MemoryStore - Block rdd_2_1 stored as values in memory (estimated size 16.0 B, free 891.6 MB)
16:30:29.984 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added rdd_2_1 in memory on 192.168.237.1:6194 (size: 16.0 B, free: 892.0 MB)
16:30:29.997 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 1.0 (TID 2). 2643 bytes result sent to driver
16:30:29.999 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 1.0 (TID 2) in 1923 ms on localhost (executor driver) (2/2)
16:30:30.000 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (show at DMReleaseCustomer.scala:75) finished in 1.935 s
16:30:30.003 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
16:30:30.004 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:30:30.005 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
16:30:30.006 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
16:30:30.007 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:30:30.029 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:30:30.048 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 43.5 KB, free 891.6 MB)
16:30:30.132 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 14.5 KB, free 891.6 MB)
16:30:30.133 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.237.1:6194 (size: 14.5 KB, free: 891.9 MB)
16:30:30.134 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1004
16:30:30.135 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 32 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
16:30:30.135 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 32 tasks
16:30:30.141 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 4715 bytes)
16:30:30.141 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 4, localhost, executor driver, partition 1, ANY, 4715 bytes)
16:30:30.142 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 5, localhost, executor driver, partition 2, ANY, 4715 bytes)
16:30:30.142 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 6, localhost, executor driver, partition 3, ANY, 4715 bytes)
16:30:30.143 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 3)
16:30:30.144 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 4)
16:30:30.166 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 5)
16:30:30.166 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 6)
16:30:30.355 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:30.359 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:30.360 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:30.360 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:30.394 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 62 ms
16:30:30.394 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 62 ms
16:30:30.394 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 61 ms
16:30:30.394 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 62 ms
16:30:30.455 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.1053 ms
16:30:30.477 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.1693 ms
16:30:30.512 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.4189 ms
16:30:30.549 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 27.0568 ms
16:30:30.569 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.9983 ms
16:30:30.997 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 5). 2844 bytes result sent to driver
16:30:30.998 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 7, localhost, executor driver, partition 4, ANY, 4715 bytes)
16:30:30.999 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 4.0 in stage 2.0 (TID 7)
16:30:31.005 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:31.005 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:30:31.173 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 5) in 1032 ms on localhost (executor driver) (1/32)
16:30:31.192 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 4). 2844 bytes result sent to driver
16:30:31.193 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 8, localhost, executor driver, partition 5, ANY, 4715 bytes)
16:30:31.194 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 4) in 1053 ms on localhost (executor driver) (2/32)
16:30:31.199 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 5.0 in stage 2.0 (TID 8)
16:30:31.211 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 3). 2887 bytes result sent to driver
16:30:31.212 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:31.212 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:31.216 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 2.0 (TID 9, localhost, executor driver, partition 6, ANY, 4715 bytes)
16:30:31.217 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 6.0 in stage 2.0 (TID 9)
16:30:31.217 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 3) in 1078 ms on localhost (executor driver) (3/32)
16:30:31.249 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:31.249 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:30:31.340 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 6). 2844 bytes result sent to driver
16:30:31.340 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 2.0 (TID 10, localhost, executor driver, partition 7, ANY, 4715 bytes)
16:30:31.341 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 7.0 in stage 2.0 (TID 10)
16:30:31.345 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 6) in 1203 ms on localhost (executor driver) (4/32)
16:30:31.350 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:31.351 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:31.434 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 4.0 in stage 2.0 (TID 7). 2844 bytes result sent to driver
16:30:31.436 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 2.0 (TID 11, localhost, executor driver, partition 8, ANY, 4715 bytes)
16:30:31.436 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 7) in 438 ms on localhost (executor driver) (5/32)
16:30:31.437 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 8.0 in stage 2.0 (TID 11)
16:30:31.445 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:31.446 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:31.501 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 6.0 in stage 2.0 (TID 9). 2844 bytes result sent to driver
16:30:31.539 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 2.0 (TID 12, localhost, executor driver, partition 9, ANY, 4715 bytes)
16:30:31.540 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 2.0 (TID 9) in 325 ms on localhost (executor driver) (6/32)
16:30:31.540 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 9.0 in stage 2.0 (TID 12)
16:30:31.550 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:31.550 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:30:31.563 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 5.0 in stage 2.0 (TID 8). 2844 bytes result sent to driver
16:30:31.565 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 2.0 (TID 13, localhost, executor driver, partition 10, ANY, 4715 bytes)
16:30:31.565 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 10.0 in stage 2.0 (TID 13)
16:30:31.565 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 8) in 372 ms on localhost (executor driver) (7/32)
16:30:31.570 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:31.570 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:30:31.603 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 7.0 in stage 2.0 (TID 10). 2844 bytes result sent to driver
16:30:31.608 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 2.0 (TID 14, localhost, executor driver, partition 11, ANY, 4715 bytes)
16:30:31.615 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 2.0 (TID 10) in 275 ms on localhost (executor driver) (8/32)
16:30:31.618 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 11.0 in stage 2.0 (TID 14)
16:30:31.687 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:31.687 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:30:31.786 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 8.0 in stage 2.0 (TID 11). 2844 bytes result sent to driver
16:30:31.787 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 12.0 in stage 2.0 (TID 15, localhost, executor driver, partition 12, ANY, 4715 bytes)
16:30:31.788 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 12.0 in stage 2.0 (TID 15)
16:30:31.788 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 2.0 (TID 11) in 353 ms on localhost (executor driver) (9/32)
16:30:31.792 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:31.798 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
16:30:31.907 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 10.0 in stage 2.0 (TID 13). 2844 bytes result sent to driver
16:30:31.911 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 13.0 in stage 2.0 (TID 16, localhost, executor driver, partition 13, ANY, 4715 bytes)
16:30:31.911 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 13.0 in stage 2.0 (TID 16)
16:30:31.911 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 2.0 (TID 13) in 347 ms on localhost (executor driver) (10/32)
16:30:31.934 INFO  [Executor task launch worker for task 16] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:31.935 INFO  [Executor task launch worker for task 16] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:30:32.040 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 11.0 in stage 2.0 (TID 14). 2887 bytes result sent to driver
16:30:32.046 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 14.0 in stage 2.0 (TID 17, localhost, executor driver, partition 14, ANY, 4715 bytes)
16:30:32.047 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 14.0 in stage 2.0 (TID 17)
16:30:32.047 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 2.0 (TID 14) in 439 ms on localhost (executor driver) (11/32)
16:30:32.052 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:32.052 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:30:32.160 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 13.0 in stage 2.0 (TID 16). 2844 bytes result sent to driver
16:30:32.162 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 15.0 in stage 2.0 (TID 18, localhost, executor driver, partition 15, ANY, 4715 bytes)
16:30:32.163 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 13.0 in stage 2.0 (TID 16) in 253 ms on localhost (executor driver) (12/32)
16:30:32.169 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 15.0 in stage 2.0 (TID 18)
16:30:32.181 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:32.181 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:30:32.181 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 12.0 in stage 2.0 (TID 15). 2887 bytes result sent to driver
16:30:32.182 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 16.0 in stage 2.0 (TID 19, localhost, executor driver, partition 16, ANY, 4715 bytes)
16:30:32.183 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 12.0 in stage 2.0 (TID 15) in 396 ms on localhost (executor driver) (13/32)
16:30:32.184 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 16.0 in stage 2.0 (TID 19)
16:30:32.193 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:32.194 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:32.275 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 9.0 in stage 2.0 (TID 12). 2844 bytes result sent to driver
16:30:32.276 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 17.0 in stage 2.0 (TID 20, localhost, executor driver, partition 17, ANY, 4715 bytes)
16:30:32.277 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 17.0 in stage 2.0 (TID 20)
16:30:32.280 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 2.0 (TID 12) in 741 ms on localhost (executor driver) (14/32)
16:30:32.282 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:32.282 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:30:32.350 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 14.0 in stage 2.0 (TID 17). 2844 bytes result sent to driver
16:30:32.351 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 18.0 in stage 2.0 (TID 21, localhost, executor driver, partition 18, ANY, 4715 bytes)
16:30:32.352 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 14.0 in stage 2.0 (TID 17) in 307 ms on localhost (executor driver) (15/32)
16:30:32.353 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Running task 18.0 in stage 2.0 (TID 21)
16:30:32.360 INFO  [Executor task launch worker for task 21] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:32.361 INFO  [Executor task launch worker for task 21] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:33.005 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 16.0 in stage 2.0 (TID 19). 2887 bytes result sent to driver
16:30:33.010 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 19.0 in stage 2.0 (TID 22, localhost, executor driver, partition 19, ANY, 4715 bytes)
16:30:33.012 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 16.0 in stage 2.0 (TID 19) in 830 ms on localhost (executor driver) (16/32)
16:30:33.012 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Running task 19.0 in stage 2.0 (TID 22)
16:30:33.012 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Finished task 18.0 in stage 2.0 (TID 21). 2887 bytes result sent to driver
16:30:33.026 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 15.0 in stage 2.0 (TID 18). 2844 bytes result sent to driver
16:30:33.027 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 17.0 in stage 2.0 (TID 20). 2887 bytes result sent to driver
16:30:33.027 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 20.0 in stage 2.0 (TID 23, localhost, executor driver, partition 20, ANY, 4715 bytes)
16:30:33.028 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 21.0 in stage 2.0 (TID 24, localhost, executor driver, partition 21, ANY, 4715 bytes)
16:30:33.028 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 22.0 in stage 2.0 (TID 25, localhost, executor driver, partition 22, ANY, 4715 bytes)
16:30:33.028 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 18.0 in stage 2.0 (TID 21) in 677 ms on localhost (executor driver) (17/32)
16:30:33.028 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Running task 20.0 in stage 2.0 (TID 23)
16:30:33.028 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 15.0 in stage 2.0 (TID 18) in 866 ms on localhost (executor driver) (18/32)
16:30:33.029 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 17.0 in stage 2.0 (TID 20) in 753 ms on localhost (executor driver) (19/32)
16:30:33.029 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Running task 21.0 in stage 2.0 (TID 24)
16:30:33.034 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Running task 22.0 in stage 2.0 (TID 25)
16:30:33.039 INFO  [Executor task launch worker for task 23] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:33.039 INFO  [Executor task launch worker for task 22] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:33.039 INFO  [Executor task launch worker for task 23] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:33.039 INFO  [Executor task launch worker for task 22] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:33.040 INFO  [Executor task launch worker for task 25] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:33.041 INFO  [Executor task launch worker for task 25] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:33.041 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:33.041 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:30:33.256 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Finished task 22.0 in stage 2.0 (TID 25). 2844 bytes result sent to driver
16:30:33.264 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 23.0 in stage 2.0 (TID 26, localhost, executor driver, partition 23, ANY, 4715 bytes)
16:30:33.266 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 22.0 in stage 2.0 (TID 25) in 238 ms on localhost (executor driver) (20/32)
16:30:33.267 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Running task 23.0 in stage 2.0 (TID 26)
16:30:33.271 INFO  [Executor task launch worker for task 26] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:33.271 INFO  [Executor task launch worker for task 26] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:30:33.314 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Finished task 19.0 in stage 2.0 (TID 22). 2844 bytes result sent to driver
16:30:33.337 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 24.0 in stage 2.0 (TID 27, localhost, executor driver, partition 24, ANY, 4715 bytes)
16:30:33.340 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 19.0 in stage 2.0 (TID 22) in 331 ms on localhost (executor driver) (21/32)
16:30:33.341 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Running task 24.0 in stage 2.0 (TID 27)
16:30:33.349 INFO  [Executor task launch worker for task 27] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:33.349 INFO  [Executor task launch worker for task 27] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:33.444 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Finished task 20.0 in stage 2.0 (TID 23). 2887 bytes result sent to driver
16:30:33.446 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 25.0 in stage 2.0 (TID 28, localhost, executor driver, partition 25, ANY, 4715 bytes)
16:30:33.446 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 20.0 in stage 2.0 (TID 23) in 419 ms on localhost (executor driver) (22/32)
16:30:33.447 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Running task 25.0 in stage 2.0 (TID 28)
16:30:33.504 INFO  [Executor task launch worker for task 28] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:33.506 INFO  [Executor task launch worker for task 28] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
16:30:33.625 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Finished task 21.0 in stage 2.0 (TID 24). 2887 bytes result sent to driver
16:30:33.628 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 26.0 in stage 2.0 (TID 29, localhost, executor driver, partition 26, ANY, 4715 bytes)
16:30:33.628 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 21.0 in stage 2.0 (TID 24) in 600 ms on localhost (executor driver) (23/32)
16:30:33.629 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Running task 26.0 in stage 2.0 (TID 29)
16:30:33.638 INFO  [Executor task launch worker for task 29] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:33.639 INFO  [Executor task launch worker for task 29] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:33.959 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Finished task 26.0 in stage 2.0 (TID 29). 2844 bytes result sent to driver
16:30:33.960 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 27.0 in stage 2.0 (TID 30, localhost, executor driver, partition 27, ANY, 4715 bytes)
16:30:33.961 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 26.0 in stage 2.0 (TID 29) in 335 ms on localhost (executor driver) (24/32)
16:30:33.961 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Running task 27.0 in stage 2.0 (TID 30)
16:30:33.966 INFO  [Executor task launch worker for task 30] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:33.966 INFO  [Executor task launch worker for task 30] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:30:34.236 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Finished task 24.0 in stage 2.0 (TID 27). 2887 bytes result sent to driver
16:30:34.239 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 28.0 in stage 2.0 (TID 31, localhost, executor driver, partition 28, ANY, 4715 bytes)
16:30:34.246 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 24.0 in stage 2.0 (TID 27) in 909 ms on localhost (executor driver) (25/32)
16:30:34.247 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Running task 28.0 in stage 2.0 (TID 31)
16:30:34.254 INFO  [Executor task launch worker for task 31] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:34.255 INFO  [Executor task launch worker for task 31] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:34.270 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Finished task 23.0 in stage 2.0 (TID 26). 2844 bytes result sent to driver
16:30:34.278 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 29.0 in stage 2.0 (TID 32, localhost, executor driver, partition 29, ANY, 4715 bytes)
16:30:34.281 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 23.0 in stage 2.0 (TID 26) in 1023 ms on localhost (executor driver) (26/32)
16:30:34.285 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Running task 29.0 in stage 2.0 (TID 32)
16:30:34.299 INFO  [Executor task launch worker for task 32] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:34.302 INFO  [Executor task launch worker for task 32] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
16:30:34.305 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Finished task 27.0 in stage 2.0 (TID 30). 2844 bytes result sent to driver
16:30:34.311 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 30.0 in stage 2.0 (TID 33, localhost, executor driver, partition 30, ANY, 4715 bytes)
16:30:34.313 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 27.0 in stage 2.0 (TID 30) in 352 ms on localhost (executor driver) (27/32)
16:30:34.315 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Running task 30.0 in stage 2.0 (TID 33)
16:30:34.324 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:34.324 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:30:34.667 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Finished task 25.0 in stage 2.0 (TID 28). 2887 bytes result sent to driver
16:30:34.676 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 31.0 in stage 2.0 (TID 34, localhost, executor driver, partition 31, ANY, 4715 bytes)
16:30:34.679 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 25.0 in stage 2.0 (TID 28) in 1234 ms on localhost (executor driver) (28/32)
16:30:34.680 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Running task 31.0 in stage 2.0 (TID 34)
16:30:34.725 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:30:34.725 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:34.771 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Finished task 28.0 in stage 2.0 (TID 31). 2844 bytes result sent to driver
16:30:34.781 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 28.0 in stage 2.0 (TID 31) in 542 ms on localhost (executor driver) (29/32)
16:30:34.878 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Finished task 30.0 in stage 2.0 (TID 33). 2887 bytes result sent to driver
16:30:34.886 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 30.0 in stage 2.0 (TID 33) in 576 ms on localhost (executor driver) (30/32)
16:30:34.936 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Finished task 29.0 in stage 2.0 (TID 32). 2887 bytes result sent to driver
16:30:34.937 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 29.0 in stage 2.0 (TID 32) in 662 ms on localhost (executor driver) (31/32)
16:30:35.061 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Finished task 31.0 in stage 2.0 (TID 34). 2844 bytes result sent to driver
16:30:35.062 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 31.0 in stage 2.0 (TID 34) in 394 ms on localhost (executor driver) (32/32)
16:30:35.063 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
16:30:35.064 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (show at DMReleaseCustomer.scala:75) finished in 4.924 s
16:30:35.064 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:30:35.064 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
16:30:35.064 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
16:30:35.064 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:30:35.065 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:30:35.071 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 23.9 KB, free 891.6 MB)
16:30:35.074 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.1 KB, free 891.5 MB)
16:30:35.075 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.237.1:6194 (size: 10.1 KB, free: 891.9 MB)
16:30:35.080 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
16:30:35.082 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(0))
16:30:35.085 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
16:30:35.091 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 35, localhost, executor driver, partition 0, ANY, 4726 bytes)
16:30:35.094 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 35)
16:30:35.688 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:30:35.688 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:36.525 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 35). 3173 bytes result sent to driver
16:30:36.525 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 35) in 1438 ms on localhost (executor driver) (1/1)
16:30:36.526 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
16:30:36.526 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at DMReleaseCustomer.scala:75) finished in 1.441 s
16:30:36.527 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at DMReleaseCustomer.scala:75, took 8.557723 s
16:30:37.662 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:75
16:30:37.663 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (show at DMReleaseCustomer.scala:75) with 4 output partitions
16:30:37.663 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (show at DMReleaseCustomer.scala:75)
16:30:37.663 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
16:30:37.663 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:30:37.664 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:30:37.667 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 23.9 KB, free 891.5 MB)
16:30:37.669 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.1 KB, free 891.5 MB)
16:30:37.670 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.237.1:6194 (size: 10.1 KB, free: 891.9 MB)
16:30:37.671 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
16:30:37.672 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
16:30:37.672 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 4 tasks
16:30:37.673 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 6.0 (TID 36, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
16:30:37.673 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 37, localhost, executor driver, partition 1, ANY, 4726 bytes)
16:30:37.674 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 6.0 (TID 38, localhost, executor driver, partition 3, ANY, 4726 bytes)
16:30:37.674 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 6.0 (TID 39, localhost, executor driver, partition 4, ANY, 4726 bytes)
16:30:37.674 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 37)
16:30:37.674 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Running task 2.0 in stage 6.0 (TID 38)
16:30:37.674 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Running task 1.0 in stage 6.0 (TID 36)
16:30:37.674 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Running task 3.0 in stage 6.0 (TID 39)
16:30:37.677 INFO  [Executor task launch worker for task 37] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:30:37.677 INFO  [Executor task launch worker for task 39] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:30:37.678 INFO  [Executor task launch worker for task 37] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:37.678 INFO  [Executor task launch worker for task 38] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:30:37.678 INFO  [Executor task launch worker for task 39] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:37.679 INFO  [Executor task launch worker for task 38] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:37.680 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 32 blocks
16:30:37.681 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:37.684 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Finished task 1.0 in stage 6.0 (TID 36). 3087 bytes result sent to driver
16:30:37.689 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 6.0 (TID 36) in 16 ms on localhost (executor driver) (1/4)
16:30:37.863 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.237.1:6194 in memory (size: 18.1 KB, free: 891.9 MB)
16:30:37.866 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 37). 3174 bytes result sent to driver
16:30:37.867 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 37) in 194 ms on localhost (executor driver) (2/4)
16:30:37.868 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Finished task 2.0 in stage 6.0 (TID 38). 3175 bytes result sent to driver
16:30:37.869 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 6.0 (TID 38) in 195 ms on localhost (executor driver) (3/4)
16:30:37.876 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Finished task 3.0 in stage 6.0 (TID 39). 3250 bytes result sent to driver
16:30:37.881 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 6.0 (TID 39) in 207 ms on localhost (executor driver) (4/4)
16:30:37.881 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
16:30:37.882 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (show at DMReleaseCustomer.scala:75) finished in 0.208 s
16:30:37.883 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: show at DMReleaseCustomer.scala:75, took 0.220723 s
16:30:37.913 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:75
16:30:37.915 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 3 (show at DMReleaseCustomer.scala:75) with 4 output partitions
16:30:37.915 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (show at DMReleaseCustomer.scala:75)
16:30:37.916 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 8)
16:30:37.916 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:30:37.918 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:30:37.921 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 56
16:30:37.922 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 55
16:30:37.924 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 23.9 KB, free 891.5 MB)
16:30:37.930 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.1 KB, free 891.5 MB)
16:30:37.930 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.237.1:6194 (size: 10.1 KB, free: 891.9 MB)
16:30:37.931 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
16:30:37.933 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 9 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(5, 6, 7, 8))
16:30:37.933 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 4 tasks
16:30:37.934 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 40, localhost, executor driver, partition 7, PROCESS_LOCAL, 4726 bytes)
16:30:37.935 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 9.0 (TID 41, localhost, executor driver, partition 8, PROCESS_LOCAL, 4726 bytes)
16:30:37.936 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 42, localhost, executor driver, partition 5, ANY, 4726 bytes)
16:30:37.936 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 43, localhost, executor driver, partition 6, ANY, 4726 bytes)
16:30:37.936 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 40)
16:30:37.936 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Running task 3.0 in stage 9.0 (TID 41)
16:30:37.937 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 42)
16:30:37.937 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 43)
16:30:37.941 INFO  [Executor task launch worker for task 42] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:30:37.945 INFO  [Executor task launch worker for task 43] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:30:37.945 INFO  [Executor task launch worker for task 42] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
16:30:37.946 INFO  [Executor task launch worker for task 43] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:37.946 INFO  [Executor task launch worker for task 40] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 32 blocks
16:30:37.947 INFO  [Executor task launch worker for task 41] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 32 blocks
16:30:37.948 INFO  [Executor task launch worker for task 40] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
16:30:37.948 INFO  [Executor task launch worker for task 41] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:30:37.973 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Finished task 3.0 in stage 9.0 (TID 41). 3087 bytes result sent to driver
16:30:37.977 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 40). 3044 bytes result sent to driver
16:30:37.985 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 9.0 (TID 41) in 50 ms on localhost (executor driver) (1/4)
16:30:37.986 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 40) in 52 ms on localhost (executor driver) (2/4)
16:30:38.016 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 42). 3196 bytes result sent to driver
16:30:38.017 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 42) in 82 ms on localhost (executor driver) (3/4)
16:30:38.021 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 43). 3175 bytes result sent to driver
16:30:38.021 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 43) in 85 ms on localhost (executor driver) (4/4)
16:30:38.022 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 9 (show at DMReleaseCustomer.scala:75) finished in 0.088 s
16:30:38.022 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
16:30:38.023 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 3 finished: show at DMReleaseCustomer.scala:75, took 0.108058 s
16:30:38.217 ERROR [main] com.lzz.release.etl.release.dm.DMReleaseCustomer$ - cannot resolve '`user_count`' given input columns: [device_num, release_session, gender, channels, area_code, bdp_day, ct, sources, age, age_range, idcard, release_status, device_type];;
'Aggregate [sources#4, channels#5, device_type#3, age_range#55, gender#8, area_code#9], [sources#4, channels#5, device_type#3, age_range#55, gender#8, area_code#9, count(distinct 'user_count) AS user_count#349, count('total_count) AS total_count#351]
+- Filter (bdp_day#17 = 2019-09-24)
   +- Project [release_session#0, release_status#1, device_num#2, device_type#3, sources#4, channels#5, idcard#6, age#7, UDF:getAgeRange(cast(age#7 as string)) AS age_range#55, gender#8, area_code#9, ct#16L, bdp_day#17]
      +- SubqueryAlias dw_release_customer
         +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,idcard#6,age#7,gender#8,area_code#9,longitude#10,latitude#11,matter_id#12,model_code#13,model_version#14,aid#15,ct#16L,bdp_day#17] parquet

org.apache.spark.sql.AnalysisException: cannot resolve '`user_count`' given input columns: [device_num, release_session, gender, channels, area_code, bdp_day, ct, sources, age, age_range, idcard, release_status, device_type];;
'Aggregate [sources#4, channels#5, device_type#3, age_range#55, gender#8, area_code#9], [sources#4, channels#5, device_type#3, age_range#55, gender#8, area_code#9, count(distinct 'user_count) AS user_count#349, count('total_count) AS total_count#351]
+- Filter (bdp_day#17 = 2019-09-24)
   +- Project [release_session#0, release_status#1, device_num#2, device_type#3, sources#4, channels#5, idcard#6, age#7, UDF:getAgeRange(cast(age#7 as string)) AS age_range#55, gender#8, area_code#9, ct#16L, bdp_day#17]
      +- SubqueryAlias dw_release_customer
         +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,idcard#6,age#7,gender#8,area_code#9,longitude#10,latitude#11,matter_id#12,model_code#13,model_version#14,aid#15,ct#16L,bdp_day#17] parquet

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:64)
	at org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:224)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.handleReleaseJob(DMReleaseCustomer.scala:95)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$$anonfun$handleJobs$1.apply(DMReleaseCustomer.scala:131)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$$anonfun$handleJobs$1.apply(DMReleaseCustomer.scala:129)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.handleJobs(DMReleaseCustomer.scala:129)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer$.main(DMReleaseCustomer.scala:150)
	at com.lzz.release.etl.release.dm.DMReleaseCustomer.main(DMReleaseCustomer.scala)
16:30:38.295 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@17f9344b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:30:38.302 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
16:30:38.405 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:30:38.654 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:30:38.678 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
16:30:38.678 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:30:38.683 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:30:38.689 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:30:38.712 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:30:38.714 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-a8210f56-51f3-4977-99af-41c17eed56ac
16:32:27.746 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:32:28.540 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
16:32:28.668 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
16:32:28.670 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
16:32:28.674 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:32:28.675 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:32:28.676 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
16:32:30.936 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 6361.
16:32:31.015 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:32:31.071 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:32:31.086 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:32:31.087 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:32:31.108 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-a0294062-86fd-4bec-9b1b-f047f8f13b5d
16:32:31.182 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
16:32:31.303 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:32:31.610 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @12517ms
16:32:31.841 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:32:31.871 INFO  [main] org.spark_project.jetty.server.Server - Started @12780ms
16:32:31.924 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@59dbf3c2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:32:31.925 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:32:31.975 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74518890{/jobs,null,AVAILABLE,@Spark}
16:32:31.976 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ea1fda{/jobs/json,null,AVAILABLE,@Spark}
16:32:31.977 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@420745d7{/jobs/job,null,AVAILABLE,@Spark}
16:32:31.978 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2392212b{/jobs/job/json,null,AVAILABLE,@Spark}
16:32:31.979 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28f8e165{/stages,null,AVAILABLE,@Spark}
16:32:31.983 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66f66866{/stages/json,null,AVAILABLE,@Spark}
16:32:31.985 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d666b41{/stages/stage,null,AVAILABLE,@Spark}
16:32:31.987 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e1162e7{/stages/stage/json,null,AVAILABLE,@Spark}
16:32:31.988 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2f1700{/stages/pool,null,AVAILABLE,@Spark}
16:32:31.989 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/stages/pool/json,null,AVAILABLE,@Spark}
16:32:31.990 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/storage,null,AVAILABLE,@Spark}
16:32:31.991 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/storage/json,null,AVAILABLE,@Spark}
16:32:31.993 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@553f3b6e{/storage/rdd,null,AVAILABLE,@Spark}
16:32:31.995 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/storage/rdd/json,null,AVAILABLE,@Spark}
16:32:31.996 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/environment,null,AVAILABLE,@Spark}
16:32:31.997 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/environment/json,null,AVAILABLE,@Spark}
16:32:32.000 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bb7cce7{/executors,null,AVAILABLE,@Spark}
16:32:32.001 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b530eb9{/executors/json,null,AVAILABLE,@Spark}
16:32:32.005 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@678040b3{/executors/threadDump,null,AVAILABLE,@Spark}
16:32:32.007 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64a1923a{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:32:32.020 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18ca3c62{/static,null,AVAILABLE,@Spark}
16:32:32.021 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53bf7094{/,null,AVAILABLE,@Spark}
16:32:32.023 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@710b30ef{/api,null,AVAILABLE,@Spark}
16:32:32.024 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18151a14{/jobs/job/kill,null,AVAILABLE,@Spark}
16:32:32.025 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@169da7f2{/stages/stage/kill,null,AVAILABLE,@Spark}
16:32:32.028 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
16:32:32.464 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:32:32.585 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 6385.
16:32:32.586 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:6385
16:32:32.595 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:32:32.689 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 6385, None)
16:32:32.696 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:6385 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 6385, None)
16:32:32.726 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 6385, None)
16:32:32.727 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 6385, None)
16:32:33.110 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@293cde83{/metrics/json,null,AVAILABLE,@Spark}
16:32:33.451 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
16:32:33.517 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
16:32:33.518 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
16:32:33.557 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b7b3edb{/SQL,null,AVAILABLE,@Spark}
16:32:33.558 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@108531c2{/SQL/json,null,AVAILABLE,@Spark}
16:32:33.559 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b2f8ab6{/SQL/execution,null,AVAILABLE,@Spark}
16:32:33.560 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2788d0fe{/SQL/execution/json,null,AVAILABLE,@Spark}
16:32:33.562 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49a71302{/static/sql,null,AVAILABLE,@Spark}
16:32:35.330 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:32:37.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:32:37.135 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:32:38.782 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:32:41.040 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:32:41.044 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:32:41.619 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:32:41.624 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:32:41.737 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:32:42.086 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:32:42.092 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
16:32:42.142 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
16:32:42.143 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
16:32:42.252 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:32:42.252 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:32:42.263 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:32:42.263 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:32:42.274 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:32:42.274 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:32:42.283 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
16:32:42.283 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
16:32:42.293 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:32:42.294 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:32:42.304 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
16:32:42.305 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
16:32:42.315 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
16:32:42.316 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
16:32:43.954 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/5fab1997-a2ed-42a7-99e0-7613e651a5d6_resources
16:32:43.969 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/5fab1997-a2ed-42a7-99e0-7613e651a5d6
16:32:43.972 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/5fab1997-a2ed-42a7-99e0-7613e651a5d6
16:32:43.989 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/5fab1997-a2ed-42a7-99e0-7613e651a5d6/_tmp_space.db
16:32:43.993 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
16:32:44.091 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:32:44.091 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
16:32:44.101 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:32:44.102 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:32:44.108 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:32:44.571 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/990e79a0-9b5e-4be7-9f59-0176483ed05c_resources
16:32:44.592 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/990e79a0-9b5e-4be7-9f59-0176483ed05c
16:32:44.595 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/990e79a0-9b5e-4be7-9f59-0176483ed05c
16:32:44.609 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/990e79a0-9b5e-4be7-9f59-0176483ed05c/_tmp_space.db
16:32:44.613 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
16:32:44.770 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:32:47.871 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
16:32:48.415 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
16:32:48.416 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
16:32:48.431 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:32:48.431 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:32:48.728 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:32:48.728 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:32:48.811 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:48.894 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:48.895 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:48.895 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:48.896 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:48.896 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:48.897 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:48.897 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:48.897 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:32:48.898 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:48.899 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:48.899 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:48.899 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:48.900 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:48.900 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:48.901 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:48.906 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:48.907 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:32:49.015 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:32:50.011 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:32:50.032 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:32:50.032 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
16:32:50.033 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:32:50.034 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:32:50.035 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:32:50.039 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: idcard
16:32:50.040 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: age
16:32:50.041 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: getAgeRange(age) as age_range
16:32:50.065 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: gender
16:32:50.065 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: area_code
16:32:50.068 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:32:50.069 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:32:50.076 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:32:50.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
16:32:50.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
16:32:50.854 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
16:32:50.865 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:32:50.865 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:32:50.906 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:32:50.906 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:32:50.948 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:50.948 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:50.951 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:50.951 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:50.951 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:50.952 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:50.952 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:50.952 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:50.953 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:32:50.953 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:50.953 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:50.955 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:50.956 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:50.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:50.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:50.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:50.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:32:50.958 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:32:51.025 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
16:32:51.026 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
16:32:51.037 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=dw_release tbl=dw_release_customer
16:32:51.037 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=dw_release tbl=dw_release_customer	
16:32:51.814 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#17),(bdp_day#17 = 2019-09-24)
16:32:51.818 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
16:32:51.821 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 9 more fields>
16:32:51.858 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: 
16:32:51.883 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
16:32:53.102 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 452.1949 ms
16:32:53.427 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 265.9 KB, free 897.3 MB)
16:32:53.580 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.7 KB, free 897.3 MB)
16:32:53.584 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:6385 (size: 22.7 KB, free: 897.6 MB)
16:32:53.606 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from persist at DMReleaseCustomer.scala:48
16:32:53.661 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
16:32:54.576 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 117.6996 ms
16:32:54.679 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:50
16:32:54.738 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DMReleaseCustomer.scala:50) with 1 output partitions
16:32:54.739 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at DMReleaseCustomer.scala:50)
16:32:54.740 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
16:32:54.746 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:32:54.777 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at show at DMReleaseCustomer.scala:50), which has no missing parents
16:32:54.989 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 26.9 KB, free 897.3 MB)
16:32:54.994 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.4 KB, free 897.3 MB)
16:32:54.994 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:6385 (size: 10.4 KB, free: 897.6 MB)
16:32:54.995 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
16:32:55.017 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at show at DMReleaseCustomer.scala:50) (first 15 tasks are for partitions Vector(0))
16:32:55.018 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
16:32:55.201 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5413 bytes)
16:32:55.263 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
16:32:55.482 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/bdp_day=2019-09-24/000000_0, range: 0-4194304, partition values: [2019-09-24]
16:32:56.982 WARN  [Executor task launch worker for task 0] org.apache.parquet.CorruptStatistics - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
16:33:00.927 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.memory.MemoryStore - Block rdd_2_0 stored as values in memory (estimated size 5.6 MB, free 891.7 MB)
16:33:00.930 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added rdd_2_0 in memory on 192.168.237.1:6385 (size: 5.6 MB, free: 892.0 MB)
16:33:01.060 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.3148 ms
16:33:01.333 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 254.307 ms
16:33:01.431 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - 1 block locks were not released by TID = 0:
[rdd_2_0]
16:33:01.462 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3016 bytes result sent to driver
16:33:01.487 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 6308 ms on localhost (executor driver) (1/1)
16:33:01.498 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
16:33:01.504 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (show at DMReleaseCustomer.scala:50) finished in 6.409 s
16:33:01.517 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DMReleaseCustomer.scala:50, took 6.837592 s
16:33:01.786 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:33:01.787 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:33:01.787 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:33:01.787 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: user_count
16:33:01.788 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: total_count
16:33:01.788 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:33:02.057 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.814 ms
16:33:02.203 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 74.4592 ms
16:33:02.391 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 157.4648 ms
16:33:02.484 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 85.5782 ms
16:33:02.614 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:75
16:33:02.620 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 7 (show at DMReleaseCustomer.scala:75)
16:33:02.623 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (show at DMReleaseCustomer.scala:75)
16:33:02.624 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at DMReleaseCustomer.scala:75) with 1 output partitions
16:33:02.625 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at DMReleaseCustomer.scala:75)
16:33:02.625 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
16:33:02.638 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
16:33:02.643 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:33:02.664 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 45.7 KB, free 891.6 MB)
16:33:02.675 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 18.0 KB, free 891.6 MB)
16:33:02.687 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:6385 (size: 18.0 KB, free: 892.0 MB)
16:33:02.688 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
16:33:02.692 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(0, 1))
16:33:02.693 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 2 tasks
16:33:02.701 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5402 bytes)
16:33:02.703 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, ANY, 5402 bytes)
16:33:02.703 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
16:33:02.709 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 1.0 (TID 2)
16:33:02.738 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.BlockManager - Found block rdd_2_0 locally
16:33:02.739 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/bdp_day=2019-09-24/000000_0, range: 4194304-5574414, partition values: [2019-09-24]
16:33:02.767 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 28.1711 ms
16:33:02.822 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.2452 ms
16:33:02.925 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 42.4066 ms
16:33:02.964 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.0214 ms
16:33:03.070 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 98.9029 ms
16:33:03.141 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.5684 ms
16:33:04.081 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2177 bytes result sent to driver
16:33:04.085 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 1390 ms on localhost (executor driver) (1/2)
16:33:04.372 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.memory.MemoryStore - Block rdd_2_1 stored as values in memory (estimated size 16.0 B, free 891.6 MB)
16:33:04.374 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added rdd_2_1 in memory on 192.168.237.1:6385 (size: 16.0 B, free: 892.0 MB)
16:33:04.388 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 1.0 (TID 2). 2600 bytes result sent to driver
16:33:04.391 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 1.0 (TID 2) in 1689 ms on localhost (executor driver) (2/2)
16:33:04.392 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
16:33:04.393 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (show at DMReleaseCustomer.scala:75) finished in 1.698 s
16:33:04.394 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:33:04.395 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
16:33:04.398 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
16:33:04.399 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:33:04.406 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:33:04.519 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 43.5 KB, free 891.6 MB)
16:33:04.595 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 14.5 KB, free 891.6 MB)
16:33:04.597 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.237.1:6385 (size: 14.5 KB, free: 891.9 MB)
16:33:04.598 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1004
16:33:04.599 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 32 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
16:33:04.599 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 32 tasks
16:33:04.611 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 4715 bytes)
16:33:04.611 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 4, localhost, executor driver, partition 1, ANY, 4715 bytes)
16:33:04.612 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 5, localhost, executor driver, partition 2, ANY, 4715 bytes)
16:33:04.613 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 6, localhost, executor driver, partition 3, ANY, 4715 bytes)
16:33:04.614 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 3)
16:33:04.614 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 4)
16:33:04.661 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 5)
16:33:04.675 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 6)
16:33:04.713 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:04.713 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:04.713 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:04.713 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:04.716 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 12 ms
16:33:04.716 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 11 ms
16:33:04.716 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 11 ms
16:33:04.716 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 11 ms
16:33:04.755 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.7895 ms
16:33:04.771 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.9337 ms
16:33:04.804 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.5111 ms
16:33:05.022 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 211.4275 ms
16:33:05.039 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.0732 ms
16:33:07.182 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.237.1:6385 in memory (size: 18.0 KB, free: 892.0 MB)
16:33:07.243 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 55
16:33:07.243 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 56
16:33:07.246 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.237.1:6385 in memory (size: 10.4 KB, free: 892.0 MB)
16:33:07.247 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
16:33:07.250 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 6). 2887 bytes result sent to driver
16:33:07.251 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 7, localhost, executor driver, partition 4, ANY, 4715 bytes)
16:33:07.252 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 4.0 in stage 2.0 (TID 7)
16:33:07.259 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:07.259 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:07.308 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 5). 2930 bytes result sent to driver
16:33:07.309 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 8, localhost, executor driver, partition 5, ANY, 4715 bytes)
16:33:07.313 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 5.0 in stage 2.0 (TID 8)
16:33:07.314 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 5) in 2702 ms on localhost (executor driver) (1/32)
16:33:07.318 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:07.319 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:07.518 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 6) in 2905 ms on localhost (executor driver) (2/32)
16:33:07.549 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 3). 2887 bytes result sent to driver
16:33:07.551 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 2.0 (TID 9, localhost, executor driver, partition 6, ANY, 4715 bytes)
16:33:07.552 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 6.0 in stage 2.0 (TID 9)
16:33:07.552 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 3) in 2947 ms on localhost (executor driver) (3/32)
16:33:07.557 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:07.562 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
16:33:07.565 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 4). 2887 bytes result sent to driver
16:33:07.569 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 2.0 (TID 10, localhost, executor driver, partition 7, ANY, 4715 bytes)
16:33:07.571 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 4) in 2960 ms on localhost (executor driver) (4/32)
16:33:07.572 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 7.0 in stage 2.0 (TID 10)
16:33:07.579 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:07.582 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
16:33:07.860 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 6.0 in stage 2.0 (TID 9). 2844 bytes result sent to driver
16:33:07.861 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 2.0 (TID 11, localhost, executor driver, partition 8, ANY, 4715 bytes)
16:33:07.862 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 2.0 (TID 9) in 311 ms on localhost (executor driver) (5/32)
16:33:07.862 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 8.0 in stage 2.0 (TID 11)
16:33:07.866 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:07.867 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:07.943 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 5.0 in stage 2.0 (TID 8). 2844 bytes result sent to driver
16:33:07.946 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 2.0 (TID 12, localhost, executor driver, partition 9, ANY, 4715 bytes)
16:33:07.948 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 8) in 639 ms on localhost (executor driver) (6/32)
16:33:07.960 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 9.0 in stage 2.0 (TID 12)
16:33:07.966 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:07.966 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:08.035 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 7.0 in stage 2.0 (TID 10). 2844 bytes result sent to driver
16:33:08.037 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 2.0 (TID 13, localhost, executor driver, partition 10, ANY, 4715 bytes)
16:33:08.038 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 10.0 in stage 2.0 (TID 13)
16:33:08.039 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 2.0 (TID 10) in 470 ms on localhost (executor driver) (7/32)
16:33:08.050 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:08.050 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:08.204 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 4.0 in stage 2.0 (TID 7). 2887 bytes result sent to driver
16:33:08.205 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 2.0 (TID 14, localhost, executor driver, partition 11, ANY, 4715 bytes)
16:33:08.205 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 11.0 in stage 2.0 (TID 14)
16:33:08.205 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 7) in 954 ms on localhost (executor driver) (8/32)
16:33:08.216 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 8.0 in stage 2.0 (TID 11). 2844 bytes result sent to driver
16:33:08.217 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 12.0 in stage 2.0 (TID 15, localhost, executor driver, partition 12, ANY, 4715 bytes)
16:33:08.218 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:08.218 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:08.218 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 12.0 in stage 2.0 (TID 15)
16:33:08.225 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:08.225 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:08.236 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 2.0 (TID 11) in 375 ms on localhost (executor driver) (9/32)
16:33:08.395 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 9.0 in stage 2.0 (TID 12). 2887 bytes result sent to driver
16:33:08.396 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 13.0 in stage 2.0 (TID 16, localhost, executor driver, partition 13, ANY, 4715 bytes)
16:33:08.397 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 2.0 (TID 12) in 451 ms on localhost (executor driver) (10/32)
16:33:08.407 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 13.0 in stage 2.0 (TID 16)
16:33:08.412 INFO  [Executor task launch worker for task 16] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:08.412 INFO  [Executor task launch worker for task 16] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:08.474 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 10.0 in stage 2.0 (TID 13). 2887 bytes result sent to driver
16:33:08.492 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 14.0 in stage 2.0 (TID 17, localhost, executor driver, partition 14, ANY, 4715 bytes)
16:33:08.507 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 2.0 (TID 13) in 471 ms on localhost (executor driver) (11/32)
16:33:08.510 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 14.0 in stage 2.0 (TID 17)
16:33:08.527 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:08.527 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:08.690 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 11.0 in stage 2.0 (TID 14). 2844 bytes result sent to driver
16:33:08.691 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 15.0 in stage 2.0 (TID 18, localhost, executor driver, partition 15, ANY, 4715 bytes)
16:33:08.692 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 2.0 (TID 14) in 488 ms on localhost (executor driver) (12/32)
16:33:08.692 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 15.0 in stage 2.0 (TID 18)
16:33:08.697 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:08.698 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:08.792 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 14.0 in stage 2.0 (TID 17). 2844 bytes result sent to driver
16:33:08.798 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 16.0 in stage 2.0 (TID 19, localhost, executor driver, partition 16, ANY, 4715 bytes)
16:33:08.800 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 14.0 in stage 2.0 (TID 17) in 308 ms on localhost (executor driver) (13/32)
16:33:08.801 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 16.0 in stage 2.0 (TID 19)
16:33:08.807 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:08.807 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:08.840 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 13.0 in stage 2.0 (TID 16). 2887 bytes result sent to driver
16:33:08.841 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 17.0 in stage 2.0 (TID 20, localhost, executor driver, partition 17, ANY, 4715 bytes)
16:33:08.841 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 17.0 in stage 2.0 (TID 20)
16:33:08.844 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:08.845 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 13.0 in stage 2.0 (TID 16) in 449 ms on localhost (executor driver) (14/32)
16:33:08.854 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 10 ms
16:33:08.929 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 12.0 in stage 2.0 (TID 15). 2844 bytes result sent to driver
16:33:08.938 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 18.0 in stage 2.0 (TID 21, localhost, executor driver, partition 18, ANY, 4715 bytes)
16:33:08.939 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 12.0 in stage 2.0 (TID 15) in 722 ms on localhost (executor driver) (15/32)
16:33:08.940 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Running task 18.0 in stage 2.0 (TID 21)
16:33:08.954 INFO  [Executor task launch worker for task 21] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:08.956 INFO  [Executor task launch worker for task 21] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:09.075 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 16.0 in stage 2.0 (TID 19). 2887 bytes result sent to driver
16:33:09.078 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 19.0 in stage 2.0 (TID 22, localhost, executor driver, partition 19, ANY, 4715 bytes)
16:33:09.078 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 16.0 in stage 2.0 (TID 19) in 281 ms on localhost (executor driver) (16/32)
16:33:09.078 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Running task 19.0 in stage 2.0 (TID 22)
16:33:09.082 INFO  [Executor task launch worker for task 22] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:09.083 INFO  [Executor task launch worker for task 22] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:09.095 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 17.0 in stage 2.0 (TID 20). 2887 bytes result sent to driver
16:33:09.098 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 20.0 in stage 2.0 (TID 23, localhost, executor driver, partition 20, ANY, 4715 bytes)
16:33:09.100 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Running task 20.0 in stage 2.0 (TID 23)
16:33:09.100 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 17.0 in stage 2.0 (TID 20) in 259 ms on localhost (executor driver) (17/32)
16:33:09.103 INFO  [Executor task launch worker for task 23] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:09.103 INFO  [Executor task launch worker for task 23] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:09.126 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Finished task 18.0 in stage 2.0 (TID 21). 2844 bytes result sent to driver
16:33:09.128 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 21.0 in stage 2.0 (TID 24, localhost, executor driver, partition 21, ANY, 4715 bytes)
16:33:09.128 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 18.0 in stage 2.0 (TID 21) in 191 ms on localhost (executor driver) (18/32)
16:33:09.128 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Running task 21.0 in stage 2.0 (TID 24)
16:33:09.132 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:09.132 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:09.153 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 15.0 in stage 2.0 (TID 18). 2844 bytes result sent to driver
16:33:09.157 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 22.0 in stage 2.0 (TID 25, localhost, executor driver, partition 22, ANY, 4715 bytes)
16:33:09.160 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Running task 22.0 in stage 2.0 (TID 25)
16:33:09.160 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 15.0 in stage 2.0 (TID 18) in 469 ms on localhost (executor driver) (19/32)
16:33:09.164 INFO  [Executor task launch worker for task 25] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:09.164 INFO  [Executor task launch worker for task 25] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:09.283 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Finished task 19.0 in stage 2.0 (TID 22). 2844 bytes result sent to driver
16:33:09.284 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 23.0 in stage 2.0 (TID 26, localhost, executor driver, partition 23, ANY, 4715 bytes)
16:33:09.287 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 19.0 in stage 2.0 (TID 22) in 209 ms on localhost (executor driver) (20/32)
16:33:09.287 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Running task 23.0 in stage 2.0 (TID 26)
16:33:09.290 INFO  [Executor task launch worker for task 26] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:09.291 INFO  [Executor task launch worker for task 26] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:09.381 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Finished task 21.0 in stage 2.0 (TID 24). 2844 bytes result sent to driver
16:33:09.382 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 24.0 in stage 2.0 (TID 27, localhost, executor driver, partition 24, ANY, 4715 bytes)
16:33:09.383 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Running task 24.0 in stage 2.0 (TID 27)
16:33:09.383 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 21.0 in stage 2.0 (TID 24) in 256 ms on localhost (executor driver) (21/32)
16:33:09.392 INFO  [Executor task launch worker for task 27] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:09.392 INFO  [Executor task launch worker for task 27] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:09.529 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Finished task 20.0 in stage 2.0 (TID 23). 2844 bytes result sent to driver
16:33:09.537 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 25.0 in stage 2.0 (TID 28, localhost, executor driver, partition 25, ANY, 4715 bytes)
16:33:09.537 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Finished task 22.0 in stage 2.0 (TID 25). 2844 bytes result sent to driver
16:33:09.538 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 26.0 in stage 2.0 (TID 29, localhost, executor driver, partition 26, ANY, 4715 bytes)
16:33:09.555 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Running task 25.0 in stage 2.0 (TID 28)
16:33:09.556 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 20.0 in stage 2.0 (TID 23) in 459 ms on localhost (executor driver) (22/32)
16:33:09.558 INFO  [Executor task launch worker for task 28] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:09.558 INFO  [Executor task launch worker for task 28] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:09.580 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Running task 26.0 in stage 2.0 (TID 29)
16:33:09.584 INFO  [Executor task launch worker for task 29] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:09.584 INFO  [Executor task launch worker for task 29] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:09.619 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 22.0 in stage 2.0 (TID 25) in 465 ms on localhost (executor driver) (23/32)
16:33:09.764 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Finished task 23.0 in stage 2.0 (TID 26). 2844 bytes result sent to driver
16:33:09.765 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 27.0 in stage 2.0 (TID 30, localhost, executor driver, partition 27, ANY, 4715 bytes)
16:33:09.766 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Running task 27.0 in stage 2.0 (TID 30)
16:33:09.769 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 23.0 in stage 2.0 (TID 26) in 486 ms on localhost (executor driver) (24/32)
16:33:09.771 INFO  [Executor task launch worker for task 30] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:09.771 INFO  [Executor task launch worker for task 30] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:09.774 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Finished task 25.0 in stage 2.0 (TID 28). 2887 bytes result sent to driver
16:33:09.775 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 28.0 in stage 2.0 (TID 31, localhost, executor driver, partition 28, ANY, 4715 bytes)
16:33:09.779 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 25.0 in stage 2.0 (TID 28) in 245 ms on localhost (executor driver) (25/32)
16:33:09.892 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Finished task 26.0 in stage 2.0 (TID 29). 2844 bytes result sent to driver
16:33:09.893 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 29.0 in stage 2.0 (TID 32, localhost, executor driver, partition 29, ANY, 4715 bytes)
16:33:09.895 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 26.0 in stage 2.0 (TID 29) in 357 ms on localhost (executor driver) (26/32)
16:33:09.895 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Running task 29.0 in stage 2.0 (TID 32)
16:33:09.901 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Finished task 24.0 in stage 2.0 (TID 27). 2887 bytes result sent to driver
16:33:09.905 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 30.0 in stage 2.0 (TID 33, localhost, executor driver, partition 30, ANY, 4715 bytes)
16:33:09.906 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Running task 28.0 in stage 2.0 (TID 31)
16:33:09.909 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 24.0 in stage 2.0 (TID 27) in 527 ms on localhost (executor driver) (27/32)
16:33:09.910 INFO  [Executor task launch worker for task 32] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:09.910 INFO  [Executor task launch worker for task 32] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:09.911 INFO  [Executor task launch worker for task 31] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:09.911 INFO  [Executor task launch worker for task 31] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:09.915 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Running task 30.0 in stage 2.0 (TID 33)
16:33:09.926 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:09.926 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:09.969 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Finished task 27.0 in stage 2.0 (TID 30). 2844 bytes result sent to driver
16:33:09.970 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 31.0 in stage 2.0 (TID 34, localhost, executor driver, partition 31, ANY, 4715 bytes)
16:33:09.970 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 27.0 in stage 2.0 (TID 30) in 205 ms on localhost (executor driver) (28/32)
16:33:09.971 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Running task 31.0 in stage 2.0 (TID 34)
16:33:09.976 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:09.976 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:10.062 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Finished task 29.0 in stage 2.0 (TID 32). 2844 bytes result sent to driver
16:33:10.063 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 29.0 in stage 2.0 (TID 32) in 170 ms on localhost (executor driver) (29/32)
16:33:10.102 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Finished task 28.0 in stage 2.0 (TID 31). 2844 bytes result sent to driver
16:33:10.103 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 28.0 in stage 2.0 (TID 31) in 328 ms on localhost (executor driver) (30/32)
16:33:10.138 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Finished task 30.0 in stage 2.0 (TID 33). 2887 bytes result sent to driver
16:33:10.143 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 30.0 in stage 2.0 (TID 33) in 238 ms on localhost (executor driver) (31/32)
16:33:10.145 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Finished task 31.0 in stage 2.0 (TID 34). 2844 bytes result sent to driver
16:33:10.147 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 31.0 in stage 2.0 (TID 34) in 177 ms on localhost (executor driver) (32/32)
16:33:10.148 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
16:33:10.149 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (show at DMReleaseCustomer.scala:75) finished in 5.547 s
16:33:10.149 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:33:10.149 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
16:33:10.149 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
16:33:10.149 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:33:10.150 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:33:10.158 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 23.9 KB, free 891.7 MB)
16:33:10.162 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.1 KB, free 891.6 MB)
16:33:10.165 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.237.1:6385 (size: 10.1 KB, free: 892.0 MB)
16:33:10.166 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
16:33:10.166 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(0))
16:33:10.174 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
16:33:10.177 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 35, localhost, executor driver, partition 0, ANY, 4726 bytes)
16:33:10.177 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 35)
16:33:10.226 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:33:10.226 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:10.257 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 35). 3216 bytes result sent to driver
16:33:10.258 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 35) in 82 ms on localhost (executor driver) (1/1)
16:33:10.258 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
16:33:10.259 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at DMReleaseCustomer.scala:75) finished in 0.084 s
16:33:10.260 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at DMReleaseCustomer.scala:75, took 7.644873 s
16:33:10.267 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:75
16:33:10.268 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (show at DMReleaseCustomer.scala:75) with 4 output partitions
16:33:10.269 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (show at DMReleaseCustomer.scala:75)
16:33:10.269 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
16:33:10.269 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:33:10.269 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:33:10.272 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 23.9 KB, free 891.6 MB)
16:33:10.274 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.1 KB, free 891.6 MB)
16:33:10.275 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.237.1:6385 (size: 10.1 KB, free: 892.0 MB)
16:33:10.276 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
16:33:10.277 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
16:33:10.277 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 4 tasks
16:33:10.278 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 6.0 (TID 36, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
16:33:10.278 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 37, localhost, executor driver, partition 1, ANY, 4726 bytes)
16:33:10.279 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 6.0 (TID 38, localhost, executor driver, partition 3, ANY, 4726 bytes)
16:33:10.279 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 6.0 (TID 39, localhost, executor driver, partition 4, ANY, 4726 bytes)
16:33:10.280 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Running task 1.0 in stage 6.0 (TID 36)
16:33:10.280 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 37)
16:33:10.282 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Running task 2.0 in stage 6.0 (TID 38)
16:33:10.282 INFO  [Executor task launch worker for task 37] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:33:10.282 INFO  [Executor task launch worker for task 37] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:10.286 INFO  [Executor task launch worker for task 38] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:33:10.287 INFO  [Executor task launch worker for task 38] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:10.287 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Running task 3.0 in stage 6.0 (TID 39)
16:33:10.290 INFO  [Executor task launch worker for task 39] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:33:10.290 INFO  [Executor task launch worker for task 39] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:10.295 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 32 blocks
16:33:10.296 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:10.298 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Finished task 1.0 in stage 6.0 (TID 36). 3044 bytes result sent to driver
16:33:10.304 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 6.0 (TID 36) in 26 ms on localhost (executor driver) (1/4)
16:33:10.325 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Finished task 3.0 in stage 6.0 (TID 39). 3207 bytes result sent to driver
16:33:10.325 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 37). 3174 bytes result sent to driver
16:33:10.329 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 6.0 (TID 39) in 50 ms on localhost (executor driver) (2/4)
16:33:10.331 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 37) in 53 ms on localhost (executor driver) (3/4)
16:33:10.365 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Finished task 2.0 in stage 6.0 (TID 38). 3175 bytes result sent to driver
16:33:10.368 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 6.0 (TID 38) in 89 ms on localhost (executor driver) (4/4)
16:33:10.369 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
16:33:10.371 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (show at DMReleaseCustomer.scala:75) finished in 0.093 s
16:33:10.372 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: show at DMReleaseCustomer.scala:75, took 0.105697 s
16:33:10.379 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:75
16:33:10.381 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 3 (show at DMReleaseCustomer.scala:75) with 4 output partitions
16:33:10.381 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (show at DMReleaseCustomer.scala:75)
16:33:10.381 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 8)
16:33:10.382 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:33:10.383 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75), which has no missing parents
16:33:10.392 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 23.9 KB, free 891.6 MB)
16:33:10.394 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.1 KB, free 891.6 MB)
16:33:10.395 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.237.1:6385 (size: 10.1 KB, free: 891.9 MB)
16:33:10.395 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
16:33:10.396 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 9 (MapPartitionsRDD[13] at show at DMReleaseCustomer.scala:75) (first 15 tasks are for partitions Vector(5, 6, 7, 8))
16:33:10.397 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 4 tasks
16:33:10.399 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 40, localhost, executor driver, partition 7, PROCESS_LOCAL, 4726 bytes)
16:33:10.399 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 9.0 (TID 41, localhost, executor driver, partition 8, PROCESS_LOCAL, 4726 bytes)
16:33:10.400 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 42, localhost, executor driver, partition 5, ANY, 4726 bytes)
16:33:10.407 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 43, localhost, executor driver, partition 6, ANY, 4726 bytes)
16:33:10.408 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 40)
16:33:10.409 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Running task 3.0 in stage 9.0 (TID 41)
16:33:10.412 INFO  [Executor task launch worker for task 40] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 32 blocks
16:33:10.412 INFO  [Executor task launch worker for task 40] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:10.413 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 42)
16:33:10.426 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 40). 3044 bytes result sent to driver
16:33:10.431 INFO  [Executor task launch worker for task 42] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:33:10.431 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 43)
16:33:10.431 INFO  [Executor task launch worker for task 42] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:10.432 INFO  [Executor task launch worker for task 41] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 32 blocks
16:33:10.445 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 40) in 46 ms on localhost (executor driver) (1/4)
16:33:10.445 INFO  [Executor task launch worker for task 43] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:33:10.446 INFO  [Executor task launch worker for task 41] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 14 ms
16:33:10.447 INFO  [Executor task launch worker for task 43] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
16:33:10.457 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Finished task 3.0 in stage 9.0 (TID 41). 3087 bytes result sent to driver
16:33:10.460 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 9.0 (TID 41) in 61 ms on localhost (executor driver) (2/4)
16:33:10.476 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 43). 3175 bytes result sent to driver
16:33:10.478 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 43) in 77 ms on localhost (executor driver) (3/4)
16:33:10.504 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 42). 3239 bytes result sent to driver
16:33:10.504 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 42) in 105 ms on localhost (executor driver) (4/4)
16:33:10.505 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
16:33:10.505 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 9 (show at DMReleaseCustomer.scala:75) finished in 0.107 s
16:33:10.506 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 3 finished: show at DMReleaseCustomer.scala:75, took 0.125501 s
16:33:10.617 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:33:10.623 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:33:10.624 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:33:10.625 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: age_range
16:33:10.629 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: gender
16:33:10.630 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: area_code
16:33:10.631 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: user_count
16:33:10.631 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: total_count
16:33:10.631 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:33:10.861 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 55.7176 ms
16:33:10.960 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 89.5103 ms
16:33:11.257 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 269.8717 ms
16:33:11.353 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 89.672 ms
16:33:11.408 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:104
16:33:11.409 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 16 (show at DMReleaseCustomer.scala:104)
16:33:11.410 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 19 (show at DMReleaseCustomer.scala:104)
16:33:11.411 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 4 (show at DMReleaseCustomer.scala:104) with 1 output partitions
16:33:11.411 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 12 (show at DMReleaseCustomer.scala:104)
16:33:11.411 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 11)
16:33:11.411 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 11)
16:33:11.414 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 10 (MapPartitionsRDD[16] at show at DMReleaseCustomer.scala:104), which has no missing parents
16:33:11.417 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 49.2 KB, free 891.5 MB)
16:33:11.424 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 18.6 KB, free 891.5 MB)
16:33:11.425 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.237.1:6385 (size: 18.6 KB, free: 891.9 MB)
16:33:11.426 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1004
16:33:11.427 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[16] at show at DMReleaseCustomer.scala:104) (first 15 tasks are for partitions Vector(0, 1))
16:33:11.427 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 2 tasks
16:33:11.428 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 44, localhost, executor driver, partition 0, PROCESS_LOCAL, 5402 bytes)
16:33:11.428 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 10.0 (TID 45, localhost, executor driver, partition 1, PROCESS_LOCAL, 5402 bytes)
16:33:11.429 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 44)
16:33:11.430 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Running task 1.0 in stage 10.0 (TID 45)
16:33:11.434 INFO  [Executor task launch worker for task 45] org.apache.spark.storage.BlockManager - Found block rdd_2_1 locally
16:33:11.436 INFO  [Executor task launch worker for task 44] org.apache.spark.storage.BlockManager - Found block rdd_2_0 locally
16:33:11.460 INFO  [Executor task launch worker for task 45] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 26.3771 ms
16:33:11.551 INFO  [Executor task launch worker for task 45] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 81.3263 ms
16:33:11.604 INFO  [Executor task launch worker for task 44] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 45.7367 ms
16:33:11.627 INFO  [Executor task launch worker for task 45] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.1683 ms
16:33:11.648 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Finished task 1.0 in stage 10.0 (TID 45). 1962 bytes result sent to driver
16:33:11.650 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 10.0 (TID 45) in 222 ms on localhost (executor driver) (1/2)
16:33:12.449 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 44). 2091 bytes result sent to driver
16:33:12.450 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 44) in 1022 ms on localhost (executor driver) (2/2)
16:33:12.450 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool 
16:33:12.451 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 10 (show at DMReleaseCustomer.scala:104) finished in 1.024 s
16:33:12.451 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:33:12.451 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
16:33:12.451 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 12, ShuffleMapStage 11)
16:33:12.451 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:33:12.452 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 11 (MapPartitionsRDD[19] at show at DMReleaseCustomer.scala:104), which has no missing parents
16:33:12.463 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 51.9 KB, free 891.5 MB)
16:33:12.465 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 16.6 KB, free 891.4 MB)
16:33:12.465 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 192.168.237.1:6385 (size: 16.6 KB, free: 891.9 MB)
16:33:12.467 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1004
16:33:12.468 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 32 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[19] at show at DMReleaseCustomer.scala:104) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
16:33:12.468 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 32 tasks
16:33:12.469 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 46, localhost, executor driver, partition 0, ANY, 4715 bytes)
16:33:12.470 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 11.0 (TID 47, localhost, executor driver, partition 1, ANY, 4715 bytes)
16:33:12.470 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 11.0 (TID 48, localhost, executor driver, partition 2, ANY, 4715 bytes)
16:33:12.470 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 11.0 (TID 49, localhost, executor driver, partition 3, ANY, 4715 bytes)
16:33:12.471 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 46)
16:33:12.471 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Running task 1.0 in stage 11.0 (TID 47)
16:33:12.471 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Running task 2.0 in stage 11.0 (TID 48)
16:33:12.471 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Running task 3.0 in stage 11.0 (TID 49)
16:33:12.474 INFO  [Executor task launch worker for task 48] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:12.474 INFO  [Executor task launch worker for task 47] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:12.476 INFO  [Executor task launch worker for task 46] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:12.477 INFO  [Executor task launch worker for task 48] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
16:33:12.477 INFO  [Executor task launch worker for task 47] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
16:33:12.478 INFO  [Executor task launch worker for task 49] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:12.478 INFO  [Executor task launch worker for task 46] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
16:33:12.493 INFO  [Executor task launch worker for task 49] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 16 ms
16:33:12.524 INFO  [Executor task launch worker for task 48] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 25.5932 ms
16:33:12.550 INFO  [Executor task launch worker for task 47] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.4082 ms
16:33:12.570 INFO  [Executor task launch worker for task 46] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.9442 ms
16:33:12.837 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 46). 2844 bytes result sent to driver
16:33:12.838 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 11.0 (TID 50, localhost, executor driver, partition 4, ANY, 4715 bytes)
16:33:12.838 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 46) in 369 ms on localhost (executor driver) (1/32)
16:33:12.838 INFO  [Executor task launch worker for task 50] org.apache.spark.executor.Executor - Running task 4.0 in stage 11.0 (TID 50)
16:33:12.843 INFO  [Executor task launch worker for task 50] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:12.844 INFO  [Executor task launch worker for task 50] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:12.877 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Finished task 2.0 in stage 11.0 (TID 48). 2844 bytes result sent to driver
16:33:12.882 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 11.0 (TID 51, localhost, executor driver, partition 5, ANY, 4715 bytes)
16:33:12.883 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 11.0 (TID 48) in 413 ms on localhost (executor driver) (2/32)
16:33:12.884 INFO  [Executor task launch worker for task 51] org.apache.spark.executor.Executor - Running task 5.0 in stage 11.0 (TID 51)
16:33:12.888 INFO  [Executor task launch worker for task 51] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:12.888 INFO  [Executor task launch worker for task 51] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:12.896 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Finished task 1.0 in stage 11.0 (TID 47). 2844 bytes result sent to driver
16:33:12.903 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 11.0 (TID 52, localhost, executor driver, partition 6, ANY, 4715 bytes)
16:33:12.904 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 11.0 (TID 47) in 434 ms on localhost (executor driver) (3/32)
16:33:12.904 INFO  [Executor task launch worker for task 52] org.apache.spark.executor.Executor - Running task 6.0 in stage 11.0 (TID 52)
16:33:12.993 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Finished task 3.0 in stage 11.0 (TID 49). 2844 bytes result sent to driver
16:33:12.993 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 11.0 (TID 53, localhost, executor driver, partition 7, ANY, 4715 bytes)
16:33:12.996 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 11.0 (TID 49) in 526 ms on localhost (executor driver) (4/32)
16:33:12.998 INFO  [Executor task launch worker for task 53] org.apache.spark.executor.Executor - Running task 7.0 in stage 11.0 (TID 53)
16:33:13.002 INFO  [Executor task launch worker for task 53] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:13.002 INFO  [Executor task launch worker for task 53] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:13.016 INFO  [Executor task launch worker for task 52] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:13.016 INFO  [Executor task launch worker for task 52] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:13.225 INFO  [Executor task launch worker for task 51] org.apache.spark.executor.Executor - Finished task 5.0 in stage 11.0 (TID 51). 2844 bytes result sent to driver
16:33:13.226 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 11.0 (TID 54, localhost, executor driver, partition 8, ANY, 4715 bytes)
16:33:13.226 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 11.0 (TID 51) in 344 ms on localhost (executor driver) (5/32)
16:33:13.227 INFO  [Executor task launch worker for task 54] org.apache.spark.executor.Executor - Running task 8.0 in stage 11.0 (TID 54)
16:33:13.231 INFO  [Executor task launch worker for task 54] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:13.231 INFO  [Executor task launch worker for task 54] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:13.250 INFO  [Executor task launch worker for task 50] org.apache.spark.executor.Executor - Finished task 4.0 in stage 11.0 (TID 50). 2844 bytes result sent to driver
16:33:13.254 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 11.0 (TID 55, localhost, executor driver, partition 9, ANY, 4715 bytes)
16:33:13.255 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 11.0 (TID 50) in 417 ms on localhost (executor driver) (6/32)
16:33:13.256 INFO  [Executor task launch worker for task 55] org.apache.spark.executor.Executor - Running task 9.0 in stage 11.0 (TID 55)
16:33:13.260 INFO  [Executor task launch worker for task 55] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:13.260 INFO  [Executor task launch worker for task 55] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:13.341 INFO  [Executor task launch worker for task 53] org.apache.spark.executor.Executor - Finished task 7.0 in stage 11.0 (TID 53). 2930 bytes result sent to driver
16:33:13.349 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 11.0 (TID 56, localhost, executor driver, partition 10, ANY, 4715 bytes)
16:33:13.351 INFO  [Executor task launch worker for task 56] org.apache.spark.executor.Executor - Running task 10.0 in stage 11.0 (TID 56)
16:33:13.351 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 11.0 (TID 53) in 358 ms on localhost (executor driver) (7/32)
16:33:13.355 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 192.168.237.1:6385 in memory (size: 10.1 KB, free: 891.9 MB)
16:33:13.356 INFO  [Executor task launch worker for task 56] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:13.357 INFO  [Executor task launch worker for task 56] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:13.363 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 192.168.237.1:6385 in memory (size: 10.1 KB, free: 891.9 MB)
16:33:13.364 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 199
16:33:13.370 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 192.168.237.1:6385 in memory (size: 18.6 KB, free: 892.0 MB)
16:33:13.373 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 200
16:33:13.411 INFO  [Executor task launch worker for task 52] org.apache.spark.executor.Executor - Finished task 6.0 in stage 11.0 (TID 52). 2930 bytes result sent to driver
16:33:13.412 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 11.0 (TID 57, localhost, executor driver, partition 11, ANY, 4715 bytes)
16:33:13.412 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 11.0 (TID 52) in 509 ms on localhost (executor driver) (8/32)
16:33:13.412 INFO  [Executor task launch worker for task 57] org.apache.spark.executor.Executor - Running task 11.0 in stage 11.0 (TID 57)
16:33:13.415 INFO  [Executor task launch worker for task 57] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:13.416 INFO  [Executor task launch worker for task 57] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:13.600 INFO  [Executor task launch worker for task 54] org.apache.spark.executor.Executor - Finished task 8.0 in stage 11.0 (TID 54). 2973 bytes result sent to driver
16:33:13.601 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 12.0 in stage 11.0 (TID 58, localhost, executor driver, partition 12, ANY, 4715 bytes)
16:33:13.602 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 11.0 (TID 54) in 376 ms on localhost (executor driver) (9/32)
16:33:13.624 INFO  [Executor task launch worker for task 58] org.apache.spark.executor.Executor - Running task 12.0 in stage 11.0 (TID 58)
16:33:13.636 INFO  [Executor task launch worker for task 58] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:13.637 INFO  [Executor task launch worker for task 58] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:13.686 INFO  [Executor task launch worker for task 55] org.apache.spark.executor.Executor - Finished task 9.0 in stage 11.0 (TID 55). 2930 bytes result sent to driver
16:33:13.687 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 13.0 in stage 11.0 (TID 59, localhost, executor driver, partition 13, ANY, 4715 bytes)
16:33:13.688 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 11.0 (TID 55) in 436 ms on localhost (executor driver) (10/32)
16:33:13.688 INFO  [Executor task launch worker for task 59] org.apache.spark.executor.Executor - Running task 13.0 in stage 11.0 (TID 59)
16:33:13.694 INFO  [Executor task launch worker for task 59] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:13.694 INFO  [Executor task launch worker for task 59] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:13.761 INFO  [Executor task launch worker for task 57] org.apache.spark.executor.Executor - Finished task 11.0 in stage 11.0 (TID 57). 2844 bytes result sent to driver
16:33:13.764 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 14.0 in stage 11.0 (TID 60, localhost, executor driver, partition 14, ANY, 4715 bytes)
16:33:13.765 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 11.0 (TID 57) in 352 ms on localhost (executor driver) (11/32)
16:33:13.765 INFO  [Executor task launch worker for task 60] org.apache.spark.executor.Executor - Running task 14.0 in stage 11.0 (TID 60)
16:33:13.769 INFO  [Executor task launch worker for task 60] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:13.770 INFO  [Executor task launch worker for task 60] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:13.791 INFO  [Executor task launch worker for task 56] org.apache.spark.executor.Executor - Finished task 10.0 in stage 11.0 (TID 56). 2887 bytes result sent to driver
16:33:13.793 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 15.0 in stage 11.0 (TID 61, localhost, executor driver, partition 15, ANY, 4715 bytes)
16:33:13.794 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 11.0 (TID 56) in 445 ms on localhost (executor driver) (12/32)
16:33:13.794 INFO  [Executor task launch worker for task 61] org.apache.spark.executor.Executor - Running task 15.0 in stage 11.0 (TID 61)
16:33:13.798 INFO  [Executor task launch worker for task 61] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:13.798 INFO  [Executor task launch worker for task 61] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:13.820 INFO  [Executor task launch worker for task 58] org.apache.spark.executor.Executor - Finished task 12.0 in stage 11.0 (TID 58). 2887 bytes result sent to driver
16:33:13.821 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 16.0 in stage 11.0 (TID 62, localhost, executor driver, partition 16, ANY, 4715 bytes)
16:33:13.826 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 12.0 in stage 11.0 (TID 58) in 226 ms on localhost (executor driver) (13/32)
16:33:13.830 INFO  [Executor task launch worker for task 62] org.apache.spark.executor.Executor - Running task 16.0 in stage 11.0 (TID 62)
16:33:13.835 INFO  [Executor task launch worker for task 62] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:13.836 INFO  [Executor task launch worker for task 62] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
16:33:14.010 INFO  [Executor task launch worker for task 59] org.apache.spark.executor.Executor - Finished task 13.0 in stage 11.0 (TID 59). 2844 bytes result sent to driver
16:33:14.011 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 17.0 in stage 11.0 (TID 63, localhost, executor driver, partition 17, ANY, 4715 bytes)
16:33:14.011 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 13.0 in stage 11.0 (TID 59) in 324 ms on localhost (executor driver) (14/32)
16:33:14.012 INFO  [Executor task launch worker for task 63] org.apache.spark.executor.Executor - Running task 17.0 in stage 11.0 (TID 63)
16:33:14.022 INFO  [Executor task launch worker for task 63] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:14.022 INFO  [Executor task launch worker for task 63] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:14.174 INFO  [Executor task launch worker for task 60] org.apache.spark.executor.Executor - Finished task 14.0 in stage 11.0 (TID 60). 2844 bytes result sent to driver
16:33:14.175 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 18.0 in stage 11.0 (TID 64, localhost, executor driver, partition 18, ANY, 4715 bytes)
16:33:14.176 INFO  [Executor task launch worker for task 64] org.apache.spark.executor.Executor - Running task 18.0 in stage 11.0 (TID 64)
16:33:14.176 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 14.0 in stage 11.0 (TID 60) in 412 ms on localhost (executor driver) (15/32)
16:33:14.177 INFO  [Executor task launch worker for task 62] org.apache.spark.executor.Executor - Finished task 16.0 in stage 11.0 (TID 62). 2844 bytes result sent to driver
16:33:14.178 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 19.0 in stage 11.0 (TID 65, localhost, executor driver, partition 19, ANY, 4715 bytes)
16:33:14.179 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 16.0 in stage 11.0 (TID 62) in 359 ms on localhost (executor driver) (16/32)
16:33:14.180 INFO  [Executor task launch worker for task 65] org.apache.spark.executor.Executor - Running task 19.0 in stage 11.0 (TID 65)
16:33:14.183 INFO  [Executor task launch worker for task 64] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:14.183 INFO  [Executor task launch worker for task 64] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:14.183 INFO  [Executor task launch worker for task 65] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:14.184 INFO  [Executor task launch worker for task 65] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:14.369 INFO  [Executor task launch worker for task 63] org.apache.spark.executor.Executor - Finished task 17.0 in stage 11.0 (TID 63). 2844 bytes result sent to driver
16:33:14.372 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 20.0 in stage 11.0 (TID 66, localhost, executor driver, partition 20, ANY, 4715 bytes)
16:33:14.373 INFO  [Executor task launch worker for task 66] org.apache.spark.executor.Executor - Running task 20.0 in stage 11.0 (TID 66)
16:33:14.373 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 17.0 in stage 11.0 (TID 63) in 362 ms on localhost (executor driver) (17/32)
16:33:14.379 INFO  [Executor task launch worker for task 66] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:14.379 INFO  [Executor task launch worker for task 66] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:14.450 INFO  [Executor task launch worker for task 61] org.apache.spark.executor.Executor - Finished task 15.0 in stage 11.0 (TID 61). 2844 bytes result sent to driver
16:33:14.451 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 21.0 in stage 11.0 (TID 67, localhost, executor driver, partition 21, ANY, 4715 bytes)
16:33:14.452 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 15.0 in stage 11.0 (TID 61) in 660 ms on localhost (executor driver) (18/32)
16:33:14.454 INFO  [Executor task launch worker for task 67] org.apache.spark.executor.Executor - Running task 21.0 in stage 11.0 (TID 67)
16:33:14.457 INFO  [Executor task launch worker for task 67] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:14.457 INFO  [Executor task launch worker for task 67] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:14.508 INFO  [Executor task launch worker for task 64] org.apache.spark.executor.Executor - Finished task 18.0 in stage 11.0 (TID 64). 2844 bytes result sent to driver
16:33:14.509 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 22.0 in stage 11.0 (TID 68, localhost, executor driver, partition 22, ANY, 4715 bytes)
16:33:14.510 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 18.0 in stage 11.0 (TID 64) in 335 ms on localhost (executor driver) (19/32)
16:33:14.510 INFO  [Executor task launch worker for task 68] org.apache.spark.executor.Executor - Running task 22.0 in stage 11.0 (TID 68)
16:33:14.513 INFO  [Executor task launch worker for task 68] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:14.513 INFO  [Executor task launch worker for task 68] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:14.557 INFO  [Executor task launch worker for task 66] org.apache.spark.executor.Executor - Finished task 20.0 in stage 11.0 (TID 66). 2844 bytes result sent to driver
16:33:14.558 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 23.0 in stage 11.0 (TID 69, localhost, executor driver, partition 23, ANY, 4715 bytes)
16:33:14.558 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 20.0 in stage 11.0 (TID 66) in 186 ms on localhost (executor driver) (20/32)
16:33:14.558 INFO  [Executor task launch worker for task 69] org.apache.spark.executor.Executor - Running task 23.0 in stage 11.0 (TID 69)
16:33:14.562 INFO  [Executor task launch worker for task 69] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:14.562 INFO  [Executor task launch worker for task 69] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:14.609 INFO  [Executor task launch worker for task 65] org.apache.spark.executor.Executor - Finished task 19.0 in stage 11.0 (TID 65). 2930 bytes result sent to driver
16:33:14.611 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 24.0 in stage 11.0 (TID 70, localhost, executor driver, partition 24, ANY, 4715 bytes)
16:33:14.612 INFO  [Executor task launch worker for task 70] org.apache.spark.executor.Executor - Running task 24.0 in stage 11.0 (TID 70)
16:33:14.612 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 19.0 in stage 11.0 (TID 65) in 434 ms on localhost (executor driver) (21/32)
16:33:14.616 INFO  [Executor task launch worker for task 70] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:14.616 INFO  [Executor task launch worker for task 70] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:14.723 INFO  [Executor task launch worker for task 67] org.apache.spark.executor.Executor - Finished task 21.0 in stage 11.0 (TID 67). 2844 bytes result sent to driver
16:33:14.727 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 25.0 in stage 11.0 (TID 71, localhost, executor driver, partition 25, ANY, 4715 bytes)
16:33:14.727 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 21.0 in stage 11.0 (TID 67) in 276 ms on localhost (executor driver) (22/32)
16:33:14.728 INFO  [Executor task launch worker for task 71] org.apache.spark.executor.Executor - Running task 25.0 in stage 11.0 (TID 71)
16:33:14.733 INFO  [Executor task launch worker for task 71] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:14.733 INFO  [Executor task launch worker for task 71] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:14.786 INFO  [Executor task launch worker for task 69] org.apache.spark.executor.Executor - Finished task 23.0 in stage 11.0 (TID 69). 2844 bytes result sent to driver
16:33:14.788 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 26.0 in stage 11.0 (TID 72, localhost, executor driver, partition 26, ANY, 4715 bytes)
16:33:14.789 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 23.0 in stage 11.0 (TID 69) in 231 ms on localhost (executor driver) (23/32)
16:33:14.789 INFO  [Executor task launch worker for task 72] org.apache.spark.executor.Executor - Running task 26.0 in stage 11.0 (TID 72)
16:33:14.795 INFO  [Executor task launch worker for task 72] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:14.795 INFO  [Executor task launch worker for task 72] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:14.820 INFO  [Executor task launch worker for task 70] org.apache.spark.executor.Executor - Finished task 24.0 in stage 11.0 (TID 70). 2844 bytes result sent to driver
16:33:14.820 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 27.0 in stage 11.0 (TID 73, localhost, executor driver, partition 27, ANY, 4715 bytes)
16:33:14.821 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 24.0 in stage 11.0 (TID 70) in 210 ms on localhost (executor driver) (24/32)
16:33:14.821 INFO  [Executor task launch worker for task 73] org.apache.spark.executor.Executor - Running task 27.0 in stage 11.0 (TID 73)
16:33:14.864 INFO  [Executor task launch worker for task 73] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:14.864 INFO  [Executor task launch worker for task 73] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:14.876 INFO  [Executor task launch worker for task 68] org.apache.spark.executor.Executor - Finished task 22.0 in stage 11.0 (TID 68). 2887 bytes result sent to driver
16:33:14.880 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 28.0 in stage 11.0 (TID 74, localhost, executor driver, partition 28, ANY, 4715 bytes)
16:33:14.881 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 22.0 in stage 11.0 (TID 68) in 372 ms on localhost (executor driver) (25/32)
16:33:14.881 INFO  [Executor task launch worker for task 74] org.apache.spark.executor.Executor - Running task 28.0 in stage 11.0 (TID 74)
16:33:14.888 INFO  [Executor task launch worker for task 74] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:14.889 INFO  [Executor task launch worker for task 74] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:14.935 INFO  [Executor task launch worker for task 71] org.apache.spark.executor.Executor - Finished task 25.0 in stage 11.0 (TID 71). 2887 bytes result sent to driver
16:33:14.941 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 29.0 in stage 11.0 (TID 75, localhost, executor driver, partition 29, ANY, 4715 bytes)
16:33:14.941 INFO  [Executor task launch worker for task 75] org.apache.spark.executor.Executor - Running task 29.0 in stage 11.0 (TID 75)
16:33:14.941 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 25.0 in stage 11.0 (TID 71) in 215 ms on localhost (executor driver) (26/32)
16:33:14.944 INFO  [Executor task launch worker for task 75] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:14.944 INFO  [Executor task launch worker for task 75] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:15.084 INFO  [Executor task launch worker for task 72] org.apache.spark.executor.Executor - Finished task 26.0 in stage 11.0 (TID 72). 2887 bytes result sent to driver
16:33:15.085 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 30.0 in stage 11.0 (TID 76, localhost, executor driver, partition 30, ANY, 4715 bytes)
16:33:15.089 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 26.0 in stage 11.0 (TID 72) in 299 ms on localhost (executor driver) (27/32)
16:33:15.091 INFO  [Executor task launch worker for task 76] org.apache.spark.executor.Executor - Running task 30.0 in stage 11.0 (TID 76)
16:33:15.116 INFO  [Executor task launch worker for task 74] org.apache.spark.executor.Executor - Finished task 28.0 in stage 11.0 (TID 74). 2887 bytes result sent to driver
16:33:15.117 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 31.0 in stage 11.0 (TID 77, localhost, executor driver, partition 31, ANY, 4715 bytes)
16:33:15.124 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 28.0 in stage 11.0 (TID 74) in 245 ms on localhost (executor driver) (28/32)
16:33:15.127 INFO  [Executor task launch worker for task 77] org.apache.spark.executor.Executor - Running task 31.0 in stage 11.0 (TID 77)
16:33:15.129 INFO  [Executor task launch worker for task 76] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:15.145 INFO  [Executor task launch worker for task 76] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 16 ms
16:33:15.175 INFO  [Executor task launch worker for task 77] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 2 blocks
16:33:15.176 INFO  [Executor task launch worker for task 77] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:15.242 INFO  [Executor task launch worker for task 73] org.apache.spark.executor.Executor - Finished task 27.0 in stage 11.0 (TID 73). 2887 bytes result sent to driver
16:33:15.243 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 27.0 in stage 11.0 (TID 73) in 423 ms on localhost (executor driver) (29/32)
16:33:15.285 INFO  [Executor task launch worker for task 75] org.apache.spark.executor.Executor - Finished task 29.0 in stage 11.0 (TID 75). 2844 bytes result sent to driver
16:33:15.285 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 29.0 in stage 11.0 (TID 75) in 348 ms on localhost (executor driver) (30/32)
16:33:15.311 INFO  [Executor task launch worker for task 77] org.apache.spark.executor.Executor - Finished task 31.0 in stage 11.0 (TID 77). 2887 bytes result sent to driver
16:33:15.312 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 31.0 in stage 11.0 (TID 77) in 195 ms on localhost (executor driver) (31/32)
16:33:15.349 INFO  [Executor task launch worker for task 76] org.apache.spark.executor.Executor - Finished task 30.0 in stage 11.0 (TID 76). 2844 bytes result sent to driver
16:33:15.350 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 30.0 in stage 11.0 (TID 76) in 266 ms on localhost (executor driver) (32/32)
16:33:15.351 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
16:33:15.351 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 11 (show at DMReleaseCustomer.scala:104) finished in 2.882 s
16:33:15.351 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:33:15.351 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
16:33:15.354 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 12)
16:33:15.354 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:33:15.355 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 12 (MapPartitionsRDD[22] at show at DMReleaseCustomer.scala:104), which has no missing parents
16:33:15.364 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 26.8 KB, free 891.5 MB)
16:33:15.367 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 10.6 KB, free 891.5 MB)
16:33:15.367 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 192.168.237.1:6385 (size: 10.6 KB, free: 891.9 MB)
16:33:15.368 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1004
16:33:15.368 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[22] at show at DMReleaseCustomer.scala:104) (first 15 tasks are for partitions Vector(0))
16:33:15.369 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 1 tasks
16:33:15.374 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 78, localhost, executor driver, partition 0, ANY, 4726 bytes)
16:33:15.374 INFO  [Executor task launch worker for task 78] org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 78)
16:33:15.386 INFO  [Executor task launch worker for task 78] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:33:15.387 INFO  [Executor task launch worker for task 78] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:15.422 INFO  [Executor task launch worker for task 78] org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 78). 3324 bytes result sent to driver
16:33:15.423 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 78) in 49 ms on localhost (executor driver) (1/1)
16:33:15.423 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool 
16:33:15.423 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 12 (show at DMReleaseCustomer.scala:104) finished in 0.050 s
16:33:15.423 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 4 finished: show at DMReleaseCustomer.scala:104, took 4.015393 s
16:33:15.428 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DMReleaseCustomer.scala:104
16:33:15.429 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 5 (show at DMReleaseCustomer.scala:104) with 2 output partitions
16:33:15.429 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (show at DMReleaseCustomer.scala:104)
16:33:15.430 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 14)
16:33:15.430 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:33:15.430 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (MapPartitionsRDD[22] at show at DMReleaseCustomer.scala:104), which has no missing parents
16:33:15.432 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 26.8 KB, free 891.5 MB)
16:33:15.434 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 10.6 KB, free 891.5 MB)
16:33:15.434 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 192.168.237.1:6385 (size: 10.6 KB, free: 891.9 MB)
16:33:15.434 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1004
16:33:15.439 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 15 (MapPartitionsRDD[22] at show at DMReleaseCustomer.scala:104) (first 15 tasks are for partitions Vector(1, 2))
16:33:15.439 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 2 tasks
16:33:15.441 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 79, localhost, executor driver, partition 1, ANY, 4726 bytes)
16:33:15.441 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 15.0 (TID 80, localhost, executor driver, partition 2, ANY, 4726 bytes)
16:33:15.441 INFO  [Executor task launch worker for task 79] org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 79)
16:33:15.443 INFO  [Executor task launch worker for task 80] org.apache.spark.executor.Executor - Running task 1.0 in stage 15.0 (TID 80)
16:33:15.444 INFO  [Executor task launch worker for task 79] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:33:15.445 INFO  [Executor task launch worker for task 79] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:33:15.446 INFO  [Executor task launch worker for task 80] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 32 non-empty blocks out of 32 blocks
16:33:15.446 INFO  [Executor task launch worker for task 80] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:33:15.460 INFO  [Executor task launch worker for task 79] org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 79). 3252 bytes result sent to driver
16:33:15.461 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 79) in 21 ms on localhost (executor driver) (1/2)
16:33:15.467 INFO  [Executor task launch worker for task 80] org.apache.spark.executor.Executor - Finished task 1.0 in stage 15.0 (TID 80). 3256 bytes result sent to driver
16:33:15.468 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 15.0 (TID 80) in 27 ms on localhost (executor driver) (2/2)
16:33:15.468 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool 
16:33:15.468 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 15 (show at DMReleaseCustomer.scala:104) finished in 0.028 s
16:33:15.470 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 5 finished: show at DMReleaseCustomer.scala:104, took 0.040685 s
16:33:15.510 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@59dbf3c2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:33:15.517 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
16:33:15.583 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:33:15.840 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:33:15.841 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
16:33:15.842 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:33:15.844 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:33:15.850 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:33:15.863 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:33:15.864 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-27f3ad38-879d-4920-a670-57638e8a4546
