00:09:36.846 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
00:09:38.250 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_exposure_job
00:09:38.390 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
00:09:38.391 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
00:09:38.392 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
00:09:38.393 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
00:09:38.396 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
00:09:40.554 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 2770.
00:09:40.687 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
00:09:40.877 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
00:09:40.893 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
00:09:40.894 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
00:09:40.944 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-0c1d72e2-8ffc-4eb3-9b15-7370ae2ce806
00:09:41.148 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
00:09:41.348 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
00:09:41.902 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @27743ms
00:09:42.192 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
00:09:42.230 INFO  [main] org.spark_project.jetty.server.Server - Started @28102ms
00:09:42.289 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@54e81b21{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
00:09:42.289 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
00:09:42.354 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/jobs,null,AVAILABLE,@Spark}
00:09:42.355 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/jobs/json,null,AVAILABLE,@Spark}
00:09:42.355 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e11ab3d{/jobs/job,null,AVAILABLE,@Spark}
00:09:42.357 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b43e173{/jobs/job/json,null,AVAILABLE,@Spark}
00:09:42.358 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545f80bf{/stages,null,AVAILABLE,@Spark}
00:09:42.359 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22fa55b2{/stages/json,null,AVAILABLE,@Spark}
00:09:42.364 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6594402a{/stages/stage,null,AVAILABLE,@Spark}
00:09:42.367 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c3f01f{/stages/stage/json,null,AVAILABLE,@Spark}
00:09:42.368 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/stages/pool,null,AVAILABLE,@Spark}
00:09:42.382 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/stages/pool/json,null,AVAILABLE,@Spark}
00:09:42.385 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/storage,null,AVAILABLE,@Spark}
00:09:42.386 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5298dead{/storage/json,null,AVAILABLE,@Spark}
00:09:42.386 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/storage/rdd,null,AVAILABLE,@Spark}
00:09:42.388 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/storage/rdd/json,null,AVAILABLE,@Spark}
00:09:42.389 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/environment,null,AVAILABLE,@Spark}
00:09:42.390 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b986cd{/environment/json,null,AVAILABLE,@Spark}
00:09:42.391 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41c62850{/executors,null,AVAILABLE,@Spark}
00:09:42.392 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@328572f0{/executors/json,null,AVAILABLE,@Spark}
00:09:42.432 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f460bb{/executors/threadDump,null,AVAILABLE,@Spark}
00:09:42.449 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d2a6eac{/executors/threadDump/json,null,AVAILABLE,@Spark}
00:09:42.524 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c0f7678{/static,null,AVAILABLE,@Spark}
00:09:42.525 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f1249d{/,null,AVAILABLE,@Spark}
00:09:42.527 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a68df9{/api,null,AVAILABLE,@Spark}
00:09:42.531 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64711bf2{/jobs/job/kill,null,AVAILABLE,@Spark}
00:09:42.532 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c1e23ff{/stages/stage/kill,null,AVAILABLE,@Spark}
00:09:42.546 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
00:09:43.101 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
00:09:43.332 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2793.
00:09:43.367 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:2793
00:09:43.429 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
00:09:43.956 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 2793, None)
00:09:43.988 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:2793 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 2793, None)
00:09:44.014 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 2793, None)
00:09:44.014 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 2793, None)
00:09:44.968 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c27d163{/metrics/json,null,AVAILABLE,@Spark}
00:09:45.231 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
00:09:45.313 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
00:09:45.314 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
00:09:45.336 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b08f438{/SQL,null,AVAILABLE,@Spark}
00:09:45.336 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b2f8ab6{/SQL/json,null,AVAILABLE,@Spark}
00:09:45.337 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65ae095c{/SQL/execution,null,AVAILABLE,@Spark}
00:09:45.340 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e140e59{/SQL/execution/json,null,AVAILABLE,@Spark}
00:09:45.342 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ae263bf{/static/sql,null,AVAILABLE,@Spark}
00:09:47.211 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
00:09:48.774 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
00:09:48.859 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
00:09:52.460 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
00:09:55.825 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
00:09:55.829 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
00:09:56.644 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
00:09:56.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
00:09:56.826 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
00:09:57.112 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
00:09:57.115 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
00:09:57.191 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
00:09:57.192 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
00:09:57.297 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
00:09:57.298 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
00:09:57.302 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
00:09:57.303 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
00:09:57.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
00:09:57.308 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
00:09:57.315 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
00:09:57.315 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
00:09:57.321 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
00:09:57.321 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
00:09:57.329 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
00:09:57.330 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
00:09:57.339 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
00:09:57.340 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
00:10:00.208 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/ee10ad4e-1ab7-4954-a700-0398ca31723a_resources
00:10:00.224 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/ee10ad4e-1ab7-4954-a700-0398ca31723a
00:10:00.228 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/ee10ad4e-1ab7-4954-a700-0398ca31723a
00:10:00.251 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/ee10ad4e-1ab7-4954-a700-0398ca31723a/_tmp_space.db
00:10:00.287 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
00:10:00.390 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
00:10:00.391 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
00:10:00.400 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
00:10:00.400 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
00:10:00.415 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
00:10:00.958 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/0b3727f4-5ab9-4285-9fbb-6299e46602b6_resources
00:10:00.968 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/0b3727f4-5ab9-4285-9fbb-6299e46602b6
00:10:00.975 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/0b3727f4-5ab9-4285-9fbb-6299e46602b6
00:10:00.995 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/0b3727f4-5ab9-4285-9fbb-6299e46602b6/_tmp_space.db
00:10:01.004 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
00:10:01.198 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
00:10:04.762 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
00:10:05.266 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
00:10:05.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
00:10:05.302 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
00:10:05.303 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
00:10:05.795 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
00:10:05.795 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
00:10:05.881 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:05.941 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:05.942 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:05.943 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:05.943 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:05.943 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:05.943 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:05.944 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:05.944 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:05.944 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
00:10:06.044 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
00:10:07.489 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
00:10:07.518 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
00:10:07.522 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
00:10:07.523 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
00:10:07.524 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
00:10:07.526 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
00:10:07.530 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') idcard
00:10:07.592 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy') as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as age
00:10:07.659 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})',2) as int)%2 as gender
00:10:07.664 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') area_code
00:10:07.666 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') longitude
00:10:07.670 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') latitude
00:10:07.672 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') matter_id
00:10:07.675 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') model_code
00:10:07.676 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') model_version
00:10:07.678 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
00:10:07.679 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
00:10:07.680 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
00:10:07.730 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
00:10:07.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
00:10:07.789 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
00:10:07.789 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
00:10:07.820 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
00:10:07.821 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
00:10:07.839 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
00:10:07.840 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
00:10:07.857 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
00:10:07.857 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
00:10:07.873 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
00:10:07.873 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
00:10:07.889 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
00:10:07.889 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
00:10:07.905 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
00:10:07.905 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
00:10:07.923 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
00:10:07.924 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
00:10:07.944 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
00:10:07.945 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
00:10:07.976 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
00:10:07.978 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
00:10:08.007 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
00:10:08.008 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
00:10:08.053 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
00:10:08.053 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
00:10:08.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
00:10:08.072 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
00:10:09.219 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
00:10:09.219 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
00:10:09.226 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
00:10:09.226 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
00:10:09.263 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
00:10:09.264 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
00:10:09.306 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:09.306 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:09.306 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:09.306 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:09.307 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:09.307 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:09.307 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:09.307 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:09.308 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:09.308 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
00:10:09.481 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
00:10:09.482 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
00:10:09.507 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
00:10:09.507 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
00:10:10.662 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-24)
00:10:10.666 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
00:10:10.671 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
00:10:10.719 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
00:10:10.727 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
00:10:11.523 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
00:10:11.919 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 558.6016 ms
00:10:12.765 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 170.9335 ms
00:10:13.395 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 264.7 KB, free 897.3 MB)
00:10:13.691 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.4 KB, free 897.3 MB)
00:10:13.694 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:2793 (size: 22.4 KB, free: 897.6 MB)
00:10:13.702 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:63
00:10:13.772 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
00:10:14.382 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:63
00:10:14.512 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:63)
00:10:14.520 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:63) with 1 output partitions
00:10:14.521 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:63)
00:10:14.522 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
00:10:14.535 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
00:10:14.569 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:63), which has no missing parents
00:10:15.128 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 24.1 KB, free 897.3 MB)
00:10:15.132 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.5 KB, free 897.3 MB)
00:10:15.133 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:2793 (size: 10.5 KB, free: 897.6 MB)
00:10:15.134 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
00:10:15.184 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:63) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
00:10:15.191 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
00:10:15.353 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5419 bytes)
00:10:15.359 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5419 bytes)
00:10:15.362 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5419 bytes)
00:10:15.362 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5419 bytes)
00:10:15.420 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
00:10:15.420 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
00:10:15.420 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
00:10:15.420 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
00:10:15.925 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 78.8679 ms
00:10:16.159 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-24]
00:10:16.159 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-24]
00:10:16.159 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-24]
00:10:16.159 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-24]
00:10:20.587 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
00:10:20.593 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
00:10:20.598 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
00:10:20.598 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
00:10:21.366 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1654 bytes result sent to driver
00:10:21.366 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1654 bytes result sent to driver
00:10:21.366 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1654 bytes result sent to driver
00:10:21.567 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 6153 ms on localhost (executor driver) (1/4)
00:10:21.614 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 6296 ms on localhost (executor driver) (2/4)
00:10:21.615 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 6253 ms on localhost (executor driver) (3/4)
00:10:35.125 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1783 bytes result sent to driver
00:10:35.170 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 19811 ms on localhost (executor driver) (4/4)
00:10:35.218 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DWReleaseCustomer.scala:63) finished in 19.912 s
00:10:35.222 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
00:10:35.223 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
00:10:35.235 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
00:10:35.243 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
00:10:35.244 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
00:10:35.274 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:63), which has no missing parents
00:10:35.324 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
00:10:35.328 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
00:10:35.358 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:2793 (size: 2.2 KB, free: 897.6 MB)
00:10:35.361 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
00:10:35.366 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:63) (first 15 tasks are for partitions Vector(0))
00:10:35.367 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
00:10:35.380 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
00:10:35.380 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
00:10:35.549 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
00:10:35.556 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 49 ms
00:10:35.900 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 2509 bytes result sent to driver
00:10:35.901 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 533 ms on localhost (executor driver) (1/1)
00:10:35.902 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
00:10:35.904 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:63) finished in 0.536 s
00:10:35.949 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:63, took 21.565993 s
00:10:36.164 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
00:10:36.234 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
00:10:36.234 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
00:10:36.327 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:36.328 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:36.329 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:36.329 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:36.329 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:36.329 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:36.330 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:36.332 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:36.332 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
00:10:36.333 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:36.333 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:36.333 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:36.333 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:36.334 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:36.334 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:36.335 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:36.335 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:36.335 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
00:10:36.943 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/.hive-staging_hive_2019-09-28_00-10-36_939_4884335333463668320-1
00:10:37.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
00:10:37.856 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
00:10:37.865 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
00:10:37.865 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
00:10:37.912 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
00:10:37.913 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
00:10:37.953 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:37.954 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:37.955 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:37.956 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:37.956 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:37.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:37.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:37.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:37.958 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:10:37.958 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
00:10:37.961 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
00:10:37.961 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
00:10:37.962 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
00:10:37.962 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
00:10:38.026 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-24)
00:10:38.027 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
00:10:38.028 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
00:10:38.029 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
00:10:38.030 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
00:10:38.151 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
00:10:38.402 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 101.4205 ms
00:10:38.432 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 264.7 KB, free 897.0 MB)
00:10:38.468 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.4 KB, free 897.0 MB)
00:10:38.468 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.237.1:2793 (size: 22.4 KB, free: 897.5 MB)
00:10:38.471 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:44
00:10:38.471 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
00:10:38.786 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:44
00:10:38.787 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (insertInto at SparkHelper.scala:44)
00:10:38.788 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (insertInto at SparkHelper.scala:44) with 4 output partitions
00:10:38.788 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (insertInto at SparkHelper.scala:44)
00:10:38.788 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
00:10:38.789 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
00:10:38.790 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:44), which has no missing parents
00:10:38.804 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 24.1 KB, free 897.0 MB)
00:10:38.818 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.5 KB, free 897.0 MB)
00:10:38.819 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.237.1:2793 (size: 10.5 KB, free: 897.5 MB)
00:10:38.820 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
00:10:38.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:44) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
00:10:38.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 4 tasks
00:10:38.823 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 5, localhost, executor driver, partition 0, ANY, 5419 bytes)
00:10:38.824 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 6, localhost, executor driver, partition 1, ANY, 5419 bytes)
00:10:38.824 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 7, localhost, executor driver, partition 2, ANY, 5419 bytes)
00:10:38.825 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 8, localhost, executor driver, partition 3, ANY, 5419 bytes)
00:10:38.825 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 5)
00:10:38.828 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 7)
00:10:38.829 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 8)
00:10:38.830 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 6)
00:10:38.860 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-24]
00:10:38.867 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-24]
00:10:38.912 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-24]
00:10:38.937 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
00:10:38.956 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-24]
00:10:38.969 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
00:10:38.999 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
00:10:39.050 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 7). 1525 bytes result sent to driver
00:10:39.094 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 7) in 270 ms on localhost (executor driver) (1/4)
00:10:39.118 INFO  [Executor task launch worker for task 8] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
00:10:39.198 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 5). 1525 bytes result sent to driver
00:10:39.201 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 5) in 378 ms on localhost (executor driver) (2/4)
00:10:39.206 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 8). 1568 bytes result sent to driver
00:10:39.212 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 8) in 388 ms on localhost (executor driver) (3/4)
00:10:44.522 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
00:10:44.991 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.237.1:2793 in memory (size: 2.2 KB, free: 897.5 MB)
00:10:45.107 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 4
00:10:45.109 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 8
00:10:45.110 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 5
00:10:45.117 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.237.1:2793 in memory (size: 10.5 KB, free: 897.5 MB)
00:10:45.118 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
00:10:45.118 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 3
00:10:45.190 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 0
00:10:45.214 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 7
00:10:45.219 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
00:10:45.220 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
00:10:45.229 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.237.1:2793 in memory (size: 22.4 KB, free: 897.6 MB)
00:10:50.582 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 6). 1783 bytes result sent to driver
00:10:50.594 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 6) in 11770 ms on localhost (executor driver) (4/4)
00:10:50.595 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
00:10:50.598 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (insertInto at SparkHelper.scala:44) finished in 11.775 s
00:10:50.598 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
00:10:50.599 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
00:10:50.599 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
00:10:50.599 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
00:10:50.602 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:44), which has no missing parents
00:10:50.693 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 159.7 KB, free 897.1 MB)
00:10:50.698 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 58.6 KB, free 897.1 MB)
00:10:50.699 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.237.1:2793 (size: 58.6 KB, free: 897.5 MB)
00:10:50.703 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
00:10:50.705 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:44) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
00:10:50.705 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 4 tasks
00:10:50.709 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 9, localhost, executor driver, partition 0, ANY, 4726 bytes)
00:10:50.709 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 10, localhost, executor driver, partition 1, ANY, 4726 bytes)
00:10:50.710 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 11, localhost, executor driver, partition 2, ANY, 4726 bytes)
00:10:50.712 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 12, localhost, executor driver, partition 3, ANY, 4726 bytes)
00:10:50.712 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 9)
00:10:50.712 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 12)
00:10:50.712 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 11)
00:10:50.712 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 10)
00:10:51.220 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
00:10:51.220 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
00:10:51.221 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
00:10:51.221 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
00:10:51.222 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
00:10:51.222 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
00:10:51.223 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
00:10:51.222 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
00:10:51.701 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 154.5195 ms
00:10:52.103 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 75.8901 ms
00:10:54.262 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
00:10:54.262 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
00:10:54.262 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
00:10:54.262 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
00:10:54.419 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.2512 ms
00:10:55.397 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 59.2588 ms
00:10:55.432 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 30.808 ms
00:10:56.780 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5eceb9ed
00:10:56.783 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@549392d6
00:10:56.783 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@112c0721
00:10:56.783 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@bb5e6d5
00:10:57.044 INFO  [Executor task launch worker for task 9] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
00:10:57.125 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
00:10:57.125 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
00:10:57.125 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
00:10:57.125 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
00:10:57.126 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/.hive-staging_hive_2019-09-28_00-10-36_939_4884335333463668320-1/-ext-10000/_temporary/0/_temporary/attempt_20190928001054_0003_m_000002_0/bdp_day=2019-09-24/part-00002-271f1ff5-c6fd-4102-9200-b88558eca47f.c000
00:10:57.126 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/.hive-staging_hive_2019-09-28_00-10-36_939_4884335333463668320-1/-ext-10000/_temporary/0/_temporary/attempt_20190928001054_0003_m_000003_0/bdp_day=2019-09-24/part-00003-271f1ff5-c6fd-4102-9200-b88558eca47f.c000
00:10:57.126 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/.hive-staging_hive_2019-09-28_00-10-36_939_4884335333463668320-1/-ext-10000/_temporary/0/_temporary/attempt_20190928001054_0003_m_000000_0/bdp_day=2019-09-24/part-00000-271f1ff5-c6fd-4102-9200-b88558eca47f.c000
00:10:57.126 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/.hive-staging_hive_2019-09-28_00-10-36_939_4884335333463668320-1/-ext-10000/_temporary/0/_temporary/attempt_20190928001054_0003_m_000001_0/bdp_day=2019-09-24/part-00001-271f1ff5-c6fd-4102-9200-b88558eca47f.c000
00:10:57.132 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression set to false
00:10:57.132 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression set to false
00:10:57.132 INFO  [Executor task launch worker for task 12] parquet.hadoop.codec.CodecConfig - Compression set to false
00:10:57.132 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression set to false
00:10:57.132 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
00:10:57.132 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
00:10:57.132 INFO  [Executor task launch worker for task 12] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
00:10:57.132 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
00:10:57.134 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
00:10:57.134 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
00:10:57.134 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
00:10:57.134 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
00:10:57.134 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
00:10:57.134 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
00:10:57.134 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
00:10:57.134 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
00:10:57.135 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
00:10:57.135 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
00:10:57.135 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
00:10:57.135 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
00:10:57.135 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Dictionary is on
00:10:57.135 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Dictionary is on
00:10:57.135 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Dictionary is on
00:10:57.135 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Dictionary is on
00:10:57.135 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Validation is off
00:10:57.135 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Validation is off
00:10:57.135 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Validation is off
00:10:57.135 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Validation is off
00:10:57.135 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
00:10:57.135 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
00:10:57.136 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
00:10:57.136 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
00:10:58.261 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@381aa36d
00:10:58.261 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@3b564af8
00:10:58.263 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@75b9027f
00:10:58.263 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@2d0043a1
00:10:59.444 INFO  [Executor task launch worker for task 10] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,740,197
00:10:59.473 INFO  [Executor task launch worker for task 9] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,877
00:10:59.479 INFO  [Executor task launch worker for task 12] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,744
00:10:59.504 INFO  [Executor task launch worker for task 11] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,740,077
00:10:59.985 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
00:10:59.991 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 579,154B for [release_session] BINARY: 21,447 values, 579,077B raw, 579,077B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
00:10:59.993 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
00:10:59.994 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
00:10:59.994 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,446 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
00:10:59.999 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
00:10:59.999 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
00:10:59.999 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
00:11:00.000 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
00:11:00.001 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 41 entries, 164B raw, 41B comp}
00:11:00.001 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 214,521B for [device_num] BINARY: 21,447 values, 214,478B raw, 214,478B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
00:11:00.001 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 2,784B for [gender] BINARY: 21,446 values, 2,753B raw, 2,753B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 2 entries, 10B raw, 2B comp}
00:11:00.002 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,447 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
00:11:00.002 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
00:11:00.003 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
00:11:00.004 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 471,909B for [idcard] BINARY: 21,447 values, 471,842B raw, 471,842B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
00:11:00.004 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,447 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 41 entries, 164B raw, 41B comp}
00:11:00.005 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 2,790B for [gender] BINARY: 21,447 values, 2,759B raw, 2,759B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 2 entries, 10B raw, 2B comp}
00:11:00.005 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 3,234B for [area_code] BINARY: 21,447 values, 3,193B raw, 3,193B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 2 entries, 20B raw, 2B comp}
00:11:00.006 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 479 entries, 7,244B raw, 479B comp}
00:11:00.006 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 339 entries, 5,248B raw, 339B comp}
00:11:00.007 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
00:11:00.008 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 10 entries, 71B raw, 10B comp}
00:11:00.008 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
00:11:00.009 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 3,269B for [area_code] BINARY: 21,446 values, 3,228B raw, 3,228B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 2 entries, 20B raw, 2B comp}
00:11:00.014 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
00:11:00.035 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
00:11:00.036 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
00:11:00.036 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
00:11:00.036 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 479 entries, 7,244B raw, 479B comp}
00:11:00.037 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
00:11:00.037 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
00:11:00.038 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,446 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
00:11:00.038 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 5,447B for [device_type] BINARY: 21,446 values, 5,416B raw, 5,416B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
00:11:00.038 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 339 entries, 5,248B raw, 339B comp}
00:11:00.038 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
00:11:00.039 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
00:11:00.039 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
00:11:00.039 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
00:11:00.040 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
00:11:00.040 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
00:11:00.040 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 10 entries, 71B raw, 10B comp}
00:11:00.040 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 41 entries, 164B raw, 41B comp}
00:11:00.040 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
00:11:00.041 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 5 entries, 40B raw, 5B comp}
00:11:00.041 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
00:11:00.044 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
00:11:00.044 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 2,782B for [gender] BINARY: 21,446 values, 2,751B raw, 2,751B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 2 entries, 10B raw, 2B comp}
00:11:00.045 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 41 entries, 164B raw, 41B comp}
00:11:00.046 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 3,233B for [area_code] BINARY: 21,446 values, 3,192B raw, 3,192B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 2 entries, 20B raw, 2B comp}
00:11:00.046 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 2,795B for [gender] BINARY: 21,446 values, 2,764B raw, 2,764B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 2 entries, 10B raw, 2B comp}
00:11:00.046 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 3,206B for [area_code] BINARY: 21,446 values, 3,165B raw, 3,165B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 2 entries, 20B raw, 2B comp}
00:11:00.046 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 479 entries, 7,244B raw, 479B comp}
00:11:00.046 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 479 entries, 7,244B raw, 479B comp}
00:11:00.047 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 339 entries, 5,248B raw, 339B comp}
00:11:00.047 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 339 entries, 5,248B raw, 339B comp}
00:11:00.047 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
00:11:00.047 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
00:11:00.047 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 10 entries, 71B raw, 10B comp}
00:11:00.047 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 10 entries, 71B raw, 10B comp}
00:11:00.048 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
00:11:00.048 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
00:11:00.048 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 5 entries, 40B raw, 5B comp}
00:11:00.048 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 5 entries, 40B raw, 5B comp}
00:11:00.049 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
00:11:00.049 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
00:11:00.050 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 5 entries, 40B raw, 5B comp}
00:11:00.051 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,447 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
00:11:03.174 INFO  [Executor task launch worker for task 10] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190928001054_0003_m_000001_0' to hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/.hive-staging_hive_2019-09-28_00-10-36_939_4884335333463668320-1/-ext-10000/_temporary/0/task_20190928001054_0003_m_000001
00:11:03.231 INFO  [Executor task launch worker for task 10] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190928001054_0003_m_000001_0: Committed
00:11:03.235 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 10). 2660 bytes result sent to driver
00:11:03.239 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 10) in 12530 ms on localhost (executor driver) (1/4)
00:11:08.346 INFO  [Executor task launch worker for task 9] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190928001054_0003_m_000000_0' to hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/.hive-staging_hive_2019-09-28_00-10-36_939_4884335333463668320-1/-ext-10000/_temporary/0/task_20190928001054_0003_m_000000
00:11:08.346 INFO  [Executor task launch worker for task 9] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190928001054_0003_m_000000_0: Committed
00:11:08.349 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 9). 2617 bytes result sent to driver
00:11:08.351 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 9) in 17643 ms on localhost (executor driver) (2/4)
00:11:08.514 INFO  [Executor task launch worker for task 12] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190928001054_0003_m_000003_0' to hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/.hive-staging_hive_2019-09-28_00-10-36_939_4884335333463668320-1/-ext-10000/_temporary/0/task_20190928001054_0003_m_000003
00:11:08.514 INFO  [Executor task launch worker for task 12] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190928001054_0003_m_000003_0: Committed
00:11:08.518 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 12). 2617 bytes result sent to driver
00:11:08.528 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 12) in 17817 ms on localhost (executor driver) (3/4)
00:11:08.648 INFO  [Executor task launch worker for task 11] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190928001054_0003_m_000002_0' to hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/.hive-staging_hive_2019-09-28_00-10-36_939_4884335333463668320-1/-ext-10000/_temporary/0/task_20190928001054_0003_m_000002
00:11:08.648 INFO  [Executor task launch worker for task 11] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190928001054_0003_m_000002_0: Committed
00:11:08.651 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 11). 2617 bytes result sent to driver
00:11:08.652 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 11) in 17943 ms on localhost (executor driver) (4/4)
00:11:08.655 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
00:11:08.656 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (insertInto at SparkHelper.scala:44) finished in 17.948 s
00:11:08.658 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: insertInto at SparkHelper.scala:44, took 29.871537 s
00:11:08.909 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
00:11:08.928 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
00:11:08.929 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
00:11:09.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
00:11:09.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
00:11:09.158 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:09.165 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:09.165 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:09.165 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:09.166 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:09.166 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:09.166 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:09.166 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:09.166 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
00:11:09.167 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:09.168 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:09.169 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:09.169 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:09.170 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:09.170 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:09.171 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:09.171 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:09.171 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
00:11:09.205 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
00:11:09.205 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
00:11:09.266 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
00:11:09.266 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
00:11:09.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
00:11:09.268 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
00:11:09.269 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
00:11:09.269 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
00:11:09.269 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
00:11:09.269 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
00:11:09.452 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
00:11:09.452 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
00:11:09.516 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[2019-09-24]
00:11:09.516 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[2019-09-24]	
00:11:09.650 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/bdp_day=2019-09-24/000000_0
00:11:09.680 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
00:11:09.759 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
00:11:09.800 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/.hive-staging_hive_2019-09-28_00-10-36_939_4884335333463668320-1/-ext-10000/bdp_day=2019-09-24/part-00000-271f1ff5-c6fd-4102-9200-b88558eca47f.c000, dest: hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/bdp_day=2019-09-24/part-00000-271f1ff5-c6fd-4102-9200-b88558eca47f.c000, Status:true
00:11:09.884 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/.hive-staging_hive_2019-09-28_00-10-36_939_4884335333463668320-1/-ext-10000/bdp_day=2019-09-24/part-00001-271f1ff5-c6fd-4102-9200-b88558eca47f.c000, dest: hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/bdp_day=2019-09-24/part-00001-271f1ff5-c6fd-4102-9200-b88558eca47f.c000, Status:true
00:11:09.908 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/.hive-staging_hive_2019-09-28_00-10-36_939_4884335333463668320-1/-ext-10000/bdp_day=2019-09-24/part-00002-271f1ff5-c6fd-4102-9200-b88558eca47f.c000, dest: hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/bdp_day=2019-09-24/part-00002-271f1ff5-c6fd-4102-9200-b88558eca47f.c000, Status:true
00:11:09.946 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/.hive-staging_hive_2019-09-28_00-10-36_939_4884335333463668320-1/-ext-10000/bdp_day=2019-09-24/part-00003-271f1ff5-c6fd-4102-9200-b88558eca47f.c000, dest: hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/bdp_day=2019-09-24/part-00003-271f1ff5-c6fd-4102-9200-b88558eca47f.c000, Status:true
00:11:09.953 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[2019-09-24]
00:11:09.953 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[2019-09-24]	
00:11:10.001 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_customer
00:11:10.001 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_customer	
00:11:10.001 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[2019-09-24]
00:11:10.134 WARN  [main] hive.log - Updating partition stats fast for: dw_release_customer
00:11:10.166 WARN  [main] hive.log - Updated size to 5782491
00:11:10.709 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop01:9000/user/hive/warehouse/dw_release.db/dw_release_customer/.hive-staging_hive_2019-09-28_00-10-36_939_4884335333463668320-1/-ext-10000/bdp_day=2019-09-24 with partSpec {bdp_day=2019-09-24}
00:11:10.761 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
00:11:10.805 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
00:11:10.806 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: dw_release	
00:11:10.822 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
00:11:10.823 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
00:11:10.866 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
00:11:10.867 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
00:11:10.906 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:10.907 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:10.907 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:10.907 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:10.907 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:10.908 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:10.908 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:10.908 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:10.908 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
00:11:10.908 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:10.909 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:10.909 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:10.909 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:10.909 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:10.910 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:10.910 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:10.910 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:11:10.910 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
00:11:11.201 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@54e81b21{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
00:11:11.258 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
00:11:11.386 INFO  [dispatcher-event-loop-2] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
00:11:11.632 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
00:11:11.654 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
00:11:11.665 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
00:11:11.692 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
00:11:11.711 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
00:11:11.756 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
00:11:11.760 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-d1c1b305-56b0-422d-b55d-29da41738948
00:17:55.174 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
00:17:57.231 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_exposure_job
00:17:57.394 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
00:17:57.402 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
00:17:57.403 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
00:17:57.404 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
00:17:57.409 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
00:18:02.813 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 2972.
00:18:02.935 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
00:18:03.014 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
00:18:03.042 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
00:18:03.043 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
00:18:03.071 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-05120627-1836-4082-81be-ae6bdb095575
00:18:03.182 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
00:18:03.384 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
00:18:03.753 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @32339ms
00:18:03.930 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
00:18:03.963 INFO  [main] org.spark_project.jetty.server.Server - Started @32550ms
00:18:04.020 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@641b459b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
00:18:04.020 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
00:18:04.076 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74518890{/jobs,null,AVAILABLE,@Spark}
00:18:04.077 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ea1fda{/jobs/json,null,AVAILABLE,@Spark}
00:18:04.078 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@420745d7{/jobs/job,null,AVAILABLE,@Spark}
00:18:04.084 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2392212b{/jobs/job/json,null,AVAILABLE,@Spark}
00:18:04.085 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28f8e165{/stages,null,AVAILABLE,@Spark}
00:18:04.086 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66f66866{/stages/json,null,AVAILABLE,@Spark}
00:18:04.087 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d666b41{/stages/stage,null,AVAILABLE,@Spark}
00:18:04.092 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e1162e7{/stages/stage/json,null,AVAILABLE,@Spark}
00:18:04.093 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2f1700{/stages/pool,null,AVAILABLE,@Spark}
00:18:04.094 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/stages/pool/json,null,AVAILABLE,@Spark}
00:18:04.098 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/storage,null,AVAILABLE,@Spark}
00:18:04.100 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/storage/json,null,AVAILABLE,@Spark}
00:18:04.101 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@553f3b6e{/storage/rdd,null,AVAILABLE,@Spark}
00:18:04.102 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/storage/rdd/json,null,AVAILABLE,@Spark}
00:18:04.103 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/environment,null,AVAILABLE,@Spark}
00:18:04.104 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/environment/json,null,AVAILABLE,@Spark}
00:18:04.106 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bb7cce7{/executors,null,AVAILABLE,@Spark}
00:18:04.107 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b530eb9{/executors/json,null,AVAILABLE,@Spark}
00:18:04.108 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@678040b3{/executors/threadDump,null,AVAILABLE,@Spark}
00:18:04.109 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64a1923a{/executors/threadDump/json,null,AVAILABLE,@Spark}
00:18:04.130 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18ca3c62{/static,null,AVAILABLE,@Spark}
00:18:04.131 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53bf7094{/,null,AVAILABLE,@Spark}
00:18:04.133 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@710b30ef{/api,null,AVAILABLE,@Spark}
00:18:04.134 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18151a14{/jobs/job/kill,null,AVAILABLE,@Spark}
00:18:04.135 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@169da7f2{/stages/stage/kill,null,AVAILABLE,@Spark}
00:18:04.140 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
00:18:04.619 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
00:18:04.857 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2995.
00:18:04.885 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:2995
00:18:04.895 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
00:18:04.984 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 2995, None)
00:18:05.028 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:2995 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 2995, None)
00:18:05.056 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 2995, None)
00:18:05.057 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 2995, None)
00:18:06.099 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@293cde83{/metrics/json,null,AVAILABLE,@Spark}
00:18:06.429 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
00:18:06.570 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
00:18:06.571 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
00:18:06.610 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a9d5dff{/SQL,null,AVAILABLE,@Spark}
00:18:06.612 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bac9ba{/SQL/json,null,AVAILABLE,@Spark}
00:18:06.614 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54f66455{/SQL/execution,null,AVAILABLE,@Spark}
00:18:06.615 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c03a37{/SQL/execution/json,null,AVAILABLE,@Spark}
00:18:06.618 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26e412ef{/static/sql,null,AVAILABLE,@Spark}
00:18:08.694 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
00:18:10.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
00:18:11.084 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
00:18:13.449 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
00:18:16.039 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
00:18:16.043 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
00:18:16.588 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
00:18:16.594 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
00:18:16.699 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
00:18:16.994 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
00:18:16.997 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
00:18:17.047 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
00:18:17.048 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
00:18:17.151 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
00:18:17.151 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
00:18:17.157 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
00:18:17.157 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
00:18:17.162 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
00:18:17.163 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
00:18:17.169 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
00:18:17.169 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
00:18:17.174 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
00:18:17.174 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
00:18:17.179 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
00:18:17.179 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
00:18:17.187 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
00:18:17.187 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
00:18:19.299 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/f6627cd6-477c-4327-ad11-57c4041ae322_resources
00:18:19.336 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/f6627cd6-477c-4327-ad11-57c4041ae322
00:18:19.341 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/f6627cd6-477c-4327-ad11-57c4041ae322
00:18:19.346 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/f6627cd6-477c-4327-ad11-57c4041ae322/_tmp_space.db
00:18:19.370 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
00:18:19.516 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
00:18:19.518 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
00:18:19.530 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
00:18:19.531 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
00:18:19.538 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
00:18:20.051 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/a8968cf9-edd8-4e0c-ae25-530a5d461a5f_resources
00:18:20.056 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/a8968cf9-edd8-4e0c-ae25-530a5d461a5f
00:18:20.064 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/a8968cf9-edd8-4e0c-ae25-530a5d461a5f
00:18:20.099 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/a8968cf9-edd8-4e0c-ae25-530a5d461a5f/_tmp_space.db
00:18:20.101 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
00:18:20.423 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
00:18:23.878 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
00:18:24.569 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
00:18:24.570 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
00:18:24.594 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
00:18:24.595 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
00:18:24.879 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
00:18:24.879 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
00:18:24.951 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:24.999 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:25.000 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:25.001 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:25.001 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:25.001 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:25.002 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:25.002 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:25.003 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:25.003 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
00:18:25.110 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
00:18:26.390 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
00:18:26.412 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
00:18:26.414 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
00:18:26.415 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
00:18:26.416 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
00:18:26.416 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
00:18:27.256 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
00:18:27.257 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
00:18:27.263 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
00:18:27.263 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
00:18:27.300 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
00:18:27.300 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
00:18:27.343 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:27.343 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:27.344 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:27.344 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:27.344 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:27.344 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:27.345 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:27.345 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:27.345 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
00:18:27.346 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
00:18:27.490 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
00:18:27.491 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
00:18:27.520 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
00:18:27.520 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
00:18:28.491 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-24)
00:18:28.493 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 03)
00:18:28.495 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 1 more field>
00:18:28.551 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
00:18:28.562 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
00:18:29.325 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
00:18:29.523 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 473.7396 ms
00:18:30.234 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 141.655 ms
00:18:30.939 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 263.8 KB, free 897.3 MB)
00:18:31.194 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.2 KB, free 897.3 MB)
00:18:31.200 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:2995 (size: 22.2 KB, free: 897.6 MB)
00:18:31.208 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseExposure.scala:52
00:18:31.315 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
00:18:31.690 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseExposure.scala:52
00:18:31.800 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (show at DWReleaseExposure.scala:52)
00:18:31.804 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseExposure.scala:52) with 1 output partitions
00:18:31.805 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseExposure.scala:52)
00:18:31.805 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
00:18:31.811 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
00:18:31.836 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at DWReleaseExposure.scala:52), which has no missing parents
00:18:32.130 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 16.7 KB, free 897.3 MB)
00:18:32.135 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.3 KB, free 897.3 MB)
00:18:32.135 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:2995 (size: 7.3 KB, free: 897.6 MB)
00:18:32.136 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
00:18:32.185 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at DWReleaseExposure.scala:52) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
00:18:32.188 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
00:18:32.324 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5419 bytes)
00:18:32.327 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5419 bytes)
00:18:32.328 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5419 bytes)
00:18:32.329 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5419 bytes)
00:18:32.354 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
00:18:32.354 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
00:18:32.354 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
00:18:32.354 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
00:18:32.660 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-24]
00:18:32.660 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-24]
00:18:32.660 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-24]
00:18:32.660 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-24]
00:18:35.535 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
00:18:35.535 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
00:18:35.535 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
00:18:35.535 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
00:18:36.174 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1501 bytes result sent to driver
00:18:36.174 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1501 bytes result sent to driver
00:18:36.174 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1458 bytes result sent to driver
00:18:36.210 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 3880 ms on localhost (executor driver) (1/4)
00:18:36.219 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 3920 ms on localhost (executor driver) (2/4)
00:18:36.219 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 3892 ms on localhost (executor driver) (3/4)
00:18:39.632 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1630 bytes result sent to driver
00:18:39.633 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 7307 ms on localhost (executor driver) (4/4)
00:18:39.664 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
00:18:39.664 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DWReleaseExposure.scala:52) finished in 7.380 s
00:18:39.665 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
00:18:39.665 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
00:18:39.704 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
00:18:39.742 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
00:18:39.819 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at DWReleaseExposure.scala:52), which has no missing parents
00:18:39.863 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
00:18:39.866 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
00:18:39.867 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:2995 (size: 2.2 KB, free: 897.6 MB)
00:18:39.868 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
00:18:39.873 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at DWReleaseExposure.scala:52) (first 15 tasks are for partitions Vector(0))
00:18:39.874 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
00:18:39.877 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
00:18:39.877 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
00:18:40.091 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
00:18:40.130 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 109 ms
00:18:40.558 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 1695 bytes result sent to driver
00:18:40.564 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 690 ms on localhost (executor driver) (1/1)
00:18:40.565 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
00:18:40.567 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseExposure.scala:52) finished in 0.692 s
00:18:40.627 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseExposure.scala:52, took 8.936419 s
00:18:41.177 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@641b459b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
00:18:41.180 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
00:18:41.326 INFO  [dispatcher-event-loop-2] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
00:18:41.390 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
00:18:41.390 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
00:18:41.391 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
00:18:41.396 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
00:18:41.401 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
00:18:41.431 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
00:18:41.433 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-c858a47c-c3ed-42de-b9c7-1f7a9af8623f
19:35:14.401 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
19:35:15.788 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_exposure_job
19:35:16.030 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
19:35:16.035 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
19:35:16.036 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
19:35:16.037 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
19:35:16.054 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
19:35:18.990 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 1553.
19:35:19.092 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
19:35:19.212 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
19:35:19.241 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19:35:19.242 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
19:35:19.259 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-4a8c63e8-b9fe-4393-8d23-f90a01aa6473
19:35:19.373 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
19:35:19.553 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
19:35:19.979 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @23125ms
19:35:20.212 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
19:35:20.263 INFO  [main] org.spark_project.jetty.server.Server - Started @23409ms
19:35:20.328 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@17f9344b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:35:20.332 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
19:35:20.431 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/jobs,null,AVAILABLE,@Spark}
19:35:20.435 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/jobs/json,null,AVAILABLE,@Spark}
19:35:20.436 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/jobs/job,null,AVAILABLE,@Spark}
19:35:20.445 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fa47fea{/jobs/job/json,null,AVAILABLE,@Spark}
19:35:20.446 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b43e173{/stages,null,AVAILABLE,@Spark}
19:35:20.448 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545f80bf{/stages/json,null,AVAILABLE,@Spark}
19:35:20.450 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22fa55b2{/stages/stage,null,AVAILABLE,@Spark}
19:35:20.457 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@405325cf{/stages/stage/json,null,AVAILABLE,@Spark}
19:35:20.459 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c3f01f{/stages/pool,null,AVAILABLE,@Spark}
19:35:20.460 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/stages/pool/json,null,AVAILABLE,@Spark}
19:35:20.462 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/storage,null,AVAILABLE,@Spark}
19:35:20.464 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/storage/json,null,AVAILABLE,@Spark}
19:35:20.482 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5298dead{/storage/rdd,null,AVAILABLE,@Spark}
19:35:20.489 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/storage/rdd/json,null,AVAILABLE,@Spark}
19:35:20.504 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/environment,null,AVAILABLE,@Spark}
19:35:20.508 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/environment/json,null,AVAILABLE,@Spark}
19:35:20.509 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b986cd{/executors,null,AVAILABLE,@Spark}
19:35:20.510 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41c62850{/executors/json,null,AVAILABLE,@Spark}
19:35:20.511 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@328572f0{/executors/threadDump,null,AVAILABLE,@Spark}
19:35:20.512 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f460bb{/executors/threadDump/json,null,AVAILABLE,@Spark}
19:35:20.850 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d2a6eac{/static,null,AVAILABLE,@Spark}
19:35:20.851 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60222fd8{/,null,AVAILABLE,@Spark}
19:35:20.853 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f1249d{/api,null,AVAILABLE,@Spark}
19:35:20.854 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b1637e1{/jobs/job/kill,null,AVAILABLE,@Spark}
19:35:20.860 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64711bf2{/stages/stage/kill,null,AVAILABLE,@Spark}
19:35:20.867 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
19:35:21.659 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
19:35:21.862 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1575.
19:35:21.883 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:1575
19:35:21.917 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19:35:22.037 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 1575, None)
19:35:22.097 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:1575 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 1575, None)
19:35:22.121 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 1575, None)
19:35:22.125 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 1575, None)
19:35:22.959 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e4c3a38{/metrics/json,null,AVAILABLE,@Spark}
19:35:23.540 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
19:35:23.653 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
19:35:23.654 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
19:35:23.685 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48b22fd4{/SQL,null,AVAILABLE,@Spark}
19:35:23.686 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b08f438{/SQL/json,null,AVAILABLE,@Spark}
19:35:23.687 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73ab3aac{/SQL/execution,null,AVAILABLE,@Spark}
19:35:23.688 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65ae095c{/SQL/execution/json,null,AVAILABLE,@Spark}
19:35:23.692 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57aa341b{/static/sql,null,AVAILABLE,@Spark}
19:35:26.098 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19:35:28.178 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19:35:28.269 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
19:35:31.539 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19:35:35.133 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
19:35:35.137 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
19:35:35.907 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
19:35:35.916 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
19:35:36.049 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
19:35:36.299 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
19:35:36.309 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
19:35:36.367 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
19:35:36.368 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
19:35:36.490 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
19:35:36.490 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19:35:36.495 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
19:35:36.496 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
19:35:36.506 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
19:35:36.506 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
19:35:36.515 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
19:35:36.515 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
19:35:36.529 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_nshop pat=*
19:35:36.529 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_nshop pat=*	
19:35:36.541 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
19:35:36.541 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
19:35:36.556 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
19:35:36.556 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
19:35:36.573 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
19:35:36.573 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
19:35:39.604 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/df893fe1-958e-4215-a59e-55b59fa7f7aa_resources
19:35:39.656 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/df893fe1-958e-4215-a59e-55b59fa7f7aa
19:35:39.662 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/df893fe1-958e-4215-a59e-55b59fa7f7aa
19:35:39.679 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/df893fe1-958e-4215-a59e-55b59fa7f7aa/_tmp_space.db
19:35:39.713 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
19:35:39.908 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:35:39.939 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:35:39.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
19:35:39.986 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
19:35:40.003 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
19:35:41.448 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/b77eacaf-75e2-4fb6-a0c8-4805c5622e51_resources
19:35:41.454 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/b77eacaf-75e2-4fb6-a0c8-4805c5622e51
19:35:41.469 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/b77eacaf-75e2-4fb6-a0c8-4805c5622e51
19:35:41.480 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/b77eacaf-75e2-4fb6-a0c8-4805c5622e51/_tmp_space.db
19:35:41.487 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
19:35:41.829 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
19:35:46.011 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
19:35:46.740 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:35:46.740 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:35:46.798 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:35:46.798 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:35:47.272 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:35:47.272 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:35:47.339 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:47.385 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:47.385 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:47.386 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:47.386 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:47.387 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:47.388 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:47.388 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:47.390 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:47.393 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:35:47.474 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
19:35:49.002 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') user_id
19:35:49.068 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
19:35:49.070 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
19:35:49.070 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
19:35:49.071 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
19:35:49.072 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
19:35:49.082 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
19:35:49.114 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:35:49.115 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:35:50.383 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:35:50.384 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:35:50.397 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:35:50.398 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:35:50.475 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:35:50.476 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:35:50.529 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:50.530 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:50.530 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:50.530 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:50.531 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:50.531 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:50.531 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:50.533 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:50.534 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:35:50.535 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:35:50.694 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
19:35:50.694 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
19:35:50.724 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
19:35:50.725 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
19:35:51.900 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-24)
19:35:51.908 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 06)
19:35:51.911 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
19:35:51.979 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
19:35:51.993 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
19:35:53.259 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 583.5054 ms
19:35:54.972 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 269.3515 ms
19:35:55.568 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 264.3 KB, free 897.3 MB)
19:35:56.447 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.3 KB, free 897.3 MB)
19:35:56.462 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:1575 (size: 22.3 KB, free: 897.6 MB)
19:35:56.485 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseRegister.scala:52
19:35:56.562 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
19:35:57.137 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseRegister.scala:52
19:35:57.243 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseRegister.scala:52)
19:35:57.250 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseRegister.scala:52) with 1 output partitions
19:35:57.251 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseRegister.scala:52)
19:35:57.252 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
19:35:57.258 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
19:35:57.289 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseRegister.scala:52), which has no missing parents
19:35:57.786 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 21.1 KB, free 897.3 MB)
19:35:57.797 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.3 KB, free 897.3 MB)
19:35:57.798 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:1575 (size: 9.3 KB, free: 897.6 MB)
19:35:57.803 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
19:35:57.870 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseRegister.scala:52) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
19:35:57.876 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
19:35:58.144 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5419 bytes)
19:35:58.148 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5419 bytes)
19:35:58.149 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5419 bytes)
19:35:58.150 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5419 bytes)
19:35:58.209 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
19:35:58.210 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
19:35:58.210 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
19:35:58.210 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
19:35:58.775 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 44.1742 ms
19:35:58.931 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
19:35:58.945 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-24]
19:35:58.946 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-24]
19:35:58.945 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-24]
19:35:58.945 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-24]
19:36:03.162 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:36:03.163 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:36:03.475 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1654 bytes result sent to driver
19:36:03.475 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1654 bytes result sent to driver
19:36:03.522 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 5480 ms on localhost (executor driver) (1/4)
19:36:03.554 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 5404 ms on localhost (executor driver) (2/4)
19:36:03.806 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:36:03.902 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1568 bytes result sent to driver
19:36:03.920 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 5772 ms on localhost (executor driver) (3/4)
19:36:03.997 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:36:17.367 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1826 bytes result sent to driver
19:36:17.470 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 19322 ms on localhost (executor driver) (4/4)
19:36:17.475 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DWReleaseRegister.scala:52) finished in 19.498 s
19:36:17.477 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
19:36:17.478 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
19:36:17.594 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
19:36:17.634 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
19:36:17.678 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
19:36:17.761 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseRegister.scala:52), which has no missing parents
19:36:17.822 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
19:36:17.829 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
19:36:17.834 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:1575 (size: 2.2 KB, free: 897.6 MB)
19:36:17.840 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
19:36:17.850 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseRegister.scala:52) (first 15 tasks are for partitions Vector(0))
19:36:17.850 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
19:36:17.864 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
19:36:17.865 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
19:36:18.087 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
19:36:18.170 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 163 ms
19:36:18.455 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 1397 bytes result sent to driver
19:36:18.457 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 605 ms on localhost (executor driver) (1/1)
19:36:18.457 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
19:36:18.458 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseRegister.scala:52) finished in 0.606 s
19:36:18.533 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseRegister.scala:52, took 21.396209 s
19:36:18.855 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@17f9344b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:36:18.876 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
19:36:19.023 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
19:36:19.152 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
19:36:19.158 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
19:36:19.181 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
19:36:19.195 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
19:36:19.204 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
19:36:19.317 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
19:36:19.319 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-2ff80597-ab29-4f48-a39c-bbd88d5e69f5
19:44:11.670 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
19:44:12.757 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_exposure_job
19:44:12.875 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
19:44:12.877 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
19:44:12.880 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
19:44:12.881 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
19:44:12.882 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
19:44:15.988 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 1848.
19:44:16.076 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
19:44:16.176 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
19:44:16.193 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19:44:16.194 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
19:44:16.228 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-30883ccb-0dad-4f41-b96d-d7823e6374eb
19:44:16.326 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
19:44:16.536 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
19:44:16.942 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @19254ms
19:44:17.235 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
19:44:17.279 INFO  [main] org.spark_project.jetty.server.Server - Started @19591ms
19:44:17.349 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@54e81b21{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:44:17.350 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
19:44:17.422 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/jobs,null,AVAILABLE,@Spark}
19:44:17.424 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/jobs/json,null,AVAILABLE,@Spark}
19:44:17.431 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e11ab3d{/jobs/job,null,AVAILABLE,@Spark}
19:44:17.433 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b43e173{/jobs/job/json,null,AVAILABLE,@Spark}
19:44:17.438 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545f80bf{/stages,null,AVAILABLE,@Spark}
19:44:17.440 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22fa55b2{/stages/json,null,AVAILABLE,@Spark}
19:44:17.443 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6594402a{/stages/stage,null,AVAILABLE,@Spark}
19:44:17.460 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c3f01f{/stages/stage/json,null,AVAILABLE,@Spark}
19:44:17.471 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/stages/pool,null,AVAILABLE,@Spark}
19:44:17.473 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/stages/pool/json,null,AVAILABLE,@Spark}
19:44:17.474 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/storage,null,AVAILABLE,@Spark}
19:44:17.476 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5298dead{/storage/json,null,AVAILABLE,@Spark}
19:44:17.478 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/storage/rdd,null,AVAILABLE,@Spark}
19:44:17.480 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/storage/rdd/json,null,AVAILABLE,@Spark}
19:44:17.491 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/environment,null,AVAILABLE,@Spark}
19:44:17.492 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b986cd{/environment/json,null,AVAILABLE,@Spark}
19:44:17.501 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41c62850{/executors,null,AVAILABLE,@Spark}
19:44:17.502 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@328572f0{/executors/json,null,AVAILABLE,@Spark}
19:44:17.508 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f460bb{/executors/threadDump,null,AVAILABLE,@Spark}
19:44:17.517 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d2a6eac{/executors/threadDump/json,null,AVAILABLE,@Spark}
19:44:17.536 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c0f7678{/static,null,AVAILABLE,@Spark}
19:44:17.538 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f1249d{/,null,AVAILABLE,@Spark}
19:44:17.540 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a68df9{/api,null,AVAILABLE,@Spark}
19:44:17.541 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64711bf2{/jobs/job/kill,null,AVAILABLE,@Spark}
19:44:17.543 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c1e23ff{/stages/stage/kill,null,AVAILABLE,@Spark}
19:44:17.566 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
19:44:18.169 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
19:44:18.385 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1869.
19:44:18.405 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:1869
19:44:18.411 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19:44:18.505 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 1869, None)
19:44:18.720 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:1869 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 1869, None)
19:44:18.751 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 1869, None)
19:44:18.751 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 1869, None)
19:44:19.648 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c27d163{/metrics/json,null,AVAILABLE,@Spark}
19:44:20.056 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/GP23_Release/target/classes/hive-site.xml
19:44:20.178 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/GP23_Release/spark-warehouse').
19:44:20.179 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/GP23_Release/spark-warehouse'.
19:44:20.207 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b474074{/SQL,null,AVAILABLE,@Spark}
19:44:20.210 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48b22fd4{/SQL/json,null,AVAILABLE,@Spark}
19:44:20.212 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bdd5689{/SQL/execution,null,AVAILABLE,@Spark}
19:44:20.213 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73ab3aac{/SQL/execution/json,null,AVAILABLE,@Spark}
19:44:20.220 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5cd61783{/static/sql,null,AVAILABLE,@Spark}
19:44:22.602 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19:44:24.690 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19:44:24.757 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
19:44:28.081 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19:44:30.939 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
19:44:30.942 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
19:44:31.601 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
19:44:31.607 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
19:44:31.727 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
19:44:32.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
19:44:32.158 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_all_databases	
19:44:32.212 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=db1 pat=*
19:44:32.212 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=db1 pat=*	
19:44:32.326 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
19:44:32.326 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19:44:32.336 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
19:44:32.336 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
19:44:32.343 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
19:44:32.343 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
19:44:32.352 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=exercise pat=*
19:44:32.352 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=exercise pat=*	
19:44:32.359 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_nshop pat=*
19:44:32.359 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_nshop pat=*	
19:44:32.377 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
19:44:32.378 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
19:44:32.386 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test1 pat=*
19:44:32.387 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test1 pat=*	
19:44:32.410 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test2 pat=*
19:44:32.411 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_functions: db=test2 pat=*	
19:44:35.056 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/fa197d0a-82cd-464e-ac9e-14df2d7e764c_resources
19:44:35.078 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/fa197d0a-82cd-464e-ac9e-14df2d7e764c
19:44:35.081 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/fa197d0a-82cd-464e-ac9e-14df2d7e764c
19:44:35.100 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/fa197d0a-82cd-464e-ac9e-14df2d7e764c/_tmp_space.db
19:44:35.105 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
19:44:35.227 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:44:35.227 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:44:35.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
19:44:35.268 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: global_temp	
19:44:35.276 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
19:44:35.854 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/c3c4850e-6717-49dc-960d-e08135d3d542_resources
19:44:35.871 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/c3c4850e-6717-49dc-960d-e08135d3d542
19:44:35.879 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/12647/AppData/Local/Temp/12647/c3c4850e-6717-49dc-960d-e08135d3d542
19:44:35.909 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/12647/c3c4850e-6717-49dc-960d-e08135d3d542/_tmp_space.db
19:44:35.913 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/GP23_Release/spark-warehouse
19:44:36.234 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
19:44:41.461 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
19:44:42.114 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:44:42.114 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:44:42.146 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:44:42.146 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:44:42.415 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:44:42.416 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:44:42.509 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:42.556 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:42.556 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:42.556 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:42.557 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:42.557 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:42.557 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:42.558 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:42.558 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:42.558 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:44:42.613 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
19:44:45.057 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') user_id
19:44:45.148 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
19:44:45.150 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
19:44:45.151 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
19:44:45.152 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
19:44:45.162 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
19:44:45.162 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
19:44:45.202 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:44:45.203 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: default	
19:44:46.873 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:44:46.877 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:44:46.897 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:44:46.898 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:44:46.954 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:44:46.955 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:44:47.029 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:47.030 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:47.031 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:47.032 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:47.033 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:47.033 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:47.034 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:47.034 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:47.034 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:44:47.035 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:44:47.178 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
19:44:47.179 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
19:44:47.223 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
19:44:47.225 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=12647	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
19:44:48.437 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-24)
19:44:48.440 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 06)
19:44:48.443 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
19:44:48.535 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
19:44:48.561 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
19:44:49.879 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 587.8054 ms
19:44:50.551 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 159.0571 ms
19:44:51.231 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 264.3 KB, free 897.3 MB)
19:44:52.139 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.3 KB, free 897.3 MB)
19:44:52.143 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:1869 (size: 22.3 KB, free: 897.6 MB)
19:44:52.168 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseRegister.scala:52
19:44:52.255 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
19:44:52.986 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseRegister.scala:52
19:44:53.102 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
19:44:53.225 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseRegister.scala:52)
19:44:53.240 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseRegister.scala:52) with 1 output partitions
19:44:53.243 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseRegister.scala:52)
19:44:53.249 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
19:44:53.267 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
19:44:53.316 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseRegister.scala:52), which has no missing parents
19:44:54.036 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 21.1 KB, free 897.3 MB)
19:44:54.041 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.3 KB, free 897.3 MB)
19:44:54.043 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:1869 (size: 9.3 KB, free: 897.6 MB)
19:44:54.052 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
19:44:54.129 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseRegister.scala:52) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
19:44:54.133 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
19:44:54.432 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5419 bytes)
19:44:54.435 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5419 bytes)
19:44:54.436 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5419 bytes)
19:44:54.437 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5419 bytes)
19:44:54.492 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
19:44:54.492 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
19:44:54.492 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
19:44:54.492 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
19:44:55.096 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 58.3829 ms
19:44:55.267 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-24]
19:44:55.267 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-24]
19:44:55.267 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-24]
19:44:55.267 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop01:9000/user/hive/warehouse/ods_release.db/ods_01_release_session/bdp_day=2019-09-24/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-24]
19:44:58.729 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:44:58.737 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:44:58.739 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:44:58.739 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:45:00.373 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1611 bytes result sent to driver
19:45:00.373 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1611 bytes result sent to driver
19:45:00.373 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1611 bytes result sent to driver
19:45:00.767 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 6255 ms on localhost (executor driver) (1/4)
19:45:00.860 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 6424 ms on localhost (executor driver) (2/4)
19:45:00.861 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 6458 ms on localhost (executor driver) (3/4)
19:45:09.127 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1826 bytes result sent to driver
19:45:09.133 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 14698 ms on localhost (executor driver) (4/4)
19:45:09.233 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DWReleaseRegister.scala:52) finished in 14.859 s
19:45:09.343 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
19:45:09.470 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
19:45:09.493 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
19:45:09.536 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
19:45:09.589 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
19:45:09.779 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseRegister.scala:52), which has no missing parents
19:45:09.852 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 897.3 MB)
19:45:09.855 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 897.3 MB)
19:45:10.172 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:1869 (size: 2.2 KB, free: 897.6 MB)
19:45:10.194 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
19:45:10.221 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseRegister.scala:52) (first 15 tasks are for partitions Vector(0))
19:45:10.222 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
19:45:10.227 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
19:45:10.227 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
19:45:10.580 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
19:45:10.610 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 95 ms
19:45:10.856 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 2073 bytes result sent to driver
19:45:10.857 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 633 ms on localhost (executor driver) (1/1)
19:45:10.858 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
19:45:10.859 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseRegister.scala:52) finished in 0.635 s
19:45:10.916 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseRegister.scala:52, took 17.929488 s
19:45:11.150 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@54e81b21{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:45:11.177 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
19:45:11.282 INFO  [dispatcher-event-loop-1] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
19:45:11.456 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
19:45:11.467 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
19:45:11.469 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
19:45:11.493 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
19:45:11.508 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
19:45:11.523 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
19:45:11.525 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-086e6930-cae3-40fc-832d-4018fc508457
21:59:22.343 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
21:59:23.740 INFO  [main] org.apache.spark.SparkContext - Submitted application: QuestionPartition
21:59:23.861 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
21:59:23.862 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
21:59:23.863 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
21:59:23.864 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
21:59:23.865 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
21:59:28.162 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 4622.
21:59:28.273 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
21:59:28.363 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
21:59:28.380 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21:59:28.405 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
21:59:28.451 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-7a7d0a08-6c17-417c-b17a-a48f1ebadc85
21:59:28.563 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
21:59:28.711 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
21:59:29.015 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @14423ms
21:59:29.256 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
21:59:29.287 INFO  [main] org.spark_project.jetty.server.Server - Started @14697ms
21:59:29.338 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@1734f68{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
21:59:29.339 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
21:59:29.411 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3dddbe65{/jobs,null,AVAILABLE,@Spark}
21:59:29.412 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d35442b{/jobs/json,null,AVAILABLE,@Spark}
21:59:29.413 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4593ff34{/jobs/job,null,AVAILABLE,@Spark}
21:59:29.416 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@581d969c{/jobs/job/json,null,AVAILABLE,@Spark}
21:59:29.417 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b46a8c1{/stages,null,AVAILABLE,@Spark}
21:59:29.427 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29caf222{/stages/json,null,AVAILABLE,@Spark}
21:59:29.428 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5851bd4f{/stages/stage,null,AVAILABLE,@Spark}
21:59:29.432 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69c43e48{/stages/stage/json,null,AVAILABLE,@Spark}
21:59:29.433 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a80515c{/stages/pool,null,AVAILABLE,@Spark}
21:59:29.434 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c807b1d{/stages/pool/json,null,AVAILABLE,@Spark}
21:59:29.435 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b39fd82{/storage,null,AVAILABLE,@Spark}
21:59:29.437 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21680803{/storage/json,null,AVAILABLE,@Spark}
21:59:29.438 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c8b96ec{/storage/rdd,null,AVAILABLE,@Spark}
21:59:29.439 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d8f2f3a{/storage/rdd/json,null,AVAILABLE,@Spark}
21:59:29.443 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7048f722{/environment,null,AVAILABLE,@Spark}
21:59:29.444 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58a55449{/environment/json,null,AVAILABLE,@Spark}
21:59:29.450 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e0ff644{/executors,null,AVAILABLE,@Spark}
21:59:29.451 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a2bb0eb{/executors/json,null,AVAILABLE,@Spark}
21:59:29.452 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d0566ba{/executors/threadDump,null,AVAILABLE,@Spark}
21:59:29.453 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7728643a{/executors/threadDump/json,null,AVAILABLE,@Spark}
21:59:29.474 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5167268{/static,null,AVAILABLE,@Spark}
21:59:29.511 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@512d92b{/,null,AVAILABLE,@Spark}
21:59:29.517 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7bdf6bb7{/api,null,AVAILABLE,@Spark}
21:59:29.518 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@160c3ec1{/jobs/job/kill,null,AVAILABLE,@Spark}
21:59:29.520 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d0402b{/stages/stage/kill,null,AVAILABLE,@Spark}
21:59:29.524 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
21:59:30.123 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
21:59:30.368 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4639.
21:59:30.379 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:4639
21:59:30.389 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21:59:30.530 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 4639, None)
21:59:30.555 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:4639 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 4639, None)
21:59:30.573 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 4639, None)
21:59:30.577 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 4639, None)
21:59:31.339 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@245a060f{/metrics/json,null,AVAILABLE,@Spark}
21:59:32.940 INFO  [main] org.apache.spark.SparkContext - Starting job: foreach at QuestionPartition.scala:35
21:59:33.006 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 2 (map at QuestionPartition.scala:26)
21:59:33.009 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (map at QuestionPartition.scala:27)
21:59:33.025 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (foreach at QuestionPartition.scala:35) with 3 output partitions
21:59:33.028 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (foreach at QuestionPartition.scala:35)
21:59:33.030 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
21:59:33.037 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
21:59:33.069 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at map at QuestionPartition.scala:26), which has no missing parents
21:59:33.806 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 897.6 MB)
21:59:34.026 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1753.0 B, free 897.6 MB)
21:59:34.033 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:4639 (size: 1753.0 B, free: 897.6 MB)
21:59:34.052 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1004
21:59:34.101 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at map at QuestionPartition.scala:26) (first 15 tasks are for partitions Vector(0, 1, 2))
21:59:34.102 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 3 tasks
21:59:34.139 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[3] at map at QuestionPartition.scala:27), which has no missing parents
21:59:34.146 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 2.7 KB, free 897.6 MB)
21:59:34.150 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1758.0 B, free 897.6 MB)
21:59:34.151 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:4639 (size: 1758.0 B, free: 897.6 MB)
21:59:34.153 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
21:59:34.156 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[3] at map at QuestionPartition.scala:27) (first 15 tasks are for partitions Vector(0, 1, 2))
21:59:34.156 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 3 tasks
21:59:34.219 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4771 bytes)
21:59:34.223 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4773 bytes)
21:59:34.225 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4773 bytes)
21:59:34.252 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
21:59:34.252 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
21:59:34.253 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
21:59:34.693 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 898 bytes result sent to driver
21:59:34.694 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 941 bytes result sent to driver
21:59:34.695 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 941 bytes result sent to driver
21:59:34.710 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4773 bytes)
21:59:34.711 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 3)
21:59:34.712 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 4777 bytes)
21:59:34.721 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 4777 bytes)
21:59:34.724 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 1.0 in stage 1.0 (TID 4)
21:59:34.729 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 2.0 in stage 1.0 (TID 5)
21:59:34.779 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 2.0 in stage 1.0 (TID 5). 855 bytes result sent to driver
21:59:34.787 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 1.0 in stage 1.0 (TID 4). 898 bytes result sent to driver
21:59:34.799 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 3). 855 bytes result sent to driver
21:59:34.813 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 1.0 (TID 5) in 97 ms on localhost (executor driver) (1/3)
21:59:34.817 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 593 ms on localhost (executor driver) (1/3)
21:59:34.817 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 593 ms on localhost (executor driver) (2/3)
21:59:34.817 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 633 ms on localhost (executor driver) (3/3)
21:59:34.827 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
21:59:34.828 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 3) in 120 ms on localhost (executor driver) (2/3)
21:59:34.829 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 1.0 (TID 4) in 118 ms on localhost (executor driver) (3/3)
21:59:34.829 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
21:59:34.844 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (map at QuestionPartition.scala:26) finished in 0.696 s
21:59:34.845 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
21:59:34.846 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
21:59:34.847 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
21:59:34.848 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
21:59:34.855 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (map at QuestionPartition.scala:27) finished in 0.629 s
21:59:34.855 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
21:59:34.855 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
21:59:34.855 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
21:59:34.855 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
21:59:34.858 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[6] at join at QuestionPartition.scala:28), which has no missing parents
21:59:34.887 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 897.6 MB)
21:59:34.894 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2030.0 B, free 897.6 MB)
21:59:34.896 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:4639 (size: 2030.0 B, free: 897.6 MB)
21:59:34.897 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
21:59:34.901 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at join at QuestionPartition.scala:28) (first 15 tasks are for partitions Vector(0, 1, 2))
21:59:34.902 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 3 tasks
21:59:34.906 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
21:59:34.906 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 7, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
21:59:34.907 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 8, localhost, executor driver, partition 2, PROCESS_LOCAL, 4684 bytes)
21:59:34.909 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 6)
21:59:34.910 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 7)
21:59:34.923 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 8)
21:59:34.969 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 3 non-empty blocks out of 3 blocks
21:59:34.969 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 3 non-empty blocks out of 3 blocks
21:59:34.969 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 3 non-empty blocks out of 3 blocks
21:59:34.973 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 19 ms
21:59:34.973 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 19 ms
21:59:34.973 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 19 ms
21:59:34.986 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 3 non-empty blocks out of 3 blocks
21:59:34.986 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 3 non-empty blocks out of 3 blocks
21:59:34.986 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 3 non-empty blocks out of 3 blocks
21:59:34.986 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
21:59:34.986 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
21:59:34.987 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
21:59:35.171 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 6). 1095 bytes result sent to driver
21:59:35.171 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 8). 1095 bytes result sent to driver
21:59:35.171 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 7). 1138 bytes result sent to driver
21:59:35.173 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 6) in 269 ms on localhost (executor driver) (1/3)
21:59:35.173 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 8) in 266 ms on localhost (executor driver) (2/3)
21:59:35.174 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 7) in 268 ms on localhost (executor driver) (3/3)
21:59:35.175 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
21:59:35.176 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (foreach at QuestionPartition.scala:35) finished in 0.273 s
21:59:35.220 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: foreach at QuestionPartition.scala:35, took 2.279854 s
21:59:35.240 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
21:59:35.254 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@1734f68{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
21:59:35.257 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
21:59:35.312 INFO  [dispatcher-event-loop-2] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
21:59:35.375 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
21:59:35.376 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
21:59:35.402 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
21:59:35.410 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
21:59:35.424 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
21:59:35.425 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
21:59:35.426 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-c9b1eac1-8e63-4bb7-a917-21a6b3cae28e
22:00:36.972 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
22:00:38.271 INFO  [main] org.apache.spark.SparkContext - Submitted application: QuestionPartition
22:00:38.363 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 12647
22:00:38.365 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 12647
22:00:38.365 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
22:00:38.368 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
22:00:38.385 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(12647); groups with view permissions: Set(); users  with modify permissions: Set(12647); groups with modify permissions: Set()
22:00:43.615 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 4673.
22:00:43.664 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
22:00:43.701 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
22:00:43.708 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22:00:43.710 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
22:00:43.724 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\12647\AppData\Local\Temp\blockmgr-ee14151c-6754-47b7-9d1f-06999d59cb67
22:00:43.757 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 897.6 MB
22:00:43.857 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
22:00:43.996 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @10578ms
22:00:44.153 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
22:00:44.181 INFO  [main] org.spark_project.jetty.server.Server - Started @10763ms
22:00:44.228 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@2609abeb{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
22:00:44.228 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
22:00:44.268 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669253b7{/jobs,null,AVAILABLE,@Spark}
22:00:44.269 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2575f671{/jobs/json,null,AVAILABLE,@Spark}
22:00:44.270 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ecf9fb3{/jobs/job,null,AVAILABLE,@Spark}
22:00:44.275 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4593ff34{/jobs/job/json,null,AVAILABLE,@Spark}
22:00:44.279 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30c0ccff{/stages,null,AVAILABLE,@Spark}
22:00:44.280 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22db8f4{/stages/json,null,AVAILABLE,@Spark}
22:00:44.280 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d572e62{/stages/stage,null,AVAILABLE,@Spark}
22:00:44.282 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cd1ac19{/stages/stage/json,null,AVAILABLE,@Spark}
22:00:44.285 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3caa4757{/stages/pool,null,AVAILABLE,@Spark}
22:00:44.286 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1804f60d{/stages/pool/json,null,AVAILABLE,@Spark}
22:00:44.287 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@547e29a4{/storage,null,AVAILABLE,@Spark}
22:00:44.288 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@238b521e{/storage/json,null,AVAILABLE,@Spark}
22:00:44.290 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e2fc448{/storage/rdd,null,AVAILABLE,@Spark}
22:00:44.291 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@588ab592{/storage/rdd/json,null,AVAILABLE,@Spark}
22:00:44.298 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4cc61eb1{/environment,null,AVAILABLE,@Spark}
22:00:44.300 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2024293c{/environment/json,null,AVAILABLE,@Spark}
22:00:44.302 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c074c0c{/executors,null,AVAILABLE,@Spark}
22:00:44.304 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5949eba8{/executors/json,null,AVAILABLE,@Spark}
22:00:44.305 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58dea0a5{/executors/threadDump,null,AVAILABLE,@Spark}
22:00:44.306 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c291aad{/executors/threadDump/json,null,AVAILABLE,@Spark}
22:00:44.322 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@733037{/static,null,AVAILABLE,@Spark}
22:00:44.327 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40e4ea87{/,null,AVAILABLE,@Spark}
22:00:44.330 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a7b503d{/api,null,AVAILABLE,@Spark}
22:00:44.331 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47d93e0d{/jobs/job/kill,null,AVAILABLE,@Spark}
22:00:44.333 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@751e664e{/stages/stage/kill,null,AVAILABLE,@Spark}
22:00:44.336 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.237.1:4040
22:00:44.658 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
22:00:44.718 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4691.
22:00:44.721 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.237.1:4691
22:00:44.723 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22:00:44.781 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.237.1, 4691, None)
22:00:44.803 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.237.1:4691 with 897.6 MB RAM, BlockManagerId(driver, 192.168.237.1, 4691, None)
22:00:44.813 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.237.1, 4691, None)
22:00:44.814 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.237.1, 4691, None)
22:00:45.246 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79767781{/metrics/json,null,AVAILABLE,@Spark}
22:00:46.281 INFO  [main] org.apache.spark.SparkContext - Starting job: foreach at QuestionPartition.scala:35
22:00:46.306 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (map at QuestionPartition.scala:27)
22:00:46.308 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 2 (map at QuestionPartition.scala:26)
22:00:46.313 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (foreach at QuestionPartition.scala:35) with 3 output partitions
22:00:46.314 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (foreach at QuestionPartition.scala:35)
22:00:46.315 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
22:00:46.317 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
22:00:46.324 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at QuestionPartition.scala:27), which has no missing parents
22:00:46.815 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 897.6 MB)
22:00:46.958 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1758.0 B, free 897.6 MB)
22:00:46.975 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.237.1:4691 (size: 1758.0 B, free: 897.6 MB)
22:00:46.979 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1004
22:00:47.014 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at QuestionPartition.scala:27) (first 15 tasks are for partitions Vector(0, 1, 2))
22:00:47.035 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 3 tasks
22:00:47.102 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[2] at map at QuestionPartition.scala:26), which has no missing parents
22:00:47.111 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 2.7 KB, free 897.6 MB)
22:00:47.115 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1753.0 B, free 897.6 MB)
22:00:47.117 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.237.1:4691 (size: 1753.0 B, free: 897.6 MB)
22:00:47.120 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
22:00:47.125 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[2] at map at QuestionPartition.scala:26) (first 15 tasks are for partitions Vector(0, 1, 2))
22:00:47.125 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 3 tasks
22:00:47.197 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4773 bytes)
22:00:47.212 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4777 bytes)
22:00:47.214 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4777 bytes)
22:00:47.277 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
22:00:47.279 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
22:00:47.281 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
22:00:47.597 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 941 bytes result sent to driver
22:00:47.614 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4771 bytes)
22:00:47.615 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 3)
22:00:47.635 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 941 bytes result sent to driver
22:00:47.678 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 4773 bytes)
22:00:47.679 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 1.0 in stage 1.0 (TID 4)
22:00:47.698 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 476 ms on localhost (executor driver) (1/3)
22:00:47.712 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 511 ms on localhost (executor driver) (2/3)
22:00:47.743 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 855 bytes result sent to driver
22:00:47.747 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 4773 bytes)
22:00:47.749 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 2.0 in stage 1.0 (TID 5)
22:00:47.785 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 1.0 in stage 1.0 (TID 4). 855 bytes result sent to driver
22:00:47.790 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 3). 855 bytes result sent to driver
22:00:47.791 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 3) in 185 ms on localhost (executor driver) (1/3)
22:00:47.820 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 2.0 in stage 1.0 (TID 5). 855 bytes result sent to driver
22:00:47.822 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 683 ms on localhost (executor driver) (3/3)
22:00:47.824 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
22:00:47.825 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (map at QuestionPartition.scala:27) finished in 0.724 s
22:00:47.825 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 1.0 (TID 5) in 79 ms on localhost (executor driver) (2/3)
22:00:47.828 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
22:00:47.830 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
22:00:47.831 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
22:00:47.833 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
22:00:47.872 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 1.0 (TID 4) in 236 ms on localhost (executor driver) (3/3)
22:00:47.873 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
22:00:47.874 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (map at QuestionPartition.scala:26) finished in 0.657 s
22:00:47.875 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
22:00:47.875 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
22:00:47.876 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
22:00:47.876 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
22:00:47.878 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[6] at join at QuestionPartition.scala:28), which has no missing parents
22:00:47.915 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 897.6 MB)
22:00:47.924 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2030.0 B, free 897.6 MB)
22:00:47.926 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.237.1:4691 (size: 2030.0 B, free: 897.6 MB)
22:00:47.928 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
22:00:47.931 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at join at QuestionPartition.scala:28) (first 15 tasks are for partitions Vector(0, 1, 2))
22:00:47.931 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 3 tasks
22:00:47.936 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
22:00:47.937 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 7, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
22:00:47.938 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 8, localhost, executor driver, partition 2, PROCESS_LOCAL, 4684 bytes)
22:00:47.940 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 6)
22:00:47.940 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 7)
22:00:47.940 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 8)
22:00:47.986 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 3 non-empty blocks out of 3 blocks
22:00:47.986 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 3 non-empty blocks out of 3 blocks
22:00:47.986 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 3 non-empty blocks out of 3 blocks
22:00:47.989 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 12 ms
22:00:47.989 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 12 ms
22:00:47.989 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 12 ms
22:00:47.998 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 3 non-empty blocks out of 3 blocks
22:00:47.998 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 3 non-empty blocks out of 3 blocks
22:00:47.999 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
22:00:47.999 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
22:00:47.999 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 3 non-empty blocks out of 3 blocks
22:00:47.999 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
22:00:48.156 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 6). 1095 bytes result sent to driver
22:00:48.160 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 8). 1095 bytes result sent to driver
22:00:48.163 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 6) in 231 ms on localhost (executor driver) (1/3)
22:00:48.165 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 8) in 228 ms on localhost (executor driver) (2/3)
22:00:48.196 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 7). 1052 bytes result sent to driver
22:00:48.197 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 7) in 261 ms on localhost (executor driver) (3/3)
22:00:48.198 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
22:00:48.199 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (foreach at QuestionPartition.scala:35) finished in 0.266 s
22:00:48.211 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: foreach at QuestionPartition.scala:35, took 1.929191 s
22:00:48.228 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
22:00:48.238 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@2609abeb{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
22:00:48.242 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.237.1:4040
22:00:48.266 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
22:00:48.528 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
22:00:48.529 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
22:00:48.537 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
22:00:48.541 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
22:00:48.547 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
22:00:48.548 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
22:00:48.549 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\12647\AppData\Local\Temp\spark-ee83fb59-0d69-4e47-8881-5e4252104b0d
